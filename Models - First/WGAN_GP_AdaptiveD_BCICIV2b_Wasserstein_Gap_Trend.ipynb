{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0330dd92",
      "metadata": {
        "id": "0330dd92"
      },
      "source": [
        "# WGAN-GP + Adaptive Discriminator Hooks (Wasserstein Gap Trend)\n",
        "**Dataset:** BCI Competition IV 2b (.gdf)\n",
        "\n",
        "This notebook is a **research-ready experimental template** that implements:\n",
        "\n",
        "- Loading and preprocessing of **BCI Competition IV 2b** EEG `.gdf` files from `BCICIV_2b_gdf.zip`\n",
        "- Standard **WGAN-GP** training (critic updates + gradient penalty)\n",
        "- **Adaptive discriminator control based on the Wasserstein critic gap trend**\n",
        "  - Tracks the temporal evolution of E[D(x_real)] − E[D(G(z))]\n",
        "  - Uses gap trends to detect discriminator dominance and adjust critic update frequency\n",
        "- Full **training history logging**\n",
        "- **Checkpoint saving**, **best-model tracking**, and **resume training support**\n",
        "- **Training curve visualization** for losses and adaptive signals\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f411f730",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f411f730",
        "outputId": "6544a34c-88f5-4e8d-a69d-986af061ce19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Cell 1 — Install deps (Colab-friendly)\n",
        "# If you already installed these, you can skip this cell.\n",
        "!pip -q install mne numpy scipy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c373e28",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c373e28",
        "outputId": "794744c0-5cb0-44a5-cc8b-09691c5f7ef6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Cell 2 — Imports & Reproducibility\n",
        "import os\n",
        "import random\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, Optional, Tuple\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def seed_everything(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed_everything(42)\n",
        "print(\"Device:\", DEVICE)\n",
        "\n",
        "# Optional: reduce common numba-related surprises with some MNE setups\n",
        "os.environ[\"NUMBA_DISABLE_JIT\"] = \"1\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91fa4fd9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91fa4fd9",
        "outputId": "51e8abe0-1755-4206-f889-a38f8a98679b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using dataset directory: /content/data/BCICIV_2b_gdf\n",
            "Found GDF files: 45\n"
          ]
        }
      ],
      "source": [
        "# Cell 3 — Dataset path check (NO unzip)\n",
        "from pathlib import Path\n",
        "\n",
        "DATA_DIR = Path(\"../BCICIV_2b_gdf\")\n",
        "\n",
        "if not DATA_DIR.exists():\n",
        "    raise FileNotFoundError(\n",
        "        \"DATA_DIR not found. Upload the extracted 'BCICIV_2b_gdf/' folder to ./data/\"\n",
        "    )\n",
        "\n",
        "print(\"Using dataset directory:\", DATA_DIR.resolve())\n",
        "print(\"Found GDF files:\", len(list(DATA_DIR.rglob(\"*.gdf\"))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "33642779",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33642779",
        "outputId": "48bb0e4d-4a9a-483d-b318-e64e1585cdd4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Reading: B0101T.gdf ===\n",
            "Available annotation events: {np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11}\n",
            "Using internal event codes: {'left': 10, 'right': 11} (for labels ('769', '770') )\n",
            "\n",
            "=== Reading: B0102T.gdf ===\n",
            "Available annotation events: {np.str_('1023'): 1, np.str_('32766'): 2, np.str_('768'): 3, np.str_('769'): 4, np.str_('770'): 5}\n",
            "Using internal event codes: {'left': 4, 'right': 5} (for labels ('769', '770') )\n",
            "\n",
            "=== Reading: B0103T.gdf ===\n",
            "Available annotation events: {np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11, np.str_('781'): 12}\n",
            "Using internal event codes: {'left': 10, 'right': 11} (for labels ('769', '770') )\n",
            "\n",
            "=== Reading: B0201T.gdf ===\n",
            "Available annotation events: {np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11}\n",
            "Using internal event codes: {'left': 10, 'right': 11} (for labels ('769', '770') )\n",
            "\n",
            "=== Reading: B0202T.gdf ===\n",
            "Available annotation events: {np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11}\n",
            "Using internal event codes: {'left': 10, 'right': 11} (for labels ('769', '770') )\n",
            "\n",
            "=== Reading: B0203T.gdf ===\n",
            "Available annotation events: {np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11, np.str_('781'): 12}\n",
            "Using internal event codes: {'left': 10, 'right': 11} (for labels ('769', '770') )\n",
            "\n",
            "=== Reading: B0301T.gdf ===\n",
            "Available annotation events: {np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11}\n",
            "Using internal event codes: {'left': 10, 'right': 11} (for labels ('769', '770') )\n",
            "\n",
            "=== Reading: B0302T.gdf ===\n",
            "Available annotation events: {np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11}\n",
            "Using internal event codes: {'left': 10, 'right': 11} (for labels ('769', '770') )\n",
            "\n",
            "=== Reading: B0303T.gdf ===\n",
            "Available annotation events: {np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11, np.str_('781'): 12}\n",
            "Using internal event codes: {'left': 10, 'right': 11} (for labels ('769', '770') )\n",
            "\n",
            "=== Reading: B0401T.gdf ===\n",
            "Available annotation events: {np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11}\n",
            "Using internal event codes: {'left': 10, 'right': 11} (for labels ('769', '770') )\n",
            "\n",
            "=== Reading: B0402T.gdf ===\n",
            "Available annotation events: {np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11}\n",
            "Using internal event codes: {'left': 10, 'right': 11} (for labels ('769', '770') )\n",
            "\n",
            "=== Reading: B0403T.gdf ===\n",
            "Available annotation events: {np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11, np.str_('781'): 12}\n",
            "Using internal event codes: {'left': 10, 'right': 11} (for labels ('769', '770') )\n",
            "\n",
            "=== Reading: B0501T.gdf ===\n",
            "Available annotation events: {np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11}\n",
            "Using internal event codes: {'left': 10, 'right': 11} (for labels ('769', '770') )\n",
            "\n",
            "=== Reading: B0502T.gdf ===\n",
            "Available annotation events: {np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11}\n",
            "Using internal event codes: {'left': 10, 'right': 11} (for labels ('769', '770') )\n",
            "\n",
            "=== Reading: B0503T.gdf ===\n",
            "Available annotation events: {np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11, np.str_('781'): 12}\n",
            "Using internal event codes: {'left': 10, 'right': 11} (for labels ('769', '770') )\n",
            "\n",
            "=== Reading: B0601T.gdf ===\n",
            "Available annotation events: {np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11}\n",
            "Using internal event codes: {'left': 10, 'right': 11} (for labels ('769', '770') )\n",
            "\n",
            "=== Reading: B0602T.gdf ===\n",
            "Available annotation events: {np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11}\n",
            "Using internal event codes: {'left': 10, 'right': 11} (for labels ('769', '770') )\n",
            "\n",
            "=== Reading: B0603T.gdf ===\n",
            "Available annotation events: {np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11, np.str_('781'): 12}\n",
            "Using internal event codes: {'left': 10, 'right': 11} (for labels ('769', '770') )\n",
            "\n",
            "=== Reading: B0701T.gdf ===\n",
            "Available annotation events: {np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11}\n",
            "Using internal event codes: {'left': 10, 'right': 11} (for labels ('769', '770') )\n",
            "\n",
            "=== Reading: B0702T.gdf ===\n",
            "Available annotation events: {np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11}\n",
            "Using internal event codes: {'left': 10, 'right': 11} (for labels ('769', '770') )\n",
            "\n",
            "=== Reading: B0703T.gdf ===\n",
            "Available annotation events: {np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11, np.str_('781'): 12}\n",
            "Using internal event codes: {'left': 10, 'right': 11} (for labels ('769', '770') )\n",
            "\n",
            "=== Reading: B0801T.gdf ===\n",
            "Available annotation events: {np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11}\n",
            "Using internal event codes: {'left': 10, 'right': 11} (for labels ('769', '770') )\n",
            "\n",
            "=== Reading: B0802T.gdf ===\n",
            "Available annotation events: {np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11}\n",
            "Using internal event codes: {'left': 10, 'right': 11} (for labels ('769', '770') )\n",
            "\n",
            "=== Reading: B0803T.gdf ===\n",
            "Available annotation events: {np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11, np.str_('781'): 12}\n",
            "Using internal event codes: {'left': 10, 'right': 11} (for labels ('769', '770') )\n",
            "\n",
            "=== Reading: B0901T.gdf ===\n",
            "Available annotation events: {np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11}\n",
            "Using internal event codes: {'left': 10, 'right': 11} (for labels ('769', '770') )\n",
            "\n",
            "=== Reading: B0902T.gdf ===\n",
            "Available annotation events: {np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11}\n",
            "Using internal event codes: {'left': 10, 'right': 11} (for labels ('769', '770') )\n",
            "\n",
            "=== Reading: B0903T.gdf ===\n",
            "Available annotation events: {np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11, np.str_('781'): 12}\n",
            "Using internal event codes: {'left': 10, 'right': 11} (for labels ('769', '770') )\n",
            "Cropping SEQ_LEN from 1025 -> 1024 to satisfy architecture constraint.\n",
            "CHANNELS: 6 SEQ_LEN: 1024\n"
          ]
        }
      ],
      "source": [
        "# Cell 4 — Load BCICIV-2b (GDF -> epochs -> tensors) [FIXED: use annotation labels]\n",
        "import mne\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from typing import Dict, Optional, Tuple\n",
        "\n",
        "def _find_gdf_files(root: Path) -> list:\n",
        "    return sorted([p for p in root.rglob(\"*.gdf\")])\n",
        "\n",
        "def load_bciciv2b_epochs_by_labels(\n",
        "    root: Path,\n",
        "    file_glob: str = \"*T.gdf\",\n",
        "    tmin: float = 0.0,\n",
        "    tmax: float = 4.0,\n",
        "    resample_hz: int = 256,\n",
        "    picks: str = \"eeg\",\n",
        "    baseline: Optional[Tuple[float, float]] = None,\n",
        "    wanted_labels: Tuple[str, str] = (\"769\", \"770\"),  # left/right in BCICIV\n",
        "    verbose: bool = True,\n",
        ") -> Tuple[np.ndarray, np.ndarray, int, Dict[str, int]]:\n",
        "    all_files = _find_gdf_files(root)\n",
        "    if file_glob:\n",
        "        import fnmatch\n",
        "        all_files = [f for f in all_files if fnmatch.fnmatch(f.name, file_glob)]\n",
        "\n",
        "    if len(all_files) == 0:\n",
        "        raise FileNotFoundError(f\"No .gdf files found under {root} (file_glob={file_glob})\")\n",
        "\n",
        "    X_list, y_list = [], []\n",
        "    last_sfreq = None\n",
        "\n",
        "    # Fixed class order: wanted_labels[0] -> class 0, wanted_labels[1] -> class 1\n",
        "    global_event_ids = {\"left\": wanted_labels[0], \"right\": wanted_labels[1]}\n",
        "\n",
        "    for gdf_path in all_files:\n",
        "        print(\"\\n=== Reading:\", gdf_path.name, \"===\")\n",
        "\n",
        "        raw = mne.io.read_raw_gdf(str(gdf_path), preload=True, verbose=\"ERROR\")\n",
        "        raw.pick(picks)\n",
        "        raw.resample(resample_hz)\n",
        "\n",
        "        events, event_dict = mne.events_from_annotations(raw, verbose=\"ERROR\")\n",
        "        if verbose:\n",
        "            print(\"Available annotation events:\", event_dict)\n",
        "\n",
        "        # Convert wanted annotation labels -> internal event codes for THIS file\n",
        "        missing = [lab for lab in wanted_labels if lab not in event_dict]\n",
        "        if len(missing) > 0:\n",
        "            print(f\"Skipping {gdf_path.name} (missing labels {missing})\")\n",
        "            continue\n",
        "\n",
        "        event_id = {\"left\": event_dict[wanted_labels[0]], \"right\": event_dict[wanted_labels[1]]}\n",
        "        print(\"Using internal event codes:\", event_id, \"(for labels\", wanted_labels, \")\")\n",
        "\n",
        "        epochs = mne.Epochs(\n",
        "            raw,\n",
        "            events=events,\n",
        "            event_id=event_id,\n",
        "            tmin=tmin,\n",
        "            tmax=tmax,\n",
        "            baseline=baseline,\n",
        "            preload=True,\n",
        "            reject=None,\n",
        "            on_missing=\"warn\",\n",
        "            verbose=\"ERROR\",\n",
        "        )\n",
        "\n",
        "        if len(epochs) == 0:\n",
        "            print(f\"No epochs created for {gdf_path.name}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        data = epochs.get_data().astype(np.float32)  # (N, C, L)\n",
        "        labels = epochs.events[:, -1]  # internal codes (e.g., 4/5)\n",
        "\n",
        "        # Map internal codes to 0/1 consistently using event_id dict\n",
        "        code_to_idx = {int(event_id[\"left\"]): 0, int(event_id[\"right\"]): 1}\n",
        "        y = np.array([code_to_idx[int(c)] for c in labels], dtype=np.int64)\n",
        "\n",
        "        X_list.append(data)\n",
        "        y_list.append(y)\n",
        "        last_sfreq = int(raw.info[\"sfreq\"])\n",
        "\n",
        "    if len(X_list) == 0:\n",
        "        raise RuntimeError(\n",
        "            \"No epochs were created from any file. \"\n",
        "            \"Double-check dataset contents and wanted_labels.\"\n",
        "        )\n",
        "\n",
        "    X = np.concatenate(X_list, axis=0)\n",
        "    y = np.concatenate(y_list, axis=0)\n",
        "    return X, y, last_sfreq, global_event_ids\n",
        "\n",
        "# ---- Configure epoching ----\n",
        "RESAMPLE_HZ = 256\n",
        "TMIN, TMAX = 0.0, 4.0\n",
        "FILE_GLOB = \"*T.gdf\"  # good default for training runs\n",
        "\n",
        "X_np, y_np, sfreq, USED_LABELS = load_bciciv2b_epochs_by_labels(\n",
        "    root=DATA_DIR,\n",
        "    file_glob=FILE_GLOB,\n",
        "    tmin=TMIN,\n",
        "    tmax=TMAX,\n",
        "    resample_hz=RESAMPLE_HZ,\n",
        "    picks=\"eeg\",\n",
        "    baseline=None,\n",
        "    wanted_labels=(\"769\", \"770\"),\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "# Fix SEQ_LEN to be divisible by 16 (crop the last sample if needed)\n",
        "SEQ_LEN = X_np.shape[2]\n",
        "if SEQ_LEN % 16 != 0:\n",
        "    new_len = (SEQ_LEN // 16) * 16  # floor to nearest multiple of 16\n",
        "    print(f\"Cropping SEQ_LEN from {SEQ_LEN} -> {new_len} to satisfy architecture constraint.\")\n",
        "    X_np = X_np[:, :, :new_len]\n",
        "\n",
        "CHANNELS = X_np.shape[1]\n",
        "SEQ_LEN  = X_np.shape[2]\n",
        "print(\"CHANNELS:\", CHANNELS, \"SEQ_LEN:\", SEQ_LEN)\n",
        "assert SEQ_LEN % 16 == 0\n",
        "\n",
        "# print(\"\\nLoaded X:\", X_np.shape, \"y:\", y_np.shape, \"sfreq:\", sfreq)\n",
        "# CHANNELS = X_np.shape[1]\n",
        "# SEQ_LEN  = X_np.shape[2]\n",
        "# print(\"CHANNELS:\", CHANNELS, \"SEQ_LEN:\", SEQ_LEN, \"(must be divisible by 16)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "28db329c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28db329c",
        "outputId": "d788592e-d83c-44c2-b08c-1aa862f4b7ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batches: 57\n"
          ]
        }
      ],
      "source": [
        "# Cell 5 — Torch Dataset + Normalization\n",
        "class EEGTensorDataset(Dataset):\n",
        "    def __init__(self, X: np.ndarray, y: Optional[np.ndarray] = None, zscore_per_channel: bool = True):\n",
        "        \"\"\"\n",
        "        X: (N, C, L)\n",
        "        y: optional (N,)\n",
        "        zscore_per_channel:\n",
        "          - global per-channel mean/std computed across (N,L) for each channel\n",
        "        \"\"\"\n",
        "        assert X.ndim == 3\n",
        "        self.X = X.astype(np.float32)\n",
        "        self.y = None if y is None else y.astype(np.int64)\n",
        "\n",
        "        if zscore_per_channel:\n",
        "            mean = self.X.mean(axis=(0, 2), keepdims=True)\n",
        "            std  = self.X.std(axis=(0, 2), keepdims=True) + 1e-6\n",
        "            self.X = (self.X - mean) / std\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.X.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.from_numpy(self.X[idx])  # (C, L)\n",
        "        if self.y is None:\n",
        "            return x\n",
        "        return x, int(self.y[idx])\n",
        "\n",
        "# Unconditional GAN: we only use x; y is available if you want conditional later\n",
        "dataset = EEGTensorDataset(X_np, y=None, zscore_per_channel=True)\n",
        "loader = DataLoader(dataset, batch_size=64, shuffle=True, drop_last=True, num_workers=0, pin_memory=(DEVICE==\"cuda\"))\n",
        "print(\"Batches:\", len(loader))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "6eb65678",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eb65678",
        "outputId": "f8a7958d-acf8-4803-8b54-d9d4cfe5b37d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "G params: 4.918086 M\n",
            "D params: 0.725185 M\n"
          ]
        }
      ],
      "source": [
        "# Cell 6 — Models (Generator / Critic) + Init\n",
        "def weights_init(m):\n",
        "    if isinstance(m, (nn.Conv1d, nn.ConvTranspose1d, nn.Linear)):\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "        if m.bias is not None:\n",
        "            nn.init.constant_(m.bias.data, 0)\n",
        "\n",
        "class Generator1D(nn.Module):\n",
        "    def __init__(self, z_dim: int, out_channels: int, seq_len: int, base: int = 64):\n",
        "        super().__init__()\n",
        "        assert seq_len % 16 == 0\n",
        "        self.init_len = seq_len // 16\n",
        "        self.fc = nn.Linear(z_dim, base * 8 * self.init_len)\n",
        "        self.net = nn.Sequential(\n",
        "            nn.ConvTranspose1d(base * 8, base * 4, 4, 2, 1),\n",
        "            nn.BatchNorm1d(base * 4),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose1d(base * 4, base * 2, 4, 2, 1),\n",
        "            nn.BatchNorm1d(base * 2),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose1d(base * 2, base, 4, 2, 1),\n",
        "            nn.BatchNorm1d(base),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose1d(base, out_channels, 4, 2, 1),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        x = self.fc(z)\n",
        "        x = x.view(z.size(0), -1, self.init_len)\n",
        "        return self.net(x)\n",
        "\n",
        "class Critic1D(nn.Module):\n",
        "    def __init__(self, in_channels: int, seq_len: int, base: int = 64):\n",
        "        super().__init__()\n",
        "        assert seq_len % 16 == 0\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(in_channels, base, 4, 2, 1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv1d(base, base * 2, 4, 2, 1),\n",
        "            nn.InstanceNorm1d(base * 2, affine=True),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv1d(base * 2, base * 4, 4, 2, 1),\n",
        "            nn.InstanceNorm1d(base * 4, affine=True),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv1d(base * 4, base * 8, 4, 2, 1),\n",
        "            nn.InstanceNorm1d(base * 8, affine=True),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        )\n",
        "        self.out = nn.Linear(base * 8 * (seq_len // 16), 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.net(x)\n",
        "        h = h.view(x.size(0), -1)\n",
        "        return self.out(h).view(-1)  # (B,)\n",
        "\n",
        "Z_DIM = 128\n",
        "G = Generator1D(Z_DIM, CHANNELS, SEQ_LEN).to(DEVICE)\n",
        "D = Critic1D(CHANNELS, SEQ_LEN).to(DEVICE)\n",
        "G.apply(weights_init)\n",
        "D.apply(weights_init)\n",
        "\n",
        "print(\"G params:\", sum(p.numel() for p in G.parameters())/1e6, \"M\")\n",
        "print(\"D params:\", sum(p.numel() for p in D.parameters())/1e6, \"M\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "98ad3361",
      "metadata": {
        "id": "98ad3361"
      },
      "outputs": [],
      "source": [
        "# Cell 7 — WGAN-GP utilities (gradient penalty) + sampling\n",
        "def gradient_penalty(critic: nn.Module, real: torch.Tensor, fake: torch.Tensor) -> torch.Tensor:\n",
        "    bsz = real.size(0)\n",
        "    eps = torch.rand(bsz, 1, 1, device=real.device)\n",
        "    x_hat = eps * real + (1 - eps) * fake\n",
        "    x_hat.requires_grad_(True)\n",
        "    d_hat = critic(x_hat)\n",
        "\n",
        "    grads = torch.autograd.grad(\n",
        "        outputs=d_hat,\n",
        "        inputs=x_hat,\n",
        "        grad_outputs=torch.ones_like(d_hat),\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "        only_inputs=True,\n",
        "    )[0]\n",
        "    grads = grads.view(bsz, -1)\n",
        "    gp = ((grads.norm(2, dim=1) - 1.0) ** 2).mean()\n",
        "    return gp\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_generator(generator: nn.Module, n: int, z_dim: int) -> torch.Tensor:\n",
        "    z = torch.randn(n, z_dim, device=DEVICE)\n",
        "    return generator(z).cpu()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "b2c9c267",
      "metadata": {
        "id": "b2c9c267"
      },
      "outputs": [],
      "source": [
        "# Cell 8 — Adaptive-D Controller (EMPTY hooks)\n",
        "@dataclass\n",
        "class TrainState:\n",
        "    step: int = 0\n",
        "    epoch: int = 0\n",
        "    n_critic: int = 5\n",
        "    lambda_gp: float = 10.0\n",
        "\n",
        "    wasserstein_gap_ema: float = 0.0\n",
        "    ema_beta: float = 0.99\n",
        "\n",
        "    best_score: float = -1e18  # IMPORTANT: for best-checkpoint logic\n",
        "\n",
        "class AdaptiveDiscriminatorController:\n",
        "    def __init__(self):\n",
        "        # Put your adaptive variables / EMA buffers / thresholds here\n",
        "        pass\n",
        "\n",
        "    def on_batch_start(self, state: TrainState) -> None:\n",
        "        # TODO: optionally adjust state.n_critic / state.lambda_gp etc.\n",
        "        pass\n",
        "\n",
        "    def on_after_critic_update(self, state: TrainState, metrics: Dict[str, float], optim_D: torch.optim.Optimizer) -> None:\n",
        "        # TODO: optionally adjust critic optimizer hyperparams, freeze layers, etc.\n",
        "        pass\n",
        "\n",
        "    def on_after_generator_update(self, state: TrainState, metrics: Dict[str, float], optim_G: torch.optim.Optimizer) -> None:\n",
        "        # TODO: optionally adjust based on generator gradient norm (metrics[\"g_grad_norm\"])\n",
        "        pass\n",
        "\n",
        "controller = AdaptiveDiscriminatorController()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "6fcb008d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fcb008d",
        "outputId": "428d8b55-2c44-4541-caee-8dab5e13c095"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LR: 0.0001 BETAS: (0.0, 0.9) n_critic: 5 lambda_gp: 10.0\n"
          ]
        }
      ],
      "source": [
        "# Cell 9 — Optimizers + checkpoint utils + history init\n",
        "LR = 1e-4\n",
        "BETAS = (0.0, 0.9)\n",
        "optim_G = torch.optim.Adam(G.parameters(), lr=LR, betas=BETAS)\n",
        "optim_D = torch.optim.Adam(D.parameters(), lr=LR, betas=BETAS)\n",
        "\n",
        "state = TrainState(step=0, epoch=0, n_critic=5, lambda_gp=10.0)\n",
        "\n",
        "CKPT_DIR = Path(\"./checkpoints\")\n",
        "CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "history = defaultdict(list)\n",
        "\n",
        "def save_checkpoint(path: str, G, D, optim_G, optim_D, state: TrainState, history: dict):\n",
        "    payload = {\n",
        "        \"G\": G.state_dict(),\n",
        "        \"D\": D.state_dict(),\n",
        "        \"optim_G\": optim_G.state_dict(),\n",
        "        \"optim_D\": optim_D.state_dict(),\n",
        "        \"state\": state.__dict__,\n",
        "        \"history\": dict(history),\n",
        "    }\n",
        "    torch.save(payload, path)\n",
        "\n",
        "def load_checkpoint(path: str, G, D, optim_G=None, optim_D=None) -> Tuple[TrainState, dict]:\n",
        "    ckpt = torch.load(path, map_location=DEVICE)\n",
        "    G.load_state_dict(ckpt[\"G\"])\n",
        "    D.load_state_dict(ckpt[\"D\"])\n",
        "    if optim_G is not None and \"optim_G\" in ckpt:\n",
        "        optim_G.load_state_dict(ckpt[\"optim_G\"])\n",
        "    if optim_D is not None and \"optim_D\" in ckpt:\n",
        "        optim_D.load_state_dict(ckpt[\"optim_D\"])\n",
        "    state = TrainState(**ckpt[\"state\"])\n",
        "    hist = ckpt.get(\"history\", {})\n",
        "    return state, hist\n",
        "\n",
        "print(\"LR:\", LR, \"BETAS:\", BETAS, \"n_critic:\", state.n_critic, \"lambda_gp:\", state.lambda_gp)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "777533e9",
      "metadata": {
        "id": "777533e9"
      },
      "outputs": [],
      "source": [
        "# Cell 10 — Training Loop (Standard WGAN-GP + Adaptive Hooks + Saving)\n",
        "def compute_grad_norm(parameters) -> float:\n",
        "    total = 0.0\n",
        "    for p in parameters:\n",
        "        if p.grad is None:\n",
        "            continue\n",
        "        param_norm = p.grad.detach().data.norm(2).item()\n",
        "        total += param_norm ** 2\n",
        "    return float(total ** 0.5)\n",
        "\n",
        "def train_wgan_gp(\n",
        "    loader: DataLoader,\n",
        "    G: nn.Module,\n",
        "    D: nn.Module,\n",
        "    optim_G: torch.optim.Optimizer,\n",
        "    optim_D: torch.optim.Optimizer,\n",
        "    state: TrainState,\n",
        "    controller: AdaptiveDiscriminatorController,\n",
        "    history: dict,\n",
        "    z_dim: int,\n",
        "    epochs: int = 10,\n",
        "    log_every: int = 50,\n",
        "    save_every_steps: int = 500,\n",
        "    best_metric: str = \"gap_ema\",  # \"gap_ema\" (default) or \"gap\" etc.\n",
        "):\n",
        "    G.train(); D.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        state.epoch = epoch\n",
        "\n",
        "        for batch_idx, real in enumerate(loader):\n",
        "            state.step += 1\n",
        "            controller.on_batch_start(state)\n",
        "\n",
        "            real = real.to(DEVICE)\n",
        "            bsz = real.size(0)\n",
        "\n",
        "            # -------------------------\n",
        "            # Critic updates (n_critic)\n",
        "            # -------------------------\n",
        "            metrics_D = None\n",
        "            for _ in range(state.n_critic):\n",
        "                z = torch.randn(bsz, z_dim, device=DEVICE)\n",
        "                fake = G(z).detach()\n",
        "\n",
        "                d_real = D(real).mean()\n",
        "                d_fake = D(fake).mean()\n",
        "                gap = (d_real - d_fake).item()\n",
        "\n",
        "                gp = gradient_penalty(D, real, fake)\n",
        "                loss_D = (d_fake - d_real) + state.lambda_gp * gp\n",
        "\n",
        "                optim_D.zero_grad(set_to_none=True)\n",
        "                loss_D.backward()\n",
        "                optim_D.step()\n",
        "\n",
        "                # EMA of the gap (useful for adaptive control)\n",
        "                state.wasserstein_gap_ema = state.ema_beta * state.wasserstein_gap_ema + (1 - state.ema_beta) * gap\n",
        "\n",
        "                metrics_D = {\n",
        "                    \"d_real\": float(d_real.item()),\n",
        "                    \"d_fake\": float(d_fake.item()),\n",
        "                    \"gap\": float(gap),\n",
        "                    \"gap_ema\": float(state.wasserstein_gap_ema),\n",
        "                    \"gp\": float(gp.item()),\n",
        "                    \"loss_D\": float(loss_D.item()),\n",
        "                }\n",
        "                controller.on_after_critic_update(state, metrics_D, optim_D)\n",
        "\n",
        "            # -------------------------\n",
        "            # Generator update\n",
        "            # -------------------------\n",
        "            z = torch.randn(bsz, z_dim, device=DEVICE)\n",
        "            fake = G(z)\n",
        "            loss_G = -D(fake).mean()\n",
        "\n",
        "            optim_G.zero_grad(set_to_none=True)\n",
        "            loss_G.backward()\n",
        "\n",
        "            # Compute Generator grad norm (optional adaptive signal)\n",
        "            g_grad_norm = compute_grad_norm(G.parameters())\n",
        "\n",
        "            optim_G.step()\n",
        "\n",
        "            metrics_G = {\"loss_G\": float(loss_G.item()), \"g_grad_norm\": float(g_grad_norm)}\n",
        "            controller.on_after_generator_update(state, metrics_G, optim_G)\n",
        "\n",
        "            # -------------------------\n",
        "            # Log + store history\n",
        "            # -------------------------\n",
        "            if metrics_D is not None:\n",
        "                history[\"step\"].append(int(state.step))\n",
        "                history[\"epoch\"].append(int(state.epoch))\n",
        "                history[\"loss_D\"].append(float(metrics_D[\"loss_D\"]))\n",
        "                history[\"loss_G\"].append(float(metrics_G[\"loss_G\"]))\n",
        "                history[\"d_real\"].append(float(metrics_D[\"d_real\"]))\n",
        "                history[\"d_fake\"].append(float(metrics_D[\"d_fake\"]))\n",
        "                history[\"gap\"].append(float(metrics_D[\"gap\"]))\n",
        "                history[\"gap_ema\"].append(float(metrics_D[\"gap_ema\"]))\n",
        "                history[\"gp\"].append(float(metrics_D[\"gp\"]))\n",
        "                history[\"g_grad_norm\"].append(float(metrics_G[\"g_grad_norm\"]))\n",
        "                history[\"n_critic\"].append(int(state.n_critic))\n",
        "                history[\"lambda_gp\"].append(float(state.lambda_gp))\n",
        "\n",
        "            if (batch_idx % log_every) == 0 and metrics_D is not None:\n",
        "                print(\n",
        "                    f\"[Epoch {epoch:03d}/{epochs:03d}] [Batch {batch_idx:04d}/{len(loader):04d}] \"\n",
        "                    f\"[n_critic: {state.n_critic}] \"\n",
        "                    f\"[gap: {metrics_D['gap']:+.3f} | ema: {metrics_D['gap_ema']:+.3f}] \"\n",
        "                    f\"[GP: {metrics_D['gp']:.3f}] \"\n",
        "                    f\"[g|grad|: {metrics_G['g_grad_norm']:.3f}] \"\n",
        "                    f\"[D: {metrics_D['loss_D']:+.3f}] [G: {metrics_G['loss_G']:+.3f}]\"\n",
        "                )\n",
        "\n",
        "            # -------------------------\n",
        "            # Save periodic checkpoint\n",
        "            # -------------------------\n",
        "            if save_every_steps > 0 and (state.step % save_every_steps) == 0:\n",
        "                ckpt_path = CKPT_DIR / f\"ckpt_step_{state.step}.pt\"\n",
        "                save_checkpoint(str(ckpt_path), G, D, optim_G, optim_D, state, history)\n",
        "                print(\"Saved:\", ckpt_path)\n",
        "\n",
        "            # -------------------------\n",
        "            # Save \"best\" checkpoint\n",
        "            # -------------------------\n",
        "            if metrics_D is not None:\n",
        "                score = float(metrics_D.get(best_metric, -1e18))\n",
        "                if score > float(state.best_score):\n",
        "                    state.best_score = float(score)\n",
        "                    best_path = CKPT_DIR / \"best.pt\"\n",
        "                    save_checkpoint(str(best_path), G, D, optim_G, optim_D, state, history)\n",
        "                    print(f\"New BEST ({best_metric}={score:.4f}) -> saved to {best_path}\")\n",
        "\n",
        "    return state, history\n",
        "\n",
        "# ---- Run training ----\n",
        "state, history = train_wgan_gp(\n",
        "    loader=loader,\n",
        "    G=G,\n",
        "    D=D,\n",
        "    optim_G=optim_G,\n",
        "    optim_D=optim_D,\n",
        "    state=state,\n",
        "    controller=controller,\n",
        "    history=history,\n",
        "    z_dim=Z_DIM,\n",
        "    epochs=2,\n",
        "    log_every=50,\n",
        "    save_every_steps=500,\n",
        "    best_metric=\"gap_ema\",\n",
        ")\n",
        "\n",
        "# Optional — Save final generator/critic weights (inference-only)\n",
        "torch.save(G.state_dict(), \"generator_final.pt\")\n",
        "torch.save(D.state_dict(), \"critic_final.pt\")\n",
        "print(\"Saved inference-only weights: generator_final.pt, critic_final.pt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7003f0e1",
      "metadata": {
        "id": "7003f0e1"
      },
      "outputs": [],
      "source": [
        "# Cell 11 — Resume from a checkpoint (example)\n",
        "# Uncomment to resume:\n",
        "# ckpt_path = \"./checkpoints/best.pt\"   # or \"./checkpoints/ckpt_step_500.pt\"\n",
        "# state, loaded_hist = load_checkpoint(ckpt_path, G, D, optim_G, optim_D)\n",
        "# # If you want to continue appending history:\n",
        "# history = defaultdict(list, loaded_hist)\n",
        "# print(\"Resumed at step:\", state.step, \"epoch:\", state.epoch, \"best_score:\", state.best_score)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3521cd7d",
      "metadata": {
        "id": "3521cd7d"
      },
      "outputs": [],
      "source": [
        "# Cell 12 — Training Curve Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def _safe_get(history: dict, key: str):\n",
        "    v = history.get(key, None)\n",
        "    if v is None:\n",
        "        return None\n",
        "    return list(v)\n",
        "\n",
        "def plot_history(history: dict):\n",
        "    keys = list(history.keys())\n",
        "    if len(keys) == 0:\n",
        "        print(\"History is empty. Train first.\")\n",
        "        return\n",
        "\n",
        "    step = _safe_get(history, \"step\")\n",
        "    if step is None:\n",
        "        # fallback to index\n",
        "        step = list(range(len(next(iter(history.values())))))\n",
        "\n",
        "    plt.figure(figsize=(16, 12))\n",
        "\n",
        "    # Losses\n",
        "    plt.subplot(3, 2, 1)\n",
        "    if \"loss_G\" in history: plt.plot(step, history[\"loss_G\"], label=\"loss_G\")\n",
        "    if \"loss_D\" in history: plt.plot(step, history[\"loss_D\"], label=\"loss_D\")\n",
        "    plt.title(\"Generator / Critic Loss\")\n",
        "    plt.xlabel(\"step\"); plt.ylabel(\"loss\")\n",
        "    plt.legend(); plt.grid(True)\n",
        "\n",
        "    # Wasserstein gap\n",
        "    plt.subplot(3, 2, 2)\n",
        "    if \"gap\" in history: plt.plot(step, history[\"gap\"], label=\"gap\")\n",
        "    if \"gap_ema\" in history: plt.plot(step, history[\"gap_ema\"], label=\"gap_ema\")\n",
        "    plt.title(\"Wasserstein Gap\")\n",
        "    plt.xlabel(\"step\"); plt.ylabel(\"E[D(real)] - E[D(fake)]\")\n",
        "    plt.legend(); plt.grid(True)\n",
        "\n",
        "    # Gradient penalty\n",
        "    plt.subplot(3, 2, 3)\n",
        "    if \"gp\" in history: plt.plot(step, history[\"gp\"], label=\"gp\")\n",
        "    plt.title(\"Gradient Penalty\")\n",
        "    plt.xlabel(\"step\"); plt.ylabel(\"gp\")\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Generator gradient norm\n",
        "    plt.subplot(3, 2, 4)\n",
        "    if \"g_grad_norm\" in history: plt.plot(step, history[\"g_grad_norm\"], label=\"||∇G||\")\n",
        "    plt.title(\"Generator Gradient Norm\")\n",
        "    plt.xlabel(\"step\"); plt.ylabel(\"L2 norm\")\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Critic outputs (optional)\n",
        "    plt.subplot(3, 2, 5)\n",
        "    if \"d_real\" in history: plt.plot(step, history[\"d_real\"], label=\"D(real)\")\n",
        "    if \"d_fake\" in history: plt.plot(step, history[\"d_fake\"], label=\"D(fake)\")\n",
        "    plt.title(\"Critic Outputs\")\n",
        "    plt.xlabel(\"step\"); plt.ylabel(\"score\")\n",
        "    plt.legend(); plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_history(history)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
