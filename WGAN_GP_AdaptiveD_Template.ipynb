{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd20fb5c",
   "metadata": {},
   "source": [
    "# WGAN-GP (Standard) + Adaptive-Discriminator Hooks (Template)\n",
    "\n",
    "This notebook provides:\n",
    "- A **standard WGAN-GP** training loop (critic + gradient penalty).\n",
    "- A **pluggable \"Adaptive Discriminator\" controller** with clearly marked hooks so you can experiment with different discriminator/critic adjustment strategies (e.g., dynamic `n_critic`, LR, GP weight, architecture toggles, etc.).\n",
    "\n",
    "> Notes: WGAN-GP uses a gradient penalty to enforce the 1-Lipschitz constraint instead of weight clipping (see WGAN-GP objective in the referenced survey paper). fileciteturn0file0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a012ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 — Imports & Reproducibility\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Dict, Any, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(42)\n",
    "print(\"Device:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafee5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 — Simple 1D Dataset Placeholder (Replace with your EEG loader)\n",
    "# Expect samples shaped as (C, L) where:\n",
    "#   C = number of channels (e.g., EEG electrodes or feature channels)\n",
    "#   L = sequence length (time points)\n",
    "#\n",
    "# Replace this with your real EEG dataset.\n",
    "class RandomEEGDataset(Dataset):\n",
    "    def __init__(self, n_samples: int = 10_000, channels: int = 8, length: int = 256):\n",
    "        self.n = n_samples\n",
    "        self.c = channels\n",
    "        self.l = length\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Random \"EEG-like\" noise as a placeholder\n",
    "        x = torch.randn(self.c, self.l)\n",
    "        return x\n",
    "\n",
    "# --- Configure data shape here ---\n",
    "CHANNELS = 8\n",
    "SEQ_LEN  = 256\n",
    "\n",
    "dataset = RandomEEGDataset(n_samples=5000, channels=CHANNELS, length=SEQ_LEN)\n",
    "loader  = DataLoader(dataset, batch_size=64, shuffle=True, drop_last=True, num_workers=0, pin_memory=(DEVICE==\"cuda\"))\n",
    "print(\"Batches:\", len(loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff7118a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 — Generator & Critic (1D Conv, Standard WGAN-GP Style)\n",
    "# This is a *standard* WGAN-GP setup: the \"discriminator\" is a *critic* with a linear output (no sigmoid).\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, (nn.Conv1d, nn.ConvTranspose1d, nn.Linear)):\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "class Generator1D(nn.Module):\n",
    "    def __init__(self, z_dim: int, out_channels: int, seq_len: int, base: int = 64):\n",
    "        super().__init__()\n",
    "        assert seq_len % 16 == 0, \"For this template, seq_len should be divisible by 16.\"\n",
    "        self.z_dim = z_dim\n",
    "        self.out_channels = out_channels\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        # Project noise to a small temporal resolution then upsample x16 via ConvTranspose1d\n",
    "        self.init_len = seq_len // 16\n",
    "        self.fc = nn.Linear(z_dim, base * 8 * self.init_len)\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.ConvTranspose1d(base * 8, base * 4, kernel_size=4, stride=2, padding=1),  # x2\n",
    "            nn.BatchNorm1d(base * 4),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose1d(base * 4, base * 2, kernel_size=4, stride=2, padding=1),  # x4\n",
    "            nn.BatchNorm1d(base * 2),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose1d(base * 2, base, kernel_size=4, stride=2, padding=1),      # x8\n",
    "            nn.BatchNorm1d(base),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose1d(base, out_channels, kernel_size=4, stride=2, padding=1),  # x16\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.fc(z)\n",
    "        x = x.view(z.size(0), -1, self.init_len)\n",
    "        x = self.net(x)\n",
    "        return x\n",
    "\n",
    "class Critic1D(nn.Module):\n",
    "    def __init__(self, in_channels: int, seq_len: int, base: int = 64):\n",
    "        super().__init__()\n",
    "        assert seq_len % 16 == 0, \"For this template, seq_len should be divisible by 16.\"\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, base, kernel_size=4, stride=2, padding=1),   # /2\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv1d(base, base * 2, kernel_size=4, stride=2, padding=1),      # /4\n",
    "            nn.InstanceNorm1d(base * 2, affine=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv1d(base * 2, base * 4, kernel_size=4, stride=2, padding=1),  # /8\n",
    "            nn.InstanceNorm1d(base * 4, affine=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv1d(base * 4, base * 8, kernel_size=4, stride=2, padding=1),  # /16\n",
    "            nn.InstanceNorm1d(base * 8, affine=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "        self.out = nn.Linear(base * 8 * (seq_len // 16), 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.net(x)\n",
    "        h = h.view(x.size(0), -1)\n",
    "        return self.out(h).view(-1)\n",
    "\n",
    "# --- Model configs ---\n",
    "Z_DIM = 128\n",
    "\n",
    "G = Generator1D(z_dim=Z_DIM, out_channels=CHANNELS, seq_len=SEQ_LEN).to(DEVICE)\n",
    "D = Critic1D(in_channels=CHANNELS, seq_len=SEQ_LEN).to(DEVICE)\n",
    "\n",
    "G.apply(weights_init)\n",
    "D.apply(weights_init)\n",
    "\n",
    "print(\"G params:\", sum(p.numel() for p in G.parameters())/1e6, \"M\")\n",
    "print(\"D params:\", sum(p.numel() for p in D.parameters())/1e6, \"M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2143adad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 — WGAN-GP Utilities (Gradient Penalty, Losses)\n",
    "# WGAN-GP objective uses:\n",
    "#   loss_D = E[D(fake)] - E[D(real)] + lambda_gp * (||∇_x_hat D(x_hat)||_2 - 1)^2\n",
    "#   loss_G = -E[D(fake)]\n",
    "# This matches the standard WGAN-GP formulation referenced in the survey paper. fileciteturn0file0\n",
    "\n",
    "def gradient_penalty(critic: nn.Module, real: torch.Tensor, fake: torch.Tensor) -> torch.Tensor:\n",
    "    bsz = real.size(0)\n",
    "    eps = torch.rand(bsz, 1, 1, device=real.device)\n",
    "    x_hat = eps * real + (1 - eps) * fake\n",
    "    x_hat.requires_grad_(True)\n",
    "\n",
    "    d_hat = critic(x_hat)\n",
    "    grads = torch.autograd.grad(\n",
    "        outputs=d_hat,\n",
    "        inputs=x_hat,\n",
    "        grad_outputs=torch.ones_like(d_hat),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )[0]\n",
    "    grads = grads.view(bsz, -1)\n",
    "    gp = ((grads.norm(2, dim=1) - 1.0) ** 2).mean()\n",
    "    return gp\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_generator(generator: nn.Module, n: int, z_dim: int) -> torch.Tensor:\n",
    "    z = torch.randn(n, z_dim, device=DEVICE)\n",
    "    return generator(z).cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b187349a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 — Adaptive Discriminator Controller (Hooks / Empty Spots)\n",
    "# This is the *intended place* to experiment with \"Adaptive D\" logic.\n",
    "# You can:\n",
    "#   - change n_critic dynamically\n",
    "#   - change critic LR / betas\n",
    "#   - change lambda_gp\n",
    "#   - freeze/unfreeze layers\n",
    "#   - swap architectures\n",
    "#   - etc.\n",
    "#\n",
    "# IMPORTANT: The default implementation below does *nothing* (standard WGAN-GP behavior).\n",
    "# You should only edit the \"TODO\" sections.\n",
    "\n",
    "@dataclass\n",
    "class TrainState:\n",
    "    step: int = 0\n",
    "    epoch: int = 0\n",
    "    n_critic: int = 5\n",
    "    lambda_gp: float = 10.0\n",
    "\n",
    "    # Common diagnostics you may want to use\n",
    "    wasserstein_gap_ema: float = 0.0\n",
    "    ema_beta: float = 0.99\n",
    "\n",
    "class AdaptiveDiscriminatorController:\n",
    "    def __init__(self):\n",
    "        # Store any stateful variables for your adaptive method here\n",
    "        pass\n",
    "\n",
    "    def on_batch_start(self, state: TrainState) -> None:\n",
    "        # TODO (optional): adjust settings at the start of each batch\n",
    "        # Example knobs:\n",
    "        #   state.n_critic = ...\n",
    "        #   state.lambda_gp = ...\n",
    "        pass\n",
    "\n",
    "    def on_after_critic_update(self, state: TrainState, metrics: Dict[str, float], optim_D: torch.optim.Optimizer) -> None:\n",
    "        # TODO (optional): react after each critic update\n",
    "        # You can:\n",
    "        #   - change optimizer learning rate: for g in optim_D.param_groups: g[\"lr\"] = ...\n",
    "        #   - change betas / weight decay\n",
    "        #   - freeze layers based on metrics\n",
    "        pass\n",
    "\n",
    "    def on_after_generator_update(self, state: TrainState, metrics: Dict[str, float], optim_G: torch.optim.Optimizer) -> None:\n",
    "        # TODO (optional): react after generator update\n",
    "        pass\n",
    "\n",
    "controller = AdaptiveDiscriminatorController()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429ebaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6 — Optimizers & Hyperparameters (Standard WGAN-GP Defaults)\n",
    "LR = 1e-4\n",
    "BETAS = (0.0, 0.9)   # standard WGAN-GP choice\n",
    "\n",
    "optim_G = torch.optim.Adam(G.parameters(), lr=LR, betas=BETAS)\n",
    "optim_D = torch.optim.Adam(D.parameters(), lr=LR, betas=BETAS)\n",
    "\n",
    "state = TrainState(step=0, epoch=0, n_critic=5, lambda_gp=10.0)\n",
    "\n",
    "print(\"LR:\", LR, \"BETAS:\", BETAS, \"n_critic:\", state.n_critic, \"lambda_gp:\", state.lambda_gp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db72a7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7 — Training Loop (Standard WGAN-GP + Adaptive Hooks)\n",
    "# Logs:\n",
    "#   - wasserstein gap: E[D(real)] - E[D(fake)]  (higher generally means critic separates better)\n",
    "#   - gp: gradient penalty term\n",
    "#   - losses for D and G\n",
    "\n",
    "def train_wgan_gp(\n",
    "    loader: DataLoader,\n",
    "    G: nn.Module,\n",
    "    D: nn.Module,\n",
    "    optim_G: torch.optim.Optimizer,\n",
    "    optim_D: torch.optim.Optimizer,\n",
    "    state: TrainState,\n",
    "    controller: AdaptiveDiscriminatorController,\n",
    "    z_dim: int,\n",
    "    epochs: int = 10,\n",
    "    log_every: int = 50,\n",
    "):\n",
    "    G.train(); D.train()\n",
    "    for epoch in range(epochs):\n",
    "        state.epoch = epoch\n",
    "        for batch_idx, real in enumerate(loader):\n",
    "            state.step += 1\n",
    "            controller.on_batch_start(state)\n",
    "\n",
    "            real = real.to(DEVICE)\n",
    "            bsz = real.size(0)\n",
    "\n",
    "            # -------------------------\n",
    "            # Critic updates (n_critic)\n",
    "            # -------------------------\n",
    "            for _ in range(state.n_critic):\n",
    "                z = torch.randn(bsz, z_dim, device=DEVICE)\n",
    "                fake = G(z).detach()\n",
    "\n",
    "                d_real = D(real).mean()\n",
    "                d_fake = D(fake).mean()\n",
    "                gap = (d_real - d_fake).item()  # Wasserstein gap estimate\n",
    "\n",
    "                gp = gradient_penalty(D, real, fake)\n",
    "                loss_D = (d_fake - d_real) + state.lambda_gp * gp\n",
    "\n",
    "                optim_D.zero_grad(set_to_none=True)\n",
    "                loss_D.backward()\n",
    "                optim_D.step()\n",
    "\n",
    "                # Update EMA of the gap (useful for adaptive control)\n",
    "                state.wasserstein_gap_ema = state.ema_beta * state.wasserstein_gap_ema + (1 - state.ema_beta) * gap\n",
    "\n",
    "                metrics_D = {\n",
    "                    \"d_real\": float(d_real.item()),\n",
    "                    \"d_fake\": float(d_fake.item()),\n",
    "                    \"gap\": float(gap),\n",
    "                    \"gap_ema\": float(state.wasserstein_gap_ema),\n",
    "                    \"gp\": float(gp.item()),\n",
    "                    \"loss_D\": float(loss_D.item()),\n",
    "                }\n",
    "                controller.on_after_critic_update(state, metrics_D, optim_D)\n",
    "\n",
    "            # -------------------------\n",
    "            # Generator update\n",
    "            # -------------------------\n",
    "            z = torch.randn(bsz, z_dim, device=DEVICE)\n",
    "            fake = G(z)\n",
    "            loss_G = -D(fake).mean()\n",
    "\n",
    "            optim_G.zero_grad(set_to_none=True)\n",
    "            loss_G.backward()\n",
    "            optim_G.step()\n",
    "\n",
    "            metrics_G = {\"loss_G\": float(loss_G.item())}\n",
    "            controller.on_after_generator_update(state, metrics_G, optim_G)\n",
    "\n",
    "            if (batch_idx % log_every) == 0:\n",
    "                print(\n",
    "                    f\"[Epoch {epoch:03d}/{epochs:03d}] [Batch {batch_idx:04d}/{len(loader):04d}] \"\n",
    "                    f\"[n_critic: {state.n_critic}] \"\n",
    "                    f\"[gap: {metrics_D['gap']:+.3f} | ema: {metrics_D['gap_ema']:+.3f}] \"\n",
    "                    f\"[GP: {metrics_D['gp']:.3f}] \"\n",
    "                    f\"[D: {metrics_D['loss_D']:+.3f}] [G: {metrics_G['loss_G']:+.3f}]\"\n",
    "                )\n",
    "\n",
    "train_wgan_gp(loader, G, D, optim_G, optim_D, state, controller, z_dim=Z_DIM, epochs=2, log_every=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd61a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8 — Quick Sanity Sample (Optional)\n",
    "# This just samples a few generated sequences so you can inspect shapes.\n",
    "with torch.no_grad():\n",
    "    fake = sample_generator(G, n=4, z_dim=Z_DIM)\n",
    "print(\"Generated batch shape:\", tuple(fake.shape))  # (N, C, L)\n",
    "print(\"Example stats:\", fake.mean().item(), fake.std().item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
