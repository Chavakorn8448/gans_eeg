{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd20fb5c",
   "metadata": {},
   "source": [
    "# WGAN-GP (Standard) + Adaptive-Discriminator Hooks (Template)\n",
    "\n",
    "This notebook provides:\n",
    "- A **standard WGAN-GP** training loop (critic + gradient penalty).\n",
    "- A **pluggable \"Adaptive Discriminator\" controller** with clearly marked hooks so you can experiment with different discriminator/critic adjustment strategies (e.g., dynamic `n_critic`, LR, GP weight, architecture toggles, etc.).\n",
    "\n",
    "> Notes: WGAN-GP uses a gradient penalty to enforce the 1-Lipschitz constraint instead of weight clipping (see WGAN-GP objective in the referenced survey paper). ÓàÄfileciteÓàÇturn0file0ÓàÅ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "45a012ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 ‚Äî Imports & Reproducibility\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Dict, Any, Tuple\n",
    "from pathlib import Path\n",
    "import mne\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(42)\n",
    "print(\"Device:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2b49240c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models(tag_dir: Path, run_tag: str, G: nn.Module, D: nn.Module, history: Dict[str, list]):\n",
    "    # Generator only (for generating new samples)\n",
    "    generator_path = tag_dir / f\"generator_{run_tag}.pt\"\n",
    "    torch.save(G.state_dict(), generator_path)\n",
    "    print(f\"‚úÖ Saved Generator: {generator_path}\")\n",
    "\n",
    "    # Critic only (for evaluation if needed)\n",
    "    critic_path = tag_dir / f\"critic_{run_tag}.pt\"\n",
    "    torch.save(D.state_dict(), critic_path)\n",
    "    print(f\"‚úÖ Saved Critic: {critic_path}\")\n",
    "\n",
    "    # Save training history as well\n",
    "    import json\n",
    "    history_path = tag_dir / f\"history_{run_tag}.json\"\n",
    "    with open(history_path, \"w\") as f:\n",
    "        json.dump(history, f)\n",
    "    print(f\"‚úÖ Saved history: {history_path}\")\n",
    "\n",
    "    print(f\"\\nüìÅ All models saved to: {tag_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "677f5aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history: dict, TAG_DIR: Path , RUN_TAG: str = \"default\"):\n",
    "    if len(history.get(\"step\", [])) == 0:\n",
    "        print(\"History is empty. Train first.\")\n",
    "        return\n",
    "\n",
    "    step = history[\"step\"]\n",
    "    fig, axes = plt.subplots(4, 2, figsize=(16, 12))\n",
    "\n",
    "    # Losses\n",
    "    axes[0, 0].plot(step, history[\"loss_G\"], label=\"loss_G\", alpha=0.8)\n",
    "    axes[0, 0].plot(step, history[\"loss_D\"], label=\"loss_D\", alpha=0.8)\n",
    "    axes[0, 0].set_title(\"Generator / Critic Loss\")\n",
    "    axes[0, 0].set_xlabel(\"step\")\n",
    "    axes[0, 0].set_ylabel(\"loss\")\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True)\n",
    "\n",
    "    # Wasserstein gap\n",
    "    axes[0, 1].plot(step, history[\"gap\"], label=\"gap\", alpha=0.5)\n",
    "    axes[0, 1].plot(step, history[\"gap_ema\"], label=\"gap_ema\", linewidth=2)\n",
    "    axes[0, 1].set_title(\"Wasserstein Gap\")\n",
    "    axes[0, 1].set_xlabel(\"step\")\n",
    "    axes[0, 1].set_ylabel(\"E[D(real)] - E[D(fake)]\")\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True)\n",
    "\n",
    "    # Gradient Penalty\n",
    "    axes[1, 0].plot(step, history[\"gp\"], label=\"GP\", color=\"orange\")\n",
    "    axes[1, 0].set_title(\"Gradient Penalty\")\n",
    "    axes[1, 0].set_xlabel(\"step\")\n",
    "    axes[1, 0].set_ylabel(\"gp\")\n",
    "    axes[1, 0].grid(True)\n",
    "\n",
    "    # Gradient Direction Consistency (Cosine Similarity)\n",
    "    axes[1, 1].plot(step, history[\"d_grad_cos_sim\"], label=\"D grad cos_sim\", alpha=0.8)\n",
    "    axes[1, 1].plot(step, history[\"g_grad_cos_sim\"], label=\"G grad cos_sim\", alpha=0.8)\n",
    "    axes[1, 1].axhline(y=0.9, color='r', linestyle='--', alpha=0.5, label='high threshold')\n",
    "    axes[1, 1].axhline(y=0.3, color='b', linestyle='--', alpha=0.5, label='low threshold')\n",
    "    axes[1, 1].set_title(\"Gradient Direction Consistency (Cosine Similarity)\")\n",
    "    axes[1, 1].set_xlabel(\"step\")\n",
    "    axes[1, 1].set_ylabel(\"cosine similarity\")\n",
    "    axes[1, 1].set_ylim(-1.1, 1.1)\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True)\n",
    "\n",
    "    # n_critic over time - auto-scale with padding\n",
    "    n_critic_data = history[\"n_critic\"]\n",
    "    axes[2, 0].plot(step, n_critic_data, label=\"n_critic\", color=\"green\", drawstyle='steps-post', linewidth=2)\n",
    "    axes[2, 0].set_title(\"Adaptive n_critic\")\n",
    "    axes[2, 0].set_xlabel(\"step\")\n",
    "    axes[2, 0].set_ylabel(\"n_critic\")\n",
    "    # Auto-scale with padding to make constant lines visible\n",
    "    n_min, n_max = min(n_critic_data), max(n_critic_data)\n",
    "    if n_min == n_max:  # Constant line case\n",
    "        axes[2, 0].set_ylim(n_min - 1, n_max + 1)\n",
    "        axes[2, 0].axhline(y=n_min, color='green', linestyle='-', alpha=0.3, linewidth=10)  # Highlight flat line\n",
    "    else:\n",
    "        axes[2, 0].set_ylim(max(0, n_min - 1), n_max + 1)\n",
    "    axes[2, 0].grid(True)\n",
    "    axes[2, 0].legend()\n",
    "\n",
    "    # Combined: D cosine sim vs n_critic\n",
    "    ax2 = axes[2, 1]\n",
    "    ax2.plot(step, history[\"d_grad_cos_sim\"], label=\"D grad cos_sim\", color=\"blue\", alpha=0.7)\n",
    "    ax2.set_xlabel(\"step\")\n",
    "    ax2.set_ylabel(\"D grad cosine similarity\", color=\"blue\")\n",
    "    ax2.tick_params(axis='y', labelcolor=\"blue\")\n",
    "    ax2.set_ylim(-1.1, 1.1)\n",
    "    \n",
    "    ax2_twin = ax2.twinx()\n",
    "    ax2_twin.plot(step, n_critic_data, label=\"n_critic\", color=\"green\", alpha=0.7, drawstyle='steps-post', linewidth=2)\n",
    "    ax2_twin.set_ylabel(\"n_critic\", color=\"green\")\n",
    "    ax2_twin.tick_params(axis='y', labelcolor=\"green\")\n",
    "    if n_min == n_max:\n",
    "        ax2_twin.set_ylim(n_min - 1, n_max + 1)\n",
    "    else:\n",
    "        ax2_twin.set_ylim(max(0, n_min - 1), n_max + 1)\n",
    "    \n",
    "    axes[2, 1].set_title(\"D Gradient Consistency vs n_critic\")\n",
    "    axes[2, 1].grid(True)\n",
    "    \n",
    "    # n_gen over time - auto-scale with padding\n",
    "    n_gen_data = history[\"n_gen\"]\n",
    "    axes[3, 0].plot(step, n_gen_data, label=\"n_gen\", color=\"purple\", drawstyle='steps-post', linewidth=2)\n",
    "    axes[3, 0].set_title(\"Adaptive n_gen\")\n",
    "    axes[3, 0].set_xlabel(\"step\")\n",
    "    axes[3, 0].set_ylabel(\"n_gen\")\n",
    "    g_min, g_max = min(n_gen_data), max(n_gen_data)\n",
    "    if g_min == g_max:  # Constant line case\n",
    "        axes[3, 0].set_ylim(g_min - 1, g_max + 1)\n",
    "        axes[3, 0].axhline(y=g_min, color='purple', linestyle='-', alpha=0.3, linewidth=10)\n",
    "    else:\n",
    "        axes[3, 0].set_ylim(max(0, g_min - 1), g_max + 1)\n",
    "    axes[3, 0].grid(True)\n",
    "    axes[3, 0].legend()\n",
    "    \n",
    "    # Combined: G cosine sim vs n_gen\n",
    "    ax4 = axes[3, 1]\n",
    "    ax4.plot(step, history[\"g_grad_cos_sim\"], label=\"G grad cos_sim\", color=\"blue\", alpha=0.7)\n",
    "    ax4.set_xlabel(\"step\")\n",
    "    ax4.set_ylabel(\"G grad cosine similarity\", color=\"blue\")\n",
    "    ax4.tick_params(axis='y', labelcolor=\"blue\")\n",
    "    ax4.set_ylim(-1.1, 1.1)\n",
    "    ax4.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(TAG_DIR / f\"training_history_{RUN_TAG}.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b1eac159",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparison(history_adaptive: dict, history_normal: dict, TAG_DIR: Path, RUN_TAG: str = \"comparison\"):\n",
    "    \"\"\"\n",
    "    Overlay two training histories to compare adaptive vs normal WGAN-GP.\n",
    "    \n",
    "    Args:\n",
    "        history_adaptive: History dict from adaptive (cosine similarity) training\n",
    "        history_normal: History dict from normal training\n",
    "        TAG_DIR: Directory to save the plot\n",
    "        RUN_TAG: Tag for the saved file name\n",
    "    \"\"\"\n",
    "    if len(history_adaptive.get(\"step\", [])) == 0 or len(history_normal.get(\"step\", [])) == 0:\n",
    "        print(\"One or both histories are empty. Train first.\")\n",
    "        return\n",
    "\n",
    "    fig, axes = plt.subplots(4, 2, figsize=(16, 14))\n",
    "    \n",
    "    step_a = history_adaptive[\"step\"]\n",
    "    step_n = history_normal[\"step\"]\n",
    "\n",
    "    # ---- Row 0: Losses ----\n",
    "    # Generator Loss\n",
    "    axes[0, 0].plot(step_a, history_adaptive[\"loss_G\"], label=\"Adaptive G\", alpha=0.8, color=\"blue\")\n",
    "    axes[0, 0].plot(step_n, history_normal[\"loss_G\"], label=\"Normal G\", alpha=0.8, color=\"red\", linestyle=\"--\")\n",
    "    axes[0, 0].set_title(\"Generator Loss Comparison\")\n",
    "    axes[0, 0].set_xlabel(\"step\")\n",
    "    axes[0, 0].set_ylabel(\"loss_G\")\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True)\n",
    "\n",
    "    # Critic Loss\n",
    "    axes[0, 1].plot(step_a, history_adaptive[\"loss_D\"], label=\"Adaptive D\", alpha=0.8, color=\"blue\")\n",
    "    axes[0, 1].plot(step_n, history_normal[\"loss_D\"], label=\"Normal D\", alpha=0.8, color=\"red\", linestyle=\"--\")\n",
    "    axes[0, 1].set_title(\"Critic Loss Comparison\")\n",
    "    axes[0, 1].set_xlabel(\"step\")\n",
    "    axes[0, 1].set_ylabel(\"loss_D\")\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True)\n",
    "\n",
    "    # ---- Row 1: Wasserstein Gap ----\n",
    "    # Raw gap\n",
    "    axes[1, 0].plot(step_a, history_adaptive[\"gap\"], label=\"Adaptive gap\", alpha=0.5, color=\"blue\")\n",
    "    axes[1, 0].plot(step_n, history_normal[\"gap\"], label=\"Normal gap\", alpha=0.5, color=\"red\")\n",
    "    axes[1, 0].plot(step_a, history_adaptive[\"gap_ema\"], label=\"Adaptive EMA\", linewidth=2, color=\"darkblue\")\n",
    "    axes[1, 0].plot(step_n, history_normal[\"gap_ema\"], label=\"Normal EMA\", linewidth=2, color=\"darkred\", linestyle=\"--\")\n",
    "    axes[1, 0].set_title(\"Wasserstein Gap Comparison\")\n",
    "    axes[1, 0].set_xlabel(\"step\")\n",
    "    axes[1, 0].set_ylabel(\"E[D(real)] - E[D(fake)]\")\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True)\n",
    "\n",
    "    # Gradient Penalty\n",
    "    axes[1, 1].plot(step_a, history_adaptive[\"gp\"], label=\"Adaptive GP\", alpha=0.7, color=\"blue\")\n",
    "    axes[1, 1].plot(step_n, history_normal[\"gp\"], label=\"Normal GP\", alpha=0.7, color=\"red\", linestyle=\"--\")\n",
    "    axes[1, 1].set_title(\"Gradient Penalty Comparison\")\n",
    "    axes[1, 1].set_xlabel(\"step\")\n",
    "    axes[1, 1].set_ylabel(\"gp\")\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True)\n",
    "\n",
    "    # ---- Row 2: Gradient Cosine Similarity ----\n",
    "    # D grad cos_sim\n",
    "    axes[2, 0].plot(step_a, history_adaptive[\"d_grad_cos_sim\"], label=\"Adaptive D cos_sim\", alpha=0.8, color=\"blue\")\n",
    "    axes[2, 0].plot(step_n, history_normal[\"d_grad_cos_sim\"], label=\"Normal D cos_sim\", alpha=0.8, color=\"red\", linestyle=\"--\")\n",
    "    axes[2, 0].axhline(y=0.9, color='green', linestyle=':', alpha=0.5, label='high threshold')\n",
    "    axes[2, 0].axhline(y=0.3, color='orange', linestyle=':', alpha=0.5, label='low threshold')\n",
    "    axes[2, 0].set_title(\"Discriminator Gradient Consistency\")\n",
    "    axes[2, 0].set_xlabel(\"step\")\n",
    "    axes[2, 0].set_ylabel(\"cosine similarity\")\n",
    "    axes[2, 0].set_ylim(-1.1, 1.1)\n",
    "    axes[2, 0].legend()\n",
    "    axes[2, 0].grid(True)\n",
    "\n",
    "    # G grad cos_sim\n",
    "    axes[2, 1].plot(step_a, history_adaptive[\"g_grad_cos_sim\"], label=\"Adaptive G cos_sim\", alpha=0.8, color=\"blue\")\n",
    "    axes[2, 1].plot(step_n, history_normal[\"g_grad_cos_sim\"], label=\"Normal G cos_sim\", alpha=0.8, color=\"red\", linestyle=\"--\")\n",
    "    axes[2, 1].set_title(\"Generator Gradient Consistency\")\n",
    "    axes[2, 1].set_xlabel(\"step\")\n",
    "    axes[2, 1].set_ylabel(\"cosine similarity\")\n",
    "    axes[2, 1].set_ylim(-1.1, 1.1)\n",
    "    axes[2, 1].legend()\n",
    "    axes[2, 1].grid(True)\n",
    "\n",
    "    # ---- Row 3: n_critic and n_gen ----\n",
    "    # n_critic\n",
    "    axes[3, 0].plot(step_a, history_adaptive[\"n_critic\"], label=\"Adaptive n_critic\", color=\"blue\", drawstyle='steps-post', linewidth=2)\n",
    "    axes[3, 0].plot(step_n, history_normal[\"n_critic\"], label=\"Normal n_critic\", color=\"red\", drawstyle='steps-post', linewidth=2, linestyle=\"--\")\n",
    "    axes[3, 0].set_title(\"n_critic Over Time\")\n",
    "    axes[3, 0].set_xlabel(\"step\")\n",
    "    axes[3, 0].set_ylabel(\"n_critic\")\n",
    "    all_n_critic = history_adaptive[\"n_critic\"] + history_normal[\"n_critic\"]\n",
    "    axes[3, 0].set_ylim(max(0, min(all_n_critic) - 1), max(all_n_critic) + 1)\n",
    "    axes[3, 0].legend()\n",
    "    axes[3, 0].grid(True)\n",
    "\n",
    "    # n_gen\n",
    "    axes[3, 1].plot(step_a, history_adaptive[\"n_gen\"], label=\"Adaptive n_gen\", color=\"blue\", drawstyle='steps-post', linewidth=2)\n",
    "    axes[3, 1].plot(step_n, history_normal[\"n_gen\"], label=\"Normal n_gen\", color=\"red\", drawstyle='steps-post', linewidth=2, linestyle=\"--\")\n",
    "    axes[3, 1].set_title(\"n_gen Over Time\")\n",
    "    axes[3, 1].set_xlabel(\"step\")\n",
    "    axes[3, 1].set_ylabel(\"n_gen\")\n",
    "    all_n_gen = history_adaptive[\"n_gen\"] + history_normal[\"n_gen\"]\n",
    "    axes[3, 1].set_ylim(max(0, min(all_n_gen) - 1), max(all_n_gen) + 1)\n",
    "    axes[3, 1].legend()\n",
    "    axes[3, 1].grid(True)\n",
    "\n",
    "    plt.suptitle(\"Adaptive (Cosine Similarity) vs Normal WGAN-GP\", fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(TAG_DIR / f\"comparison_{RUN_TAG}.png\", dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SUMMARY STATISTICS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"{'Metric':<25} {'Adaptive':>15} {'Normal':>15}\")\n",
    "    print(\"-\"*60)\n",
    "    print(f\"{'Final loss_G':<25} {history_adaptive['loss_G'][-1]:>15.4f} {history_normal['loss_G'][-1]:>15.4f}\")\n",
    "    print(f\"{'Final loss_D':<25} {history_adaptive['loss_D'][-1]:>15.4f} {history_normal['loss_D'][-1]:>15.4f}\")\n",
    "    print(f\"{'Final gap_ema':<25} {history_adaptive['gap_ema'][-1]:>15.4f} {history_normal['gap_ema'][-1]:>15.4f}\")\n",
    "    print(f\"{'Mean GP':<25} {np.mean(history_adaptive['gp']):>15.4f} {np.mean(history_normal['gp']):>15.4f}\")\n",
    "    print(f\"{'Mean D cos_sim':<25} {np.mean(history_adaptive['d_grad_cos_sim']):>15.4f} {np.mean(history_normal['d_grad_cos_sim']):>15.4f}\")\n",
    "    print(f\"{'Mean G cos_sim':<25} {np.mean(history_adaptive['g_grad_cos_sim']):>15.4f} {np.mean(history_normal['g_grad_cos_sim']):>15.4f}\")\n",
    "    print(f\"{'Final n_critic':<25} {history_adaptive['n_critic'][-1]:>15} {history_normal['n_critic'][-1]:>15}\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6611d860",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_gradient_vector(parameters) -> torch.Tensor:\n",
    "    \"\"\"Flatten all gradients into a single vector.\"\"\"\n",
    "    grads = []\n",
    "    for p in parameters:\n",
    "        if p.grad is not None:\n",
    "            grads.append(p.grad.detach().view(-1))\n",
    "    if len(grads) == 0:\n",
    "        return None\n",
    "    return torch.cat(grads)\n",
    "\n",
    "def cosine_similarity_gradients(grad_vec1: torch.Tensor, grad_vec2: torch.Tensor) -> float:\n",
    "    \"\"\"Compute cosine similarity between two gradient vectors.\"\"\"\n",
    "    if grad_vec1 is None or grad_vec2 is None:\n",
    "        return 0.0\n",
    "    cos_sim = F.cosine_similarity(\n",
    "        grad_vec1.unsqueeze(0), \n",
    "        grad_vec2.unsqueeze(0)\n",
    "    ).item()\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9386bbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_to_tanh(x: np.ndarray, clip: float = 3.0, eps: float = 1e-6):\n",
    "    \"\"\"\n",
    "    x: (C, T) numpy\n",
    "    per-channel standardize, clip, then map to ~[-1, 1]\n",
    "    \"\"\"\n",
    "    mean = x.mean(axis=-1, keepdims=True)\n",
    "    std  = x.std(axis=-1, keepdims=True)\n",
    "    std = np.maximum(std, eps)\n",
    "    x = (x - mean) / std\n",
    "    x = np.clip(x, -clip, clip) / clip\n",
    "    return x\n",
    "\n",
    "def descale_from_tanh(x: np.ndarray, original_mean: np.ndarray, original_std: np.ndarray, clip: float = 3.0):\n",
    "    \"\"\"\n",
    "    x: (C, T) numpy in ~[-1, 1]\n",
    "    reverse of scale_to_tanh\n",
    "    \"\"\"\n",
    "    x = x * clip\n",
    "    x = x * original_std + original_mean\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "74f6020f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_DIR exists: True\n",
      "MODEL_DIR exists: True\n",
      "Found 45 .gdf files.\n",
      "First 10 files: ['B0101T.gdf', 'B0102T.gdf', 'B0103T.gdf', 'B0104E.gdf', 'B0105E.gdf', 'B0201T.gdf', 'B0202T.gdf', 'B0203T.gdf', 'B0204E.gdf', 'B0205E.gdf']\n"
     ]
    }
   ],
   "source": [
    "PLUEM_DIR = Path.cwd()\n",
    "DATA_DIR = PLUEM_DIR.parent / \"BCICIV_2b_gdf\"\n",
    "MODEL_DIR = PLUEM_DIR / \"models\"\n",
    "\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"DATA_DIR exists: {DATA_DIR.exists()}\")\n",
    "print(f\"MODEL_DIR exists: {MODEL_DIR.exists()}\")\n",
    "\n",
    "files = list(DATA_DIR.glob(\"*.gdf\"))\n",
    "files.sort()\n",
    "print(f\"Found {len(files)} .gdf files.\")\n",
    "print(\"First 10 files:\", [f.name for f in files[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dc848a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gdf_files(data_dir: str, resample_hz: int, mode: str, verbose: bool = False)  -> list[str]:\n",
    "    if mode == \"train\":\n",
    "        pattern = \"*T.gdf\"  # Training files\n",
    "    elif mode == \"eval\":\n",
    "        pattern = \"*E.gdf\"  # Evaluation files\n",
    "    else:\n",
    "        raise ValueError(\"mode should be 'train' or 'eval'\")\n",
    "    \n",
    "    all_files = sorted([file for file in data_dir.glob(pattern)])\n",
    "    \n",
    "    if len(all_files) == 0:\n",
    "        raise ValueError(f\"No .gdf files found in {data_dir} with pattern {pattern}\")\n",
    "    \n",
    "    raws = []   \n",
    "    for file in all_files:\n",
    "        if verbose:\n",
    "            print(\"\\n=== Reading:\", file.name, \"===\")\n",
    "        raw = mne.io.read_raw_gdf(file, preload=True, verbose=\"error\")\n",
    "        raw.pick(\"eeg\")  \n",
    "        raw.resample(resample_hz)\n",
    "        raws.append(raw)\n",
    "    \n",
    "    return raws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "30ff8b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_epoch(raw: mne.io.Raw, event_id: Dict[str, int], tmin: float, tmax: float) -> np.ndarray:\n",
    "    events, event_dict = mne.events_from_annotations(raw, verbose=\"error\")\n",
    "    lh = str(event_id.get(\"LH\"))\n",
    "    rh = str(event_id.get(\"RH\"))\n",
    "    event_id = {\"LH\": event_dict.get(lh), \"RH\": event_dict.get(rh)}\n",
    "    \n",
    "    if event_id[\"LH\"] is None or event_id[\"RH\"] is None:\n",
    "        print(f\"At {raw.filenames[0].name} Event ID(lh({type(lh)}):{lh}) or RH({type(rh)}):{rh} not found in annotations: {event_dict}\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"From:{event_dict} to {event_id}\")\n",
    "    epochs = mne.Epochs(raw, events, event_id=event_id, tmin=tmin, tmax=tmax, baseline=None, preload=True, verbose=\"error\")\n",
    "    return epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6460fa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(raws: list[mne.io.Raw], tmin: float, tmax: float, event_id: Dict[str, int]) -> np.ndarray:\n",
    "    if not raws:\n",
    "        raise ValueError(\"The list of raws is empty.\")\n",
    "    \n",
    "    epochs_list = []\n",
    "    for i, raw in enumerate(raws):\n",
    "        epochs = create_epoch(raw, event_id=event_id, tmin=tmin, tmax=tmax)\n",
    "        \n",
    "        if len(epochs) == 0:\n",
    "            continue\n",
    "        \n",
    "        epochs_list.append(epochs.get_data())  # shape: (n_epochs, n_channels, n_times)\n",
    "    \n",
    "    if not epochs_list:\n",
    "        raise ValueError(\"No epochs were created from the provided raw data.\")\n",
    "    \n",
    "    dataset = np.concatenate(epochs_list, axis=0)  # shape: (total_epochs, n_channels, n_times)\n",
    "    print(f\"Dataset shape: {dataset.shape}\")\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c90bdd23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Reading: B0101T.gdf ===\n",
      "\n",
      "=== Reading: B0102T.gdf ===\n",
      "\n",
      "=== Reading: B0103T.gdf ===\n",
      "\n",
      "=== Reading: B0201T.gdf ===\n",
      "\n",
      "=== Reading: B0202T.gdf ===\n",
      "\n",
      "=== Reading: B0203T.gdf ===\n",
      "\n",
      "=== Reading: B0301T.gdf ===\n",
      "\n",
      "=== Reading: B0302T.gdf ===\n",
      "\n",
      "=== Reading: B0303T.gdf ===\n",
      "\n",
      "=== Reading: B0401T.gdf ===\n",
      "\n",
      "=== Reading: B0402T.gdf ===\n",
      "\n",
      "=== Reading: B0403T.gdf ===\n",
      "\n",
      "=== Reading: B0501T.gdf ===\n",
      "\n",
      "=== Reading: B0502T.gdf ===\n",
      "\n",
      "=== Reading: B0503T.gdf ===\n",
      "\n",
      "=== Reading: B0601T.gdf ===\n",
      "\n",
      "=== Reading: B0602T.gdf ===\n",
      "\n",
      "=== Reading: B0603T.gdf ===\n",
      "\n",
      "=== Reading: B0701T.gdf ===\n",
      "\n",
      "=== Reading: B0702T.gdf ===\n",
      "\n",
      "=== Reading: B0703T.gdf ===\n",
      "\n",
      "=== Reading: B0801T.gdf ===\n",
      "\n",
      "=== Reading: B0802T.gdf ===\n",
      "\n",
      "=== Reading: B0803T.gdf ===\n",
      "\n",
      "=== Reading: B0901T.gdf ===\n",
      "\n",
      "=== Reading: B0902T.gdf ===\n",
      "\n",
      "=== Reading: B0903T.gdf ===\n",
      "From:{np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11} to {'LH': 10, 'RH': 11}\n",
      "From:{np.str_('1023'): 1, np.str_('32766'): 2, np.str_('768'): 3, np.str_('769'): 4, np.str_('770'): 5} to {'LH': 4, 'RH': 5}\n",
      "From:{np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11, np.str_('781'): 12} to {'LH': 10, 'RH': 11}\n",
      "From:{np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11} to {'LH': 10, 'RH': 11}\n",
      "From:{np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11} to {'LH': 10, 'RH': 11}\n",
      "From:{np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11, np.str_('781'): 12} to {'LH': 10, 'RH': 11}\n",
      "From:{np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11} to {'LH': 10, 'RH': 11}\n",
      "From:{np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11} to {'LH': 10, 'RH': 11}\n",
      "From:{np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11, np.str_('781'): 12} to {'LH': 10, 'RH': 11}\n",
      "From:{np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11} to {'LH': 10, 'RH': 11}\n",
      "From:{np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11} to {'LH': 10, 'RH': 11}\n",
      "From:{np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11, np.str_('781'): 12} to {'LH': 10, 'RH': 11}\n",
      "From:{np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11} to {'LH': 10, 'RH': 11}\n",
      "From:{np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11} to {'LH': 10, 'RH': 11}\n",
      "From:{np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11, np.str_('781'): 12} to {'LH': 10, 'RH': 11}\n",
      "From:{np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11} to {'LH': 10, 'RH': 11}\n",
      "From:{np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11} to {'LH': 10, 'RH': 11}\n",
      "From:{np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11, np.str_('781'): 12} to {'LH': 10, 'RH': 11}\n",
      "From:{np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11} to {'LH': 10, 'RH': 11}\n",
      "From:{np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11} to {'LH': 10, 'RH': 11}\n",
      "From:{np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11, np.str_('781'): 12} to {'LH': 10, 'RH': 11}\n",
      "From:{np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11} to {'LH': 10, 'RH': 11}\n",
      "From:{np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11} to {'LH': 10, 'RH': 11}\n",
      "From:{np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11, np.str_('781'): 12} to {'LH': 10, 'RH': 11}\n",
      "From:{np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11} to {'LH': 10, 'RH': 11}\n",
      "From:{np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11} to {'LH': 10, 'RH': 11}\n",
      "From:{np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11, np.str_('781'): 12} to {'LH': 10, 'RH': 11}\n",
      "Dataset shape: (3680, 6, 1025)\n",
      "Dataset shape: (3680, 6, 1025)\n",
      "Cropping SEQ_LEN from 1025 -> 1024 to satisfy architecture constraint.\n",
      "CHANNELS: 6 SEQ_LEN: 1024\n"
     ]
    }
   ],
   "source": [
    "RESAMPLE_HZ = 256\n",
    "TMIN, TMAX = 0.0, 4.0\n",
    "EVENT_ID = {\"LH\": 769, \"RH\": 770}\n",
    "\n",
    "raws = load_gdf_files(DATA_DIR, RESAMPLE_HZ, verbose=True, mode=\"train\")\n",
    "X = make_dataset(raws, TMIN, TMAX, event_id=EVENT_ID)\n",
    "print(\"Dataset shape:\", X.shape)\n",
    "\n",
    "SEQ_LEN = X.shape[2]\n",
    "if SEQ_LEN % 16 != 0:\n",
    "    new_len = (SEQ_LEN // 16) * 16  # floor to nearest multiple of 16\n",
    "    print(f\"Cropping SEQ_LEN from {SEQ_LEN} -> {new_len} to satisfy architecture constraint.\")\n",
    "    X = X[:, :, :new_len]\n",
    "    \n",
    "CHANNELS = X.shape[1]\n",
    "SEQ_LEN  = X.shape[2]\n",
    "print(\"CHANNELS:\", CHANNELS, \"SEQ_LEN:\", SEQ_LEN)\n",
    "assert SEQ_LEN % 16 == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2ec847ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches: 57\n"
     ]
    }
   ],
   "source": [
    "# From first\n",
    "class EEGTensorDataset(Dataset):\n",
    "    def __init__(self, X: np.ndarray, y: Optional[np.ndarray] = None, zscore_per_channel: bool = True):\n",
    "        \"\"\"\n",
    "        X: (N, C, L)\n",
    "        y: optional (N,)\n",
    "        zscore_per_channel:\n",
    "          - global per-channel mean/std computed across (N,L) for each channel\n",
    "        \"\"\"\n",
    "        assert X.ndim == 3\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = None if y is None else y.astype(np.int64)\n",
    "\n",
    "        if zscore_per_channel:\n",
    "            mean = self.X.mean(axis=(0, 2), keepdims=True)\n",
    "            std  = self.X.std(axis=(0, 2), keepdims=True) + 1e-6\n",
    "            self.X = (self.X - mean) / std\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.from_numpy(self.X[idx])  # (C, L)\n",
    "        if self.y is None:\n",
    "            return x\n",
    "        return x, int(self.y[idx])\n",
    "\n",
    "# Unconditional GAN: we only use x; y is available if you want conditional later\n",
    "dataset = EEGTensorDataset(X, y=None, zscore_per_channel=True)\n",
    "loader = DataLoader(dataset, batch_size=64, shuffle=True, drop_last=True, num_workers=0, pin_memory=(DEVICE==\"cuda\"))\n",
    "print(\"Batches:\", len(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "dff7118a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G params: 4.918086 M\n",
      "D params: 0.725185 M\n"
     ]
    }
   ],
   "source": [
    "# Cell 3 ‚Äî Generator & Critic (1D Conv, Standard WGAN-GP Style)\n",
    "# This is a *standard* WGAN-GP setup: the \"discriminator\" is a *critic* with a linear output (no sigmoid).\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, (nn.Conv1d, nn.ConvTranspose1d, nn.Linear)):\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "class Generator1D(nn.Module):\n",
    "    def __init__(self, z_dim: int, out_channels: int, seq_len: int, base: int = 64):\n",
    "        super().__init__()\n",
    "        assert seq_len % 16 == 0, \"For this template, seq_len should be divisible by 16.\"\n",
    "        self.z_dim = z_dim\n",
    "        self.out_channels = out_channels\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        # Project noise to a small temporal resolution then upsample x16 via ConvTranspose1d\n",
    "        self.init_len = seq_len // 16\n",
    "        self.fc = nn.Linear(z_dim, base * 8 * self.init_len)\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.ConvTranspose1d(base * 8, base * 4, kernel_size=4, stride=2, padding=1),  # x2\n",
    "            nn.BatchNorm1d(base * 4),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose1d(base * 4, base * 2, kernel_size=4, stride=2, padding=1),  # x4\n",
    "            nn.BatchNorm1d(base * 2),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose1d(base * 2, base, kernel_size=4, stride=2, padding=1),      # x8\n",
    "            nn.BatchNorm1d(base),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose1d(base, out_channels, kernel_size=4, stride=2, padding=1),  # x16\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.fc(z)\n",
    "        x = x.view(z.size(0), -1, self.init_len)\n",
    "        x = self.net(x)\n",
    "        return x\n",
    "\n",
    "class Critic1D(nn.Module):\n",
    "    def __init__(self, in_channels: int, seq_len: int, base: int = 64):\n",
    "        super().__init__()\n",
    "        assert seq_len % 16 == 0, \"For this template, seq_len should be divisible by 16.\"\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, base, kernel_size=4, stride=2, padding=1),   # /2\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv1d(base, base * 2, kernel_size=4, stride=2, padding=1),      # /4\n",
    "            nn.InstanceNorm1d(base * 2, affine=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv1d(base * 2, base * 4, kernel_size=4, stride=2, padding=1),  # /8\n",
    "            nn.InstanceNorm1d(base * 4, affine=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv1d(base * 4, base * 8, kernel_size=4, stride=2, padding=1),  # /16\n",
    "            nn.InstanceNorm1d(base * 8, affine=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "        self.out = nn.Linear(base * 8 * (seq_len // 16), 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.net(x)\n",
    "        h = h.view(x.size(0), -1)\n",
    "        return self.out(h).view(-1)\n",
    "\n",
    "# --- Model configs ---\n",
    "Z_DIM = 128\n",
    "\n",
    "G = Generator1D(z_dim=Z_DIM, out_channels=CHANNELS, seq_len=SEQ_LEN).to(DEVICE)\n",
    "D = Critic1D(in_channels=CHANNELS, seq_len=SEQ_LEN).to(DEVICE)\n",
    "\n",
    "G.apply(weights_init)\n",
    "D.apply(weights_init)\n",
    "\n",
    "print(\"G params:\", sum(p.numel() for p in G.parameters())/1e6, \"M\")\n",
    "print(\"D params:\", sum(p.numel() for p in D.parameters())/1e6, \"M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2143adad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 ‚Äî WGAN-GP Utilities (Gradient Penalty, Losses)\n",
    "# WGAN-GP objective uses:\n",
    "#   loss_D = E[D(fake)] - E[D(real)] + lambda_gp * (||‚àá_x_hat D(x_hat)||_2 - 1)^2\n",
    "#   loss_G = -E[D(fake)]\n",
    "# This matches the standard WGAN-GP formulation referenced in the survey paper. ÓàÄfileciteÓàÇturn0file0ÓàÅ\n",
    "\n",
    "def gradient_penalty(critic: nn.Module, real: torch.Tensor, fake: torch.Tensor) -> torch.Tensor:\n",
    "    bsz = real.size(0)\n",
    "    eps = torch.rand(bsz, 1, 1, device=real.device)\n",
    "    x_hat = eps * real + (1 - eps) * fake\n",
    "    x_hat.requires_grad_(True)\n",
    "\n",
    "    d_hat = critic(x_hat)\n",
    "    grads = torch.autograd.grad(\n",
    "        outputs=d_hat,\n",
    "        inputs=x_hat,\n",
    "        grad_outputs=torch.ones_like(d_hat),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )[0]\n",
    "    grads = grads.view(bsz, -1)\n",
    "    gp = ((grads.norm(2, dim=1) - 1.0) ** 2).mean()\n",
    "    return gp\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_generator(generator: nn.Module, n: int, z_dim: int) -> torch.Tensor:\n",
    "    z = torch.randn(n, z_dim, device=DEVICE)\n",
    "    return generator(z).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b187349a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 ‚Äî Adaptive Discriminator Controller (Gradient Direction Consistency)\n",
    "# Uses cosine similarity between consecutive gradient vectors to measure training stability.\n",
    "# High cosine similarity (‚âà1) = consistent gradient direction (stable training)\n",
    "# Low/negative cosine similarity = oscillating gradients (potentially unstable)\n",
    "#\n",
    "# NOTE: Cosine similarity is now computed in update_critic/update_generator.\n",
    "# This controller only adjusts n_critic based on the EMA values in state.\n",
    "\n",
    "@dataclass\n",
    "class TrainState:\n",
    "    step: int = 0\n",
    "    epoch: int = 0\n",
    "    n_critic: int = 5\n",
    "    n_gen: int = 1\n",
    "    lambda_gp: float = 10.0\n",
    "\n",
    "    # Common diagnostics\n",
    "    wasserstein_gap_ema: float = 0.0\n",
    "    ema_beta: float = 0.99\n",
    "    \n",
    "    # Gradient consistency tracking\n",
    "    d_grad_cos_sim: float = 0.0  # Discriminator gradient consistency\n",
    "    g_grad_cos_sim: float = 0.0  # Generator gradient consistency\n",
    "    \n",
    "    # EMA of cosine similarities\n",
    "    d_cos_sim_ema: float | None = None\n",
    "    g_cos_sim_ema: float | None = None\n",
    "    \n",
    "    # Store previous gradient vectors\n",
    "    prev_d_grad: torch.Tensor | None = None\n",
    "    prev_g_grad: torch.Tensor | None = None\n",
    "\n",
    "class AdaptiveDiscriminatorController:\n",
    "    \"\"\"\n",
    "    Adjusts n_critic based on gradient cosine similarity EMA stored in state.\n",
    "    Cosine similarity is computed in update_critic/update_generator functions.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "        cos_sim_threshold_high: float = 0.9,  # Too consistent ‚Üí reduce n_critic\n",
    "        cos_sim_threshold_low: float = 0.3,   # Too inconsistent ‚Üí increase n_critic\n",
    "        k_min: int = 2,\n",
    "        k_max: int = 10,\n",
    "        state: TrainState = None,\n",
    "    ):\n",
    "        self.cos_sim_threshold_high = cos_sim_threshold_high\n",
    "        self.cos_sim_threshold_low = cos_sim_threshold_low\n",
    "        self.k_min = k_min\n",
    "        self.k_max = k_max\n",
    "        self.state = state if state is not None else TrainState()\n",
    "\n",
    "    def on_batch_start(self, state: TrainState) -> None:\n",
    "        pass\n",
    "\n",
    "    def on_after_critic_update(self, optim_D: torch.optim.Optimizer, D: nn.Module = None) -> None:\n",
    "        \"\"\"Adjust n_critic based on D gradient cosine similarity EMA.\"\"\"\n",
    "        if self.state.d_cos_sim_ema is None:\n",
    "            return\n",
    "            \n",
    "        # Adaptive control based on gradient consistency\n",
    "        if self.state.d_cos_sim_ema > self.cos_sim_threshold_high:\n",
    "            # Gradients too consistent ‚Üí D might be too strong, reduce training\n",
    "            self.state.n_critic = max(self.k_min, self.state.n_critic - 1)\n",
    "        elif self.state.d_cos_sim_ema < self.cos_sim_threshold_low:\n",
    "            # Gradients too inconsistent ‚Üí D needs more updates\n",
    "            self.state.n_critic = min(self.k_max, self.state.n_critic + 1)\n",
    "\n",
    "    def on_after_generator_update(self, optim_G: torch.optim.Optimizer, G: nn.Module = None) -> None:\n",
    "        \"\"\"Optional: Could adjust n_gen based on G gradient consistency.\"\"\"\n",
    "        pass  # Currently no adjustment for generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "33ca0233",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_critic(z, real, G, D, optim_D, state, controller: Optional[AdaptiveDiscriminatorController] = None) -> Dict[str, float]:\n",
    "    fake = G(z).detach() # generate fake samples without gradients\n",
    "    \n",
    "    d_real = D(real).mean()\n",
    "    d_fake = D(fake).mean()\n",
    "    gap = (d_real - d_fake).item()\n",
    "    \n",
    "    gp = gradient_penalty(D, real, fake)\n",
    "    loss_D = (d_fake - d_real) + state.lambda_gp * gp\n",
    "    \n",
    "    optim_D.zero_grad(set_to_none=True)\n",
    "    loss_D.backward()\n",
    "    \n",
    "    # Always compute cosine similarity (for both adaptive and normal training)\n",
    "    curr_d_grad = compute_gradient_vector(D.parameters())\n",
    "    if curr_d_grad is not None and state.prev_d_grad is not None:\n",
    "        cos_sim = cosine_similarity_gradients(curr_d_grad, state.prev_d_grad)\n",
    "        # Update EMA\n",
    "        if state.d_cos_sim_ema is None:\n",
    "            state.d_cos_sim_ema = cos_sim\n",
    "        else:\n",
    "            state.d_cos_sim_ema = state.ema_beta * state.d_cos_sim_ema + (1 - state.ema_beta) * cos_sim\n",
    "        state.d_grad_cos_sim = state.d_cos_sim_ema\n",
    "    state.prev_d_grad = curr_d_grad.clone() if curr_d_grad is not None else None\n",
    "    \n",
    "    # Let controller do adaptive adjustments (if present)\n",
    "    if controller is not None:\n",
    "        controller.on_after_critic_update(optim_D, D=D)\n",
    "    \n",
    "    optim_D.step()\n",
    "    \n",
    "    return {\n",
    "        \"d_real\": float(d_real.item()),\n",
    "        \"d_fake\": float(d_fake.item()),\n",
    "        \"gap\": float(gap),\n",
    "        \"gp\": float(gp.item()),\n",
    "        \"loss_D\": float(loss_D.item()),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8d66d2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_generator(z, optim_G, G, D, state, controller: Optional[AdaptiveDiscriminatorController] = None) -> float:\n",
    "    \n",
    "    fake = G(z)\n",
    "    loss_G = -D(fake).mean()\n",
    "    \n",
    "    optim_G.zero_grad(set_to_none=True)\n",
    "    loss_G.backward()\n",
    "    \n",
    "    # Always compute cosine similarity (for both adaptive and normal training)\n",
    "    curr_g_grad = compute_gradient_vector(G.parameters())\n",
    "    if curr_g_grad is not None and state.prev_g_grad is not None:\n",
    "        cos_sim = cosine_similarity_gradients(curr_g_grad, state.prev_g_grad)\n",
    "        # Update EMA\n",
    "        if state.g_cos_sim_ema is None:\n",
    "            state.g_cos_sim_ema = cos_sim\n",
    "        else:\n",
    "            state.g_cos_sim_ema = state.ema_beta * state.g_cos_sim_ema + (1 - state.ema_beta) * cos_sim\n",
    "        state.g_grad_cos_sim = state.g_cos_sim_ema\n",
    "    state.prev_g_grad = curr_g_grad.clone() if curr_g_grad is not None else None\n",
    "    \n",
    "    # Let controller do adaptive adjustments (if present)\n",
    "    if controller is not None:\n",
    "        controller.on_after_generator_update(optim_G, G=G)\n",
    "        \n",
    "    optim_G.step()\n",
    "    \n",
    "    return {\"loss_G\": float(loss_G.item())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "053870a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(path, G, D, optim_G, optim_D, state, history):\n",
    "    \"\"\"Save full training checkpoint.\"\"\"\n",
    "    payload = {\n",
    "        \"G\": G.state_dict(),\n",
    "        \"D\": D.state_dict(),\n",
    "        \"optim_G\": optim_G.state_dict(),\n",
    "        \"optim_D\": optim_D.state_dict(),\n",
    "        \"state\": state.__dict__,\n",
    "        \"history\": dict(history),\n",
    "    }\n",
    "    torch.save(payload, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "eabb4409",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(path, G, D, optim_G=None, optim_D=None):\n",
    "    \"\"\"Load checkpoint and return state + history.\"\"\"\n",
    "    ckpt = torch.load(path, map_location=DEVICE)\n",
    "    G.load_state_dict(ckpt[\"G\"])\n",
    "    D.load_state_dict(ckpt[\"D\"])\n",
    "    if optim_G is not None and \"optim_G\" in ckpt:\n",
    "        optim_G.load_state_dict(ckpt[\"optim_G\"])\n",
    "    if optim_D is not None and \"optim_D\" in ckpt:\n",
    "        optim_D.load_state_dict(ckpt[\"optim_D\"])\n",
    "    state = TrainState(**ckpt[\"state\"])\n",
    "    hist = ckpt.get(\"history\", {})\n",
    "    return state, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a98fb62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(\n",
    "    Z_DIM: int, \n",
    "    CHANNELS: int, \n",
    "    SEQ_LEN: int, \n",
    "    LR_G: float, \n",
    "    LR_D: float,\n",
    "    BETAS: Tuple[float, float], \n",
    "    DEVICE: str\n",
    "):\n",
    "    G = Generator1D(z_dim=Z_DIM, out_channels=CHANNELS, seq_len=SEQ_LEN).to(DEVICE)\n",
    "    D = Critic1D(in_channels=CHANNELS, seq_len=SEQ_LEN).to(DEVICE)\n",
    "    \n",
    "    optim_G = torch.optim.Adam(G.parameters(), lr=LR_G, betas=BETAS)\n",
    "    optim_D = torch.optim.Adam(D.parameters(), lr=LR_D, betas=BETAS)\n",
    "    \n",
    "    state = TrainState(step=0, epoch=0, n_gen=1, n_critic=5, lambda_gp=10.0)\n",
    "\n",
    "    controller = AdaptiveDiscriminatorController(k_max=10, k_min=2, state=state)\n",
    "    \n",
    "    print(f\"LR_G: {LR_G}, LR_D: {LR_D}, BETAS: {BETAS}\")\n",
    "    print(f\"n_critic: {state.n_critic}, n_gen: {state.n_gen}, lambda_gp: {state.lambda_gp}\")\n",
    "    \n",
    "    return G, D, optim_G, optim_D, state, controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "27a25ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def update_history(history: Dict[str, list], metrics: Dict[str, Any]) -> None:\n",
    "#     for k, v in metrics.items():\n",
    "#         if k not in history:\n",
    "#             history[k] = []\n",
    "#         history[k].append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c2979d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_wgan_gp(\n",
    "    loader: DataLoader,\n",
    "    G: nn.Module,\n",
    "    D: nn.Module,\n",
    "    optim_G: torch.optim.Optimizer,\n",
    "    optim_D: torch.optim.Optimizer,\n",
    "    state: TrainState,\n",
    "    z_dim: int,\n",
    "    controller: Optional[AdaptiveDiscriminatorController] = None,\n",
    "    epochs: int = 10,\n",
    "    log_every: int = 50,\n",
    "    save_every_steps: int = 500,\n",
    "    best_metric: str = \"gap_ema\",\n",
    "    model_dir: str = MODEL_DIR,\n",
    "    run_tag: str = \"wgan_gp_adaptiveD\",\n",
    "):\n",
    "    CHECKPOINT_DIR = Path(model_dir) / \"checkpoints\"\n",
    "    CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    TAG_DIR = Path(model_dir) / run_tag\n",
    "    TAG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    RUN_TAG = run_tag\n",
    "    \n",
    "    G.train(); D.train()\n",
    "    \n",
    "    # History for plotting\n",
    "    history = {\n",
    "        \"step\": [], \"loss_D\": [], \"loss_G\": [], \"gap\": [], \"gap_ema\": [],\n",
    "        \"gp\": [], \"n_critic\": [], \"d_grad_cos_sim\": [], \"g_grad_cos_sim\": [],\n",
    "        \"n_critic\": [], \"n_gen\": [],\n",
    "    }\n",
    "    \n",
    "    best_score = getattr(state, 'best_score', -1e18)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        state.epoch = epoch\n",
    "        for batch_idx, real in enumerate(loader):\n",
    "            state.step += 1\n",
    "            if controller is not None:\n",
    "                controller.on_batch_start(state)\n",
    "                \n",
    "            real = real.to(DEVICE)\n",
    "            bsz = real.size(0)\n",
    "\n",
    "            # -------------------------\n",
    "            # Critic updates (n_critic)\n",
    "            # -------------------------\n",
    "            metrics_D = {}\n",
    "            for _ in range(state.n_critic):\n",
    "                z = torch.randn(bsz, z_dim, device=DEVICE)\n",
    "                metrics_D = update_critic(z, real, G, D, optim_D, state, controller)\n",
    "\n",
    "                # Update EMA of the gap\n",
    "                state.wasserstein_gap_ema = state.ema_beta * state.wasserstein_gap_ema + (1 - state.ema_beta) * metrics_D[\"gap\"]\n",
    "                metrics_D[\"gap_ema\"] = float(state.wasserstein_gap_ema) \n",
    "\n",
    "            # -------------------------\n",
    "            # Generator update\n",
    "            # -------------------------\n",
    "            z = torch.randn(bsz, z_dim, device=DEVICE)\n",
    "            metrics_G = update_generator(z, optim_G, G=G, D=D, state=state, controller=controller)\n",
    "\n",
    "            # -------------------------\n",
    "            # Log history (read cosine similarity from state, not metrics)\n",
    "            # -------------------------\n",
    "            history[\"step\"].append(state.step)\n",
    "            history[\"loss_D\"].append(metrics_D.get(\"loss_D\", 0))\n",
    "            history[\"loss_G\"].append(metrics_G.get(\"loss_G\", 0))\n",
    "            history[\"gap\"].append(metrics_D.get(\"gap\", 0))\n",
    "            \n",
    "            history[\"gap_ema\"].append(metrics_D.get(\"gap_ema\", 0))\n",
    "            history[\"gp\"].append(metrics_D.get(\"gp\", 0))\n",
    "            history[\"n_critic\"].append(state.n_critic)\n",
    "            history[\"n_gen\"].append(state.n_gen)\n",
    "            # Read from state where the controller stores the values\n",
    "            history[\"d_grad_cos_sim\"].append(state.d_grad_cos_sim)\n",
    "            history[\"g_grad_cos_sim\"].append(state.g_grad_cos_sim)\n",
    "\n",
    "            if (batch_idx % log_every) == 0:\n",
    "                d_cos = state.d_grad_cos_sim\n",
    "                g_cos = state.g_grad_cos_sim\n",
    "                print(\n",
    "                    f\"[Epoch {epoch:03d}/{epochs:03d}] [Batch {batch_idx:04d}/{len(loader):04d}] \"\n",
    "                    f\"[n_critic: {state.n_critic}] [n_gen: {state.n_gen}] \"\n",
    "                    f\"[gap: {metrics_D['gap']:+.3f} | ema: {metrics_D['gap_ema']:+.3f}] \"\n",
    "                    f\"[GP: {metrics_D['gp']:.3f}] \"\n",
    "                    f\"[D_cos: {d_cos:+.3f}] [G_cos: {g_cos:+.3f}] \"\n",
    "                    f\"[D: {metrics_D['loss_D']:+.3f}] [G: {metrics_G['loss_G']:+.3f}]\"\n",
    "                )\n",
    "\n",
    "            # -------------------------\n",
    "            # Save periodic checkpoint\n",
    "            # -------------------------\n",
    "            if save_every_steps > 0 and (state.step % save_every_steps) == 0:\n",
    "                ckpt_path = CHECKPOINT_DIR / f\"ckpt_{RUN_TAG}_step_{state.step}.pt\"\n",
    "                save_checkpoint(str(ckpt_path), G, D, optim_G, optim_D, state, history)\n",
    "                print(f\"üíæ Saved checkpoint: {ckpt_path}\")\n",
    "\n",
    "            # -------------------------\n",
    "            # Save best checkpoint\n",
    "            # -------------------------\n",
    "            if metrics_D:\n",
    "                score = float(metrics_D.get(best_metric, -1e18))\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_path = CHECKPOINT_DIR / f\"best_{RUN_TAG}.pt\"\n",
    "                    save_checkpoint(str(best_path), G, D, optim_G, optim_D, state, history)\n",
    "                    print(f\"üèÜ New BEST ({best_metric}={score:.4f}) -> {best_path}\")\n",
    "    \n",
    "    # Save final checkpoint\n",
    "    final_path = TAG_DIR / f\"final_{RUN_TAG}.pt\"\n",
    "    save_checkpoint(str(final_path), G, D, optim_G, optim_D, state, history)\n",
    "    print(f\"‚úÖ Saved final checkpoint: {final_path}\")\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "46e1dc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6 ‚Äî Optimizers & Hyperparameters (Standard WGAN-GP Defaults)\n",
    "LR_G = 1e-4  # Generator learning rate\n",
    "LR_D = [5e-5, 2e-5]  # Discriminator/Critic learning rate\n",
    "BETAS = (0.0, 0.9)   # standard WGAN-GP choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a56381b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 1\n",
    "TAG = \"test\"\n",
    "\n",
    "tag_dir = MODEL_DIR / TAG\n",
    "tag_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d4cb6f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_both(lr_D, lr_G, tag, tag_dir, epochs, z_dim, channels, seq_len, betas, device):\n",
    "    cos_tag = f\"cos:{tag}\"\n",
    "    cos_dir = tag_dir / cos_tag\n",
    "    cos_checkpoints = cos_dir / \"checkpoints\"\n",
    "\n",
    "    cos_checkpoints.mkdir(parents=True, exist_ok=True)\n",
    "    cos_dir.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Run directory: {cos_tag}\")\n",
    "    \n",
    "    G_cos, D_cos, optim_G_cos, optim_D_cos, state_cos, controller = initialize(\n",
    "        Z_DIM=z_dim, CHANNELS=channels, SEQ_LEN=seq_len, LR_D=lr_D, LR_G=lr_G, BETAS=betas, DEVICE=device\n",
    "    )\n",
    "\n",
    "    history_cos = train_wgan_gp(\n",
    "        loader, G_cos , D_cos, optim_G_cos, optim_D_cos, state_cos,\n",
    "        z_dim=z_dim, controller=controller,\n",
    "        epochs=epochs, log_every=50, save_every_steps=500\n",
    "    )\n",
    "    \n",
    "    save_models(cos_dir, cos_tag, G_cos, D_cos, history_cos)\n",
    "    \n",
    "    og_tag = f\"og:{tag}\"\n",
    "    og_dir = tag_dir / og_tag\n",
    "    og_checkpoints = og_dir / \"checkpoints\"\n",
    "\n",
    "    og_checkpoints.mkdir(parents=True, exist_ok=True)\n",
    "    og_dir.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Run directory: {og_tag}\")\n",
    "    \n",
    "    G_og, D_og, optim_G_og, optim_D_og, state_og, _ = initialize(\n",
    "        Z_DIM=z_dim, CHANNELS=channels, SEQ_LEN=seq_len, LR_D=lr_D, LR_G=lr_G, BETAS=betas, DEVICE=device\n",
    "    )\n",
    "\n",
    "    history_og = train_wgan_gp(\n",
    "        loader, G_og , D_og, optim_G_og, optim_D_og, state_og,\n",
    "        z_dim=z_dim, controller=controller,\n",
    "        epochs=epochs, log_every=50, save_every_steps=500\n",
    "    )\n",
    "    \n",
    "    save_models(og_dir, og_tag, G_og, D_og, history_og)\n",
    "    \n",
    "    return history_cos, history_og\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "db72a7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7 ‚Äî Training Loop (WGAN-GP + Gradient Direction Consistency Adaptive Control)\n",
    "# Logs:\n",
    "#   - wasserstein gap: E[D(real)] - E[D(fake)]\n",
    "#   - gp: gradient penalty term\n",
    "#   - d_grad_cos_sim: discriminator gradient direction consistency\n",
    "#   - g_grad_cos_sim: generator gradient direction consistency\n",
    "#   - losses for D and G\n",
    "\n",
    "# RUN_TAG = f\"grad_cos_sim:{TAG}\"\n",
    "# TAG_DIR = MODEL_DIR / RUN_TAG\n",
    "# CHECKPOINT_DIR = TAG_DIR / \"checkpoints\"\n",
    "\n",
    "# CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "# TAG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "# print(f\"Run directory: {TAG_DIR}\")\n",
    "\n",
    "# G_cos, D_cos, optim_G_cos, optim_D_cos, state_cos, controller = initialize(\n",
    "#     Z_DIM=Z_DIM, CHANNELS=CHANNELS, SEQ_LEN=SEQ_LEN, LR=LR, BETAS=BETAS, DEVICE=DEVICE\n",
    "# )\n",
    "\n",
    "# history_cos_sim = train_wgan_gp(\n",
    "#     loader, G_cos , D_cos, optim_G_cos, optim_D_cos, state_cos,\n",
    "#     z_dim=Z_DIM, controller=controller,\n",
    "#     epochs=EPOCH, log_every=50, save_every_steps=500\n",
    "# )\n",
    "\n",
    "# save_models(TAG_DIR, RUN_TAG, G_cos, D_cos, history_cos_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "acd61a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8 ‚Äî Quick Sanity Sample (Optional)\n",
    "# This just samples a few generated sequences so you can inspect shapes.\n",
    "# with torch.no_grad():\n",
    "#     fake = sample_generator(G, n=4, z_dim=Z_DIM)\n",
    "    \n",
    "# print(\"Generated batch shape:\", tuple(fake.shape))  # (N, C, L)\n",
    "# print(\"Example stats:\", fake.mean().item(), fake.std().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c649ebbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run directory: cos:test\n",
      "LR_G: 0.0001, LR_D: 5e-05, BETAS: (0.0, 0.9)\n",
      "n_critic: 5, n_gen: 1, lambda_gp: 10.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[79]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m histories = [] \u001b[38;5;66;03m# to store histories for different LR_D [5e-5, 2e-5]\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m lr_D \u001b[38;5;129;01min\u001b[39;00m LR_D:\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     history = \u001b[43mtrain_both\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr_D\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr_D\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr_G\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLR_G\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtag\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTAG\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtag_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtag_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEPOCH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43mz_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mZ_DIM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchannels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCHANNELS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSEQ_LEN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbetas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBETAS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDEVICE\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m     histories.append(history)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[76]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mtrain_both\u001b[39m\u001b[34m(lr_D, lr_G, tag, tag_dir, epochs, z_dim, channels, seq_len, betas, device)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRun directory: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcos_tag\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m G_cos, D_cos, optim_G_cos, optim_D_cos, state_cos, controller = initialize(\n\u001b[32m     11\u001b[39m     Z_DIM=z_dim, CHANNELS=channels, SEQ_LEN=seq_len, LR_D=lr_D, LR_G=lr_G, BETAS=betas, DEVICE=device\n\u001b[32m     12\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m history_cos = \u001b[43mtrain_wgan_gp\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mG_cos\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD_cos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim_G_cos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim_D_cos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_cos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mz_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mz_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontroller\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontroller\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_every\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_every_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m500\u001b[39;49m\n\u001b[32m     18\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m save_models(cos_dir, cos_tag, G_cos, D_cos, history_cos)\n\u001b[32m     22\u001b[39m og_tag = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mog:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtag\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[73]\u001b[39m\u001b[32m, line 50\u001b[39m, in \u001b[36mtrain_wgan_gp\u001b[39m\u001b[34m(loader, G, D, optim_G, optim_D, state, z_dim, controller, epochs, log_every, save_every_steps, best_metric, model_dir, run_tag)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(state.n_critic):\n\u001b[32m     49\u001b[39m     z = torch.randn(bsz, z_dim, device=DEVICE)\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     metrics_D = \u001b[43mupdate_critic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim_D\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontroller\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m     \u001b[38;5;66;03m# Update EMA of the gap\u001b[39;00m\n\u001b[32m     53\u001b[39m     state.wasserstein_gap_ema = state.ema_beta * state.wasserstein_gap_ema + (\u001b[32m1\u001b[39m - state.ema_beta) * metrics_D[\u001b[33m\"\u001b[39m\u001b[33mgap\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[67]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mupdate_critic\u001b[39m\u001b[34m(z, real, G, D, optim_D, state, controller)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mupdate_critic\u001b[39m(z, real, G, D, optim_D, state, controller: Optional[AdaptiveDiscriminatorController] = \u001b[38;5;28;01mNone\u001b[39;00m) -> Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]:\n\u001b[32m      2\u001b[39m     fake = G(z).detach() \u001b[38;5;66;03m# generate fake samples without gradients\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     d_real = \u001b[43mD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal\u001b[49m\u001b[43m)\u001b[49m.mean()\n\u001b[32m      5\u001b[39m     d_fake = D(fake).mean()\n\u001b[32m      6\u001b[39m     gap = (d_real - d_fake).item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/3rd_research/lib/python3.11/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/3rd_research/lib/python3.11/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 70\u001b[39m, in \u001b[36mCritic1D.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     h = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m     h = h.view(x.size(\u001b[32m0\u001b[39m), -\u001b[32m1\u001b[39m)\n\u001b[32m     72\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.out(h).view(-\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/3rd_research/lib/python3.11/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/3rd_research/lib/python3.11/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/3rd_research/lib/python3.11/site-packages/torch/nn/modules/container.py:253\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    250\u001b[39m \u001b[33;03mRuns the forward pass.\u001b[39;00m\n\u001b[32m    251\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/3rd_research/lib/python3.11/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/3rd_research/lib/python3.11/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/3rd_research/lib/python3.11/site-packages/torch/nn/modules/conv.py:375\u001b[39m, in \u001b[36mConv1d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    374\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m375\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/3rd_research/lib/python3.11/site-packages/torch/nn/modules/conv.py:370\u001b[39m, in \u001b[36mConv1d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv1d(\n\u001b[32m    359\u001b[39m         F.pad(\n\u001b[32m    360\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    367\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    368\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m370\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "histories = [] # to store histories for different LR_D [5e-5, 2e-5]\n",
    "tags = []\n",
    "for lr_D in LR_D:\n",
    "    TAG = f\"{TAG}_{lr_D}\"\n",
    "    history = train_both(\n",
    "        lr_D=lr_D,\n",
    "        lr_G=LR_G,\n",
    "        tag=TAG,\n",
    "        tag_dir=tag_dir,\n",
    "        epochs=EPOCH,\n",
    "        z_dim=Z_DIM,\n",
    "        channels=CHANNELS,\n",
    "        seq_len=SEQ_LEN,\n",
    "        betas=BETAS,\n",
    "        device=DEVICE\n",
    "    )\n",
    "    tags.append(TAG)\n",
    "    histories.append(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e958d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8 ‚Äî Training Curve Visualization (with Gradient Consistency)\n",
    "lr_5e_5_cos, lr_5e_5_og = histories[0]\n",
    "\n",
    "plot_training_history(lr_5e_5_cos, tag_dir, f\"{tags[0]}_adaptive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f91802",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(lr_5e_5_og, tag_dir, f\"{tags[0]}_normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6581fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparison(\n",
    "    lr_5e_5_cos,\n",
    "    lr_5e_5_og,\n",
    "    tag_dir,\n",
    "    f\"comparison_{tags[0]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c1ddac",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef41d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_2e_5_cos, lr_2e_5_og = histories[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626f3cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(lr_2e_5_cos, tag_dir, f\"{tags[1]}_adaptive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22838f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(lr_2e_5_og, tag_dir, f\"{tags[1]}_normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1931f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparison(\n",
    "    lr_2e_5_cos,\n",
    "    lr_2e_5_og,\n",
    "    tag_dir,\n",
    "    f\"comparison_{tags[1]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777f8c90",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105b1841",
   "metadata": {},
   "source": [
    "### Normal"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3rd_research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
