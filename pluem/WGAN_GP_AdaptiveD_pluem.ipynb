{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd20fb5c",
   "metadata": {},
   "source": [
    "# WGAN-GP (Standard) + Adaptive-Discriminator Hooks (Template)\n",
    "\n",
    "This notebook provides:\n",
    "- A **standard WGAN-GP** training loop (critic + gradient penalty).\n",
    "- A **pluggable \"Adaptive Discriminator\" controller** with clearly marked hooks so you can experiment with different discriminator/critic adjustment strategies (e.g., dynamic `n_critic`, LR, GP weight, architecture toggles, etc.).\n",
    "\n",
    "> Notes: WGAN-GP uses a gradient penalty to enforce the 1-Lipschitz constraint instead of weight clipping (see WGAN-GP objective in the referenced survey paper). îˆ€fileciteîˆ‚turn0file0îˆ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45a012ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 â€” Imports & Reproducibility\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Dict, Any, Tuple\n",
    "from pathlib import Path\n",
    "import mne\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(42)\n",
    "print(\"Device:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9386bbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_to_tanh(x: np.ndarray, clip: float = 3.0, eps: float = 1e-6):\n",
    "    \"\"\"\n",
    "    x: (C, T) numpy\n",
    "    per-channel standardize, clip, then map to ~[-1, 1]\n",
    "    \"\"\"\n",
    "    mean = x.mean(axis=-1, keepdims=True)\n",
    "    std  = x.std(axis=-1, keepdims=True)\n",
    "    std = np.maximum(std, eps)\n",
    "    x = (x - mean) / std\n",
    "    x = np.clip(x, -clip, clip) / clip\n",
    "    return x\n",
    "\n",
    "def descale_from_tanh(x: np.ndarray, original_mean: np.ndarray, original_std: np.ndarray, clip: float = 3.0):\n",
    "    \"\"\"\n",
    "    x: (C, T) numpy in ~[-1, 1]\n",
    "    reverse of scale_to_tanh\n",
    "    \"\"\"\n",
    "    x = x * clip\n",
    "    x = x * original_std + original_mean\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0b1871f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_gdf_files(data_dir: str, subject_ids: list[int]) -> list[str]:\n",
    "#     files = []\n",
    "#     for subject_id in subject_ids:\n",
    "#         subject_str = f\"S{subject_id:02d}\"\n",
    "#         subject_dir = os.path.join(data_dir, subject_str)\n",
    "#         for file_name in os.listdir(subject_dir):\n",
    "#             if file_name.endswith(\".gdf\"):\n",
    "#                 files.append(os.path.join(subject_dir, file_name))\n",
    "#     return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a48addc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def to_epochs(raws: list[], tmin, tmax, resample_hz, wanted_labels: Tuple[str, str] = (\"769\", \"770\")) -> int:\n",
    "#     return math.ceil((total_steps * batch_size) / dataset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74f6020f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_DIR exists: True\n",
      "MODEL_DIR exists: True\n",
      "Found 45 .gdf files.\n",
      "First 10 files: ['B0101T.gdf', 'B0102T.gdf', 'B0103T.gdf', 'B0104E.gdf', 'B0105E.gdf', 'B0201T.gdf', 'B0202T.gdf', 'B0203T.gdf', 'B0204E.gdf', 'B0205E.gdf']\n"
     ]
    }
   ],
   "source": [
    "PLUEM_DIR = Path.cwd()\n",
    "DATA_DIR = PLUEM_DIR.parent / \"BCICIV_2b_gdf\"\n",
    "MODEL_DIR = PLUEM_DIR / \"models\"\n",
    "\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"DATA_DIR exists: {DATA_DIR.exists()}\")\n",
    "print(f\"MODEL_DIR exists: {MODEL_DIR.exists()}\")\n",
    "\n",
    "files = list(DATA_DIR.glob(\"*.gdf\"))\n",
    "files.sort()\n",
    "print(f\"Found {len(files)} .gdf files.\")\n",
    "print(\"First 10 files:\", [f.name for f in files[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4a0ea93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# raws = []\n",
    "# for file in files:\n",
    "#     raw = mne.io.read_raw_gdf(file, preload=True, verbose=\"error\")\n",
    "#     raws.append(raw)\n",
    "    \n",
    "# type(raws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc848a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gdf_files(data_dir: str, resample_hz: int, mode: str, verbose: bool = False)  -> list[str]:\n",
    "    if mode == \"train\":\n",
    "        pattern = \"*T.gdf\"  # Training files\n",
    "    elif mode == \"eval\":\n",
    "        pattern = \"*E.gdf\"  # Evaluation files\n",
    "    else:\n",
    "        raise ValueError(\"mode should be 'train' or 'eval'\")\n",
    "    \n",
    "    all_files = sorted([file for file in data_dir.glob(pattern)])\n",
    "    \n",
    "    if len(all_files) == 0:\n",
    "        raise ValueError(f\"No .gdf files found in {data_dir} with pattern {pattern}\")\n",
    "    \n",
    "    raws = []   \n",
    "    for file in all_files:\n",
    "        if verbose:\n",
    "            print(\"\\n=== Reading:\", file.name, \"===\")\n",
    "        raw = mne.io.read_raw_gdf(file, preload=True, verbose=\"error\")\n",
    "        raw.pick(\"eeg\")  \n",
    "        raw.resample(resample_hz)\n",
    "        raws.append(raw)\n",
    "    \n",
    "    return raws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30ff8b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_epoch(raw: mne.io.Raw, event_id: Dict[str, int], tmin: float, tmax: float) -> np.ndarray:\n",
    "    events, event_dict = mne.events_from_annotations(raw, verbose=\"error\")\n",
    "    lh = str(event_id.get(\"LH\"))\n",
    "    rh = str(event_id.get(\"RH\"))\n",
    "    event_id = {\"LH\": event_dict.get(lh), \"RH\": event_dict.get(rh)}\n",
    "    \n",
    "    if event_id[\"LH\"] is None or event_id[\"RH\"] is None:\n",
    "        print(f\"At {raw.filenames[0].name} Event ID(lh({type(lh)}):{lh}) or RH({type(rh)}):{rh} not found in annotations: {event_dict}\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"From:{event_dict} to {event_id}\")\n",
    "    epochs = mne.Epochs(raw, events, event_id=event_id, tmin=tmin, tmax=tmax, baseline=None, preload=True, verbose=\"error\")\n",
    "    return epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6460fa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(raws: list[mne.io.Raw], tmin: float, tmax: float, event_id: Dict[str, int]) -> np.ndarray:\n",
    "    if not raws:\n",
    "        raise ValueError(\"The list of raws is empty.\")\n",
    "    \n",
    "    epochs_list = []\n",
    "    for i, raw in enumerate(raws):\n",
    "        epochs = create_epoch(raw, event_id=event_id, tmin=tmin, tmax=tmax)\n",
    "        \n",
    "        if len(epochs) == 0:\n",
    "            continue\n",
    "        \n",
    "        epochs_list.append(epochs.get_data())  # shape: (n_epochs, n_channels, n_times)\n",
    "    \n",
    "    if not epochs_list:\n",
    "        raise ValueError(\"No epochs were created from the provided raw data.\")\n",
    "    \n",
    "    dataset = np.concatenate(epochs_list, axis=0)  # shape: (total_epochs, n_channels, n_times)\n",
    "    print(f\"Dataset shape: {dataset.shape}\")\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d9bccc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # From first\n",
    "# def _find_gdf_files(root: Path) -> list:\n",
    "#     return sorted([p for p in root.rglob(\"*.gdf\")])\n",
    "\n",
    "# def load_bciciv2b_epochs_by_labels(\n",
    "#     root: Path,\n",
    "#     file_glob: str = \"*T.gdf\",\n",
    "#     tmin: float = 0.0,\n",
    "#     tmax: float = 4.0,\n",
    "#     resample_hz: int = 256,\n",
    "#     picks: str = \"eeg\",\n",
    "#     baseline: Optional[Tuple[float, float]] = None,\n",
    "#     wanted_labels: Tuple[str, str] = (\"769\", \"770\"),  # left/right in BCICIV\n",
    "#     verbose: bool = True,\n",
    "# ) -> Tuple[np.ndarray, np.ndarray, int, Dict[str, int]]:\n",
    "#     all_files = _find_gdf_files(root)\n",
    "#     if file_glob:\n",
    "#         import fnmatch\n",
    "#         all_files = [f for f in all_files if fnmatch.fnmatch(f.name, file_glob)]\n",
    "\n",
    "#     if len(all_files) == 0:\n",
    "#         raise FileNotFoundError(f\"No .gdf files found under {root} (file_glob={file_glob})\")\n",
    "\n",
    "#     X_list, y_list = [], []\n",
    "#     last_sfreq = None\n",
    "\n",
    "#     # Fixed class order: wanted_labels[0] -> class 0, wanted_labels[1] -> class 1\n",
    "#     global_event_ids = {\"left\": wanted_labels[0], \"right\": wanted_labels[1]}\n",
    "\n",
    "#     for gdf_path in all_files:\n",
    "#         print(\"\\n=== Reading:\", gdf_path.name, \"===\")\n",
    "\n",
    "#         raw = mne.io.read_raw_gdf(str(gdf_path), preload=True, verbose=\"ERROR\")\n",
    "#         raw.pick(picks)\n",
    "#         raw.resample(resample_hz)\n",
    "\n",
    "#         events, event_dict = mne.events_from_annotations(raw, verbose=\"ERROR\")\n",
    "#         if verbose:\n",
    "#             print(\"Available annotation events:\", event_dict)\n",
    "\n",
    "#         # Convert wanted annotation labels -> internal event codes for THIS file\n",
    "#         missing = [lab for lab in wanted_labels if lab not in event_dict]\n",
    "#         if len(missing) > 0:\n",
    "#             print(f\"Skipping {gdf_path.name} (missing labels {missing})\")\n",
    "#             continue\n",
    "\n",
    "#         event_id = {\"left\": event_dict[wanted_labels[0]], \"right\": event_dict[wanted_labels[1]]}\n",
    "#         print(\"Using internal event codes:\", event_id, \"(for labels\", wanted_labels, \")\")\n",
    "\n",
    "#         epochs = mne.Epochs(\n",
    "#             raw,\n",
    "#             events=events,\n",
    "#             event_id=event_id,\n",
    "#             tmin=tmin,\n",
    "#             tmax=tmax,\n",
    "#             baseline=baseline,\n",
    "#             preload=True,\n",
    "#             reject=None,\n",
    "#             on_missing=\"warn\",\n",
    "#             verbose=\"ERROR\",\n",
    "#         )\n",
    "\n",
    "#         if len(epochs) == 0:\n",
    "#             print(f\"No epochs created for {gdf_path.name}. Skipping.\")\n",
    "#             continue\n",
    "\n",
    "#         data = epochs.get_data().astype(np.float32)  # (N, C, L)\n",
    "#         labels = epochs.events[:, -1]  # internal codes (e.g., 4/5)\n",
    "\n",
    "#         # Map internal codes to 0/1 consistently using event_id dict\n",
    "#         code_to_idx = {int(event_id[\"left\"]): 0, int(event_id[\"right\"]): 1}\n",
    "#         y = np.array([code_to_idx[int(c)] for c in labels], dtype=np.int64)\n",
    "\n",
    "#         X_list.append(data)\n",
    "#         y_list.append(y)\n",
    "#         last_sfreq = int(raw.info[\"sfreq\"])\n",
    "\n",
    "#     if len(X_list) == 0:\n",
    "#         raise RuntimeError(\n",
    "#             \"No epochs were created from any file. \"\n",
    "#             \"Double-check dataset contents and wanted_labels.\"\n",
    "#         )\n",
    "\n",
    "#     X = np.concatenate(X_list, axis=0)\n",
    "#     y = np.concatenate(y_list, axis=0)\n",
    "#     return X, y, last_sfreq, global_event_ids\n",
    "\n",
    "# # ---- Configure epoching ----\n",
    "# FILE_GLOB = \"*T.gdf\"  # good default for training runs\n",
    "\n",
    "# X_np, y_np, sfreq, USED_LABELS = load_bciciv2b_epochs_by_labels(\n",
    "#     root=DATA_DIR,\n",
    "#     file_glob=FILE_GLOB,\n",
    "#     tmin=TMIN,\n",
    "#     tmax=TMAX,\n",
    "#     resample_hz=RESAMPLE_HZ,\n",
    "#     picks=\"eeg\",\n",
    "#     baseline=None,\n",
    "#     wanted_labels=(\"769\", \"770\"),\n",
    "#     verbose=True,\n",
    "# )\n",
    "\n",
    "# # Fix SEQ_LEN to be divisible by 16 (crop the last sample if needed)\n",
    "# SEQ_LEN = X_np.shape[2]\n",
    "# if SEQ_LEN % 16 != 0:\n",
    "#     new_len = (SEQ_LEN // 16) * 16  # floor to nearest multiple of 16\n",
    "#     print(f\"Cropping SEQ_LEN from {SEQ_LEN} -> {new_len} to satisfy architecture constraint.\")\n",
    "#     X_np = X_np[:, :, :new_len]\n",
    "\n",
    "# CHANNELS = X_np.shape[1]\n",
    "# SEQ_LEN  = X_np.shape[2]\n",
    "# print(\"CHANNELS:\", CHANNELS, \"SEQ_LEN:\", SEQ_LEN)\n",
    "# assert SEQ_LEN % 16 == 0\n",
    "\n",
    "# # print(\"\\nLoaded X:\", X_np.shape, \"y:\", y_np.shape, \"sfreq:\", sfreq)\n",
    "# # CHANNELS = X_np.shape[1]\n",
    "# # SEQ_LEN  = X_np.shape[2]\n",
    "# # print(\"CHANNELS:\", CHANNELS, \"SEQ_LEN:\", SEQ_LEN, \"(must be divisible by 16)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c90bdd23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Reading: B0101T.gdf ===\n",
      "\n",
      "=== Reading: B0102T.gdf ===\n",
      "\n",
      "=== Reading: B0103T.gdf ===\n",
      "\n",
      "=== Reading: B0201T.gdf ===\n",
      "\n",
      "=== Reading: B0202T.gdf ===\n",
      "\n",
      "=== Reading: B0203T.gdf ===\n",
      "\n",
      "=== Reading: B0301T.gdf ===\n",
      "\n",
      "=== Reading: B0302T.gdf ===\n",
      "\n",
      "=== Reading: B0303T.gdf ===\n",
      "\n",
      "=== Reading: B0401T.gdf ===\n",
      "\n",
      "=== Reading: B0402T.gdf ===\n",
      "\n",
      "=== Reading: B0403T.gdf ===\n",
      "\n",
      "=== Reading: B0501T.gdf ===\n",
      "\n",
      "=== Reading: B0502T.gdf ===\n",
      "\n",
      "=== Reading: B0503T.gdf ===\n",
      "\n",
      "=== Reading: B0601T.gdf ===\n",
      "\n",
      "=== Reading: B0602T.gdf ===\n",
      "\n",
      "=== Reading: B0603T.gdf ===\n",
      "\n",
      "=== Reading: B0701T.gdf ===\n",
      "\n",
      "=== Reading: B0702T.gdf ===\n",
      "\n",
      "=== Reading: B0703T.gdf ===\n",
      "\n",
      "=== Reading: B0801T.gdf ===\n",
      "\n",
      "=== Reading: B0802T.gdf ===\n",
      "\n",
      "=== Reading: B0803T.gdf ===\n",
      "\n",
      "=== Reading: B0901T.gdf ===\n",
      "\n",
      "=== Reading: B0902T.gdf ===\n",
      "\n",
      "=== Reading: B0903T.gdf ===\n",
      "From:{np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11} to {'LH': 10, 'RH': 11}\n",
      "From:{np.str_('1023'): 1, np.str_('32766'): 2, np.str_('768'): 3, np.str_('769'): 4, np.str_('770'): 5} to {'LH': 4, 'RH': 5}\n",
      "From:{np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11, np.str_('781'): 12} to {'LH': 10, 'RH': 11}\n",
      "From:{np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11} to {'LH': 10, 'RH': 11}\n",
      "From:{np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11} to {'LH': 10, 'RH': 11}\n",
      "From:{np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11, np.str_('781'): 12} to {'LH': 10, 'RH': 11}\n",
      "From:{np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11} to {'LH': 10, 'RH': 11}\n",
      "From:{np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11} to {'LH': 10, 'RH': 11}\n",
      "From:{np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11, np.str_('781'): 12} to {'LH': 10, 'RH': 11}\n",
      "From:{np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11} to {'LH': 10, 'RH': 11}\n",
      "From:{np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11} to {'LH': 10, 'RH': 11}\n",
      "From:{np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11, np.str_('781'): 12} to {'LH': 10, 'RH': 11}\n",
      "From:{np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11} to {'LH': 10, 'RH': 11}\n",
      "From:{np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11} to {'LH': 10, 'RH': 11}\n",
      "From:{np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11, np.str_('781'): 12} to {'LH': 10, 'RH': 11}\n",
      "From:{np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11} to {'LH': 10, 'RH': 11}\n",
      "From:{np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11} to {'LH': 10, 'RH': 11}\n",
      "From:{np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11, np.str_('781'): 12} to {'LH': 10, 'RH': 11}\n",
      "From:{np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11} to {'LH': 10, 'RH': 11}\n",
      "From:{np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11} to {'LH': 10, 'RH': 11}\n",
      "From:{np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11, np.str_('781'): 12} to {'LH': 10, 'RH': 11}\n",
      "From:{np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11} to {'LH': 10, 'RH': 11}\n",
      "From:{np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11} to {'LH': 10, 'RH': 11}\n",
      "From:{np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11, np.str_('781'): 12} to {'LH': 10, 'RH': 11}\n",
      "From:{np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11} to {'LH': 10, 'RH': 11}\n",
      "From:{np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11} to {'LH': 10, 'RH': 11}\n",
      "From:{np.str_('1023'): 1, np.str_('1077'): 2, np.str_('1078'): 3, np.str_('1079'): 4, np.str_('1081'): 5, np.str_('276'): 6, np.str_('277'): 7, np.str_('32766'): 8, np.str_('768'): 9, np.str_('769'): 10, np.str_('770'): 11, np.str_('781'): 12} to {'LH': 10, 'RH': 11}\n",
      "Dataset shape: (3680, 6, 1025)\n",
      "Dataset shape: (3680, 6, 1025)\n",
      "Cropping SEQ_LEN from 1025 -> 1024 to satisfy architecture constraint.\n",
      "CHANNELS: 6 SEQ_LEN: 1024\n"
     ]
    }
   ],
   "source": [
    "RESAMPLE_HZ = 256\n",
    "TMIN, TMAX = 0.0, 4.0\n",
    "EVENT_ID = {\"LH\": 769, \"RH\": 770}\n",
    "\n",
    "raws = load_gdf_files(DATA_DIR, RESAMPLE_HZ, verbose=True, mode=\"train\")\n",
    "X = make_dataset(raws, TMIN, TMAX, event_id=EVENT_ID)\n",
    "print(\"Dataset shape:\", X.shape)\n",
    "\n",
    "SEQ_LEN = X.shape[2]\n",
    "if SEQ_LEN % 16 != 0:\n",
    "    new_len = (SEQ_LEN // 16) * 16  # floor to nearest multiple of 16\n",
    "    print(f\"Cropping SEQ_LEN from {SEQ_LEN} -> {new_len} to satisfy architecture constraint.\")\n",
    "    X = X[:, :, :new_len]\n",
    "    \n",
    "CHANNELS = X.shape[1]\n",
    "SEQ_LEN  = X.shape[2]\n",
    "print(\"CHANNELS:\", CHANNELS, \"SEQ_LEN:\", SEQ_LEN)\n",
    "assert SEQ_LEN % 16 == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2ec847ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches: 57\n"
     ]
    }
   ],
   "source": [
    "# From first\n",
    "class EEGTensorDataset(Dataset):\n",
    "    def __init__(self, X: np.ndarray, y: Optional[np.ndarray] = None, zscore_per_channel: bool = True):\n",
    "        \"\"\"\n",
    "        X: (N, C, L)\n",
    "        y: optional (N,)\n",
    "        zscore_per_channel:\n",
    "          - global per-channel mean/std computed across (N,L) for each channel\n",
    "        \"\"\"\n",
    "        assert X.ndim == 3\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = None if y is None else y.astype(np.int64)\n",
    "\n",
    "        if zscore_per_channel:\n",
    "            mean = self.X.mean(axis=(0, 2), keepdims=True)\n",
    "            std  = self.X.std(axis=(0, 2), keepdims=True) + 1e-6\n",
    "            self.X = (self.X - mean) / std\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.from_numpy(self.X[idx])  # (C, L)\n",
    "        if self.y is None:\n",
    "            return x\n",
    "        return x, int(self.y[idx])\n",
    "\n",
    "# Unconditional GAN: we only use x; y is available if you want conditional later\n",
    "dataset = EEGTensorDataset(X, y=None, zscore_per_channel=True)\n",
    "loader = DataLoader(dataset, batch_size=64, shuffle=True, drop_last=True, num_workers=0, pin_memory=(DEVICE==\"cuda\"))\n",
    "print(\"Batches:\", len(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dff7118a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G params: 4.918086 M\n",
      "D params: 0.725185 M\n"
     ]
    }
   ],
   "source": [
    "# Cell 3 â€” Generator & Critic (1D Conv, Standard WGAN-GP Style)\n",
    "# This is a *standard* WGAN-GP setup: the \"discriminator\" is a *critic* with a linear output (no sigmoid).\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, (nn.Conv1d, nn.ConvTranspose1d, nn.Linear)):\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "class Generator1D(nn.Module):\n",
    "    def __init__(self, z_dim: int, out_channels: int, seq_len: int, base: int = 64):\n",
    "        super().__init__()\n",
    "        assert seq_len % 16 == 0, \"For this template, seq_len should be divisible by 16.\"\n",
    "        self.z_dim = z_dim\n",
    "        self.out_channels = out_channels\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        # Project noise to a small temporal resolution then upsample x16 via ConvTranspose1d\n",
    "        self.init_len = seq_len // 16\n",
    "        self.fc = nn.Linear(z_dim, base * 8 * self.init_len)\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.ConvTranspose1d(base * 8, base * 4, kernel_size=4, stride=2, padding=1),  # x2\n",
    "            nn.BatchNorm1d(base * 4),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose1d(base * 4, base * 2, kernel_size=4, stride=2, padding=1),  # x4\n",
    "            nn.BatchNorm1d(base * 2),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose1d(base * 2, base, kernel_size=4, stride=2, padding=1),      # x8\n",
    "            nn.BatchNorm1d(base),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose1d(base, out_channels, kernel_size=4, stride=2, padding=1),  # x16\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.fc(z)\n",
    "        x = x.view(z.size(0), -1, self.init_len)\n",
    "        x = self.net(x)\n",
    "        return x\n",
    "\n",
    "class Critic1D(nn.Module):\n",
    "    def __init__(self, in_channels: int, seq_len: int, base: int = 64):\n",
    "        super().__init__()\n",
    "        assert seq_len % 16 == 0, \"For this template, seq_len should be divisible by 16.\"\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, base, kernel_size=4, stride=2, padding=1),   # /2\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv1d(base, base * 2, kernel_size=4, stride=2, padding=1),      # /4\n",
    "            nn.InstanceNorm1d(base * 2, affine=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv1d(base * 2, base * 4, kernel_size=4, stride=2, padding=1),  # /8\n",
    "            nn.InstanceNorm1d(base * 4, affine=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv1d(base * 4, base * 8, kernel_size=4, stride=2, padding=1),  # /16\n",
    "            nn.InstanceNorm1d(base * 8, affine=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "        self.out = nn.Linear(base * 8 * (seq_len // 16), 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.net(x)\n",
    "        h = h.view(x.size(0), -1)\n",
    "        return self.out(h).view(-1)\n",
    "\n",
    "# --- Model configs ---\n",
    "Z_DIM = 128\n",
    "\n",
    "G = Generator1D(z_dim=Z_DIM, out_channels=CHANNELS, seq_len=SEQ_LEN).to(DEVICE)\n",
    "D = Critic1D(in_channels=CHANNELS, seq_len=SEQ_LEN).to(DEVICE)\n",
    "\n",
    "G.apply(weights_init)\n",
    "D.apply(weights_init)\n",
    "\n",
    "print(\"G params:\", sum(p.numel() for p in G.parameters())/1e6, \"M\")\n",
    "print(\"D params:\", sum(p.numel() for p in D.parameters())/1e6, \"M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2143adad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 â€” WGAN-GP Utilities (Gradient Penalty, Losses)\n",
    "# WGAN-GP objective uses:\n",
    "#   loss_D = E[D(fake)] - E[D(real)] + lambda_gp * (||âˆ‡_x_hat D(x_hat)||_2 - 1)^2\n",
    "#   loss_G = -E[D(fake)]\n",
    "# This matches the standard WGAN-GP formulation referenced in the survey paper. îˆ€fileciteîˆ‚turn0file0îˆ\n",
    "\n",
    "def gradient_penalty(critic: nn.Module, real: torch.Tensor, fake: torch.Tensor) -> torch.Tensor:\n",
    "    bsz = real.size(0)\n",
    "    eps = torch.rand(bsz, 1, 1, device=real.device)\n",
    "    x_hat = eps * real + (1 - eps) * fake\n",
    "    x_hat.requires_grad_(True)\n",
    "\n",
    "    d_hat = critic(x_hat)\n",
    "    grads = torch.autograd.grad(\n",
    "        outputs=d_hat,\n",
    "        inputs=x_hat,\n",
    "        grad_outputs=torch.ones_like(d_hat),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )[0]\n",
    "    grads = grads.view(bsz, -1)\n",
    "    gp = ((grads.norm(2, dim=1) - 1.0) ** 2).mean()\n",
    "    return gp\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_generator(generator: nn.Module, n: int, z_dim: int) -> torch.Tensor:\n",
    "    z = torch.randn(n, z_dim, device=DEVICE)\n",
    "    return generator(z).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "83a8a47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_gradient_vector(parameters) -> torch.Tensor:\n",
    "    \"\"\"Flatten all gradients into a single vector.\"\"\"\n",
    "    grads = []\n",
    "    for p in parameters:\n",
    "        if p.grad is not None:\n",
    "            grads.append(p.grad.detach().view(-1))\n",
    "    if len(grads) == 0:\n",
    "        return None\n",
    "    return torch.cat(grads)\n",
    "\n",
    "def cosine_similarity_gradients(grad_vec1: torch.Tensor, grad_vec2: torch.Tensor) -> float:\n",
    "    \"\"\"Compute cosine similarity between two gradient vectors.\"\"\"\n",
    "    if grad_vec1 is None or grad_vec2 is None:\n",
    "        return 0.0\n",
    "    cos_sim = F.cosine_similarity(\n",
    "        grad_vec1.unsqueeze(0), \n",
    "        grad_vec2.unsqueeze(0)\n",
    "    ).item()\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5c8993b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_n_critic(current_step: int, n_critic: int, n_critic_initial: int, n_critic_final: int, n_critic_rampup_steps: int) -> int:\n",
    "    \"\"\"Dynamically adjust the number of critic updates per generator update.\"\"\"\n",
    "    if current_step < n_critic_rampup_steps:\n",
    "        n_critic = n_critic_initial + (n_critic_final - n_critic_initial) * (current_step / n_critic_rampup_steps)\n",
    "        n_critic = int(round(n_critic))\n",
    "    else:\n",
    "        n_critic = n_critic_final\n",
    "    return n_critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b187349a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 â€” Adaptive Discriminator Controller (Gradient Direction Consistency)\n",
    "# Uses cosine similarity between consecutive gradient vectors to measure training stability.\n",
    "# High cosine similarity (â‰ˆ1) = consistent gradient direction (stable training)\n",
    "# Low/negative cosine similarity = oscillating gradients (potentially unstable)\n",
    "\n",
    "@dataclass\n",
    "class TrainState:\n",
    "    step: int = 0\n",
    "    epoch: int = 0\n",
    "    n_critic: int = 5\n",
    "    n_gen: int = 1\n",
    "    lambda_gp: float = 10.0\n",
    "\n",
    "    # Common diagnostics\n",
    "    wasserstein_gap_ema: float = 0.0\n",
    "    ema_beta: float = 0.99\n",
    "    \n",
    "    # Gradient consistency tracking\n",
    "    d_grad_cos_sim: float = 0.0  # Discriminator gradient consistency\n",
    "    g_grad_cos_sim: float = 0.0  # Generator gradient consistency\n",
    "\n",
    "class AdaptiveDiscriminatorController:\n",
    "    def __init__(\n",
    "        self,\n",
    "        cos_sim_threshold_high: float = 0.9,  # Too consistent â†’ reduce n_critic\n",
    "        cos_sim_threshold_low: float = 0.3,   # Too inconsistent â†’ increase n_critic\n",
    "        k_min: int | None = None,\n",
    "        k_max: int | None = None,\n",
    "        ema_beta: float = 0.95,\n",
    "    ):\n",
    "        self.cos_sim_threshold_high = cos_sim_threshold_high\n",
    "        self.cos_sim_threshold_low = cos_sim_threshold_low\n",
    "        self.k_min = k_min\n",
    "        self.k_max = k_max\n",
    "        self.ema_beta = ema_beta\n",
    "        \n",
    "        # Store previous gradient vectors\n",
    "        self.prev_d_grad = None\n",
    "        self.prev_g_grad = None\n",
    "        \n",
    "        # EMA of cosine similarities\n",
    "        self.d_cos_sim_ema = None\n",
    "        self.g_cos_sim_ema = None\n",
    "\n",
    "    def on_batch_start(self, state: TrainState) -> None:\n",
    "        pass\n",
    "\n",
    "    def on_after_critic_update(\n",
    "        self, \n",
    "        state: TrainState, \n",
    "        metrics: Dict[str, float], \n",
    "        optim_D: torch.optim.Optimizer,\n",
    "        D: nn.Module = None,\n",
    "    ) -> None:\n",
    "        if D is None:\n",
    "            return\n",
    "        \n",
    "        # Compute current gradient vector\n",
    "        curr_d_grad = compute_gradient_vector(D.parameters())\n",
    "        \n",
    "        if curr_d_grad is not None and self.prev_d_grad is not None:\n",
    "            cos_sim = cosine_similarity_gradients(curr_d_grad, self.prev_d_grad)\n",
    "            \n",
    "            # Update EMA\n",
    "            if self.d_cos_sim_ema is None:\n",
    "                self.d_cos_sim_ema = cos_sim\n",
    "            else:\n",
    "                self.d_cos_sim_ema = (\n",
    "                    self.ema_beta * self.d_cos_sim_ema \n",
    "                    + (1 - self.ema_beta) * cos_sim\n",
    "                )\n",
    "            \n",
    "            state.d_grad_cos_sim = self.d_cos_sim_ema\n",
    "            metrics[\"d_grad_cos_sim\"] = cos_sim\n",
    "            metrics[\"d_grad_cos_sim_ema\"] = self.d_cos_sim_ema\n",
    "            \n",
    "            # Adaptive control based on gradient consistency\n",
    "            if self.d_cos_sim_ema > self.cos_sim_threshold_high:\n",
    "                # Gradients too consistent â†’ D might be too strong\n",
    "                if self.k_min is not None:\n",
    "                    state.n_critic = max(self.k_min, state.n_critic - 1)\n",
    "                else:\n",
    "                    state.n_critic = max(1, state.n_critic - 1)\n",
    "            elif self.d_cos_sim_ema < self.cos_sim_threshold_low:\n",
    "                # Gradients too inconsistent â†’ D might need more updates\n",
    "                if self.k_max is not None:\n",
    "                    state.n_critic = min(self.k_max, state.n_critic + 1)\n",
    "                else:\n",
    "                    state.n_critic = state.n_critic + 1\n",
    "        \n",
    "        # Store for next iteration\n",
    "        self.prev_d_grad = curr_d_grad.clone() if curr_d_grad is not None else None\n",
    "\n",
    "    def on_after_generator_update(\n",
    "        self, \n",
    "        state: TrainState, \n",
    "        metrics: Dict[str, float], \n",
    "        optim_G: torch.optim.Optimizer,\n",
    "        G: nn.Module = None,\n",
    "    ) -> None:\n",
    "        if G is None:\n",
    "            return\n",
    "            \n",
    "        curr_g_grad = compute_gradient_vector(G.parameters())\n",
    "        \n",
    "        if curr_g_grad is not None and self.prev_g_grad is not None:\n",
    "            cos_sim = cosine_similarity_gradients(curr_g_grad, self.prev_g_grad)\n",
    "            \n",
    "            if self.g_cos_sim_ema is None:\n",
    "                self.g_cos_sim_ema = cos_sim\n",
    "            else:\n",
    "                self.g_cos_sim_ema = (\n",
    "                    self.ema_beta * self.g_cos_sim_ema \n",
    "                    + (1 - self.ema_beta) * cos_sim\n",
    "                )\n",
    "            \n",
    "            state.g_grad_cos_sim = self.g_cos_sim_ema\n",
    "            metrics[\"g_grad_cos_sim\"] = cos_sim\n",
    "            metrics[\"g_grad_cos_sim_ema\"] = self.g_cos_sim_ema\n",
    "            \n",
    "        self.prev_g_grad = curr_g_grad.clone() if curr_g_grad is not None else None\n",
    "\n",
    "controller = AdaptiveDiscriminatorController(k_max=None, k_min=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "429ebaf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.0001 BETAS: (0.0, 0.9) n_critic: 5 n_gen: 2 lambda_gp: 10.0\n"
     ]
    }
   ],
   "source": [
    "# Cell 6 â€” Optimizers & Hyperparameters (Standard WGAN-GP Defaults)\n",
    "LR = 1e-4\n",
    "BETAS = (0.0, 0.9)   # standard WGAN-GP choice\n",
    "\n",
    "optim_G = torch.optim.Adam(G.parameters(), lr=LR, betas=BETAS)\n",
    "optim_D = torch.optim.Adam(D.parameters(), lr=LR, betas=BETAS)\n",
    "\n",
    "state = TrainState(step=0, epoch=0, n_gen=2, n_critic=5, lambda_gp=10.0)\n",
    "\n",
    "print(\"LR:\", LR, \"BETAS:\", BETAS, \"n_critic:\", state.n_critic, \"n_gen:\", state.n_gen, \"lambda_gp:\", state.lambda_gp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "db72a7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run directory: /Users/ratchanonkhongsawi/Desktop/CMKL/3rd/S2/Research/gans_eeg/gans_eeg/pluem/models/grad_cos_sim_2\n",
      "[Epoch 000/1000] [Batch 0000/0057] [n_critic: 1] [n_gen: 2] [gap: +0.602 | ema: +0.018] [GP: 231.649] [D_cos: +0.975] [G_cos: +0.000] [D: +2315.886] [G: +1.056]\n",
      "ðŸ† New BEST (gap_ema=0.0177) -> /Users/ratchanonkhongsawi/Desktop/CMKL/3rd/S2/Research/gans_eeg/gans_eeg/pluem/models/grad_cos_sim_2/checkpoints/best_grad_cos_sim_2.pt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 181\u001b[39m\n\u001b[32m    177\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mâœ… Saved final checkpoint: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    179\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m history\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m history = \u001b[43mtrain_wgan_gp\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim_G\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim_D\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontroller\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m    \u001b[49m\u001b[43mz_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mZ_DIM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_every\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_every_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m500\u001b[39;49m\n\u001b[32m    184\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 92\u001b[39m, in \u001b[36mtrain_wgan_gp\u001b[39m\u001b[34m(loader, G, D, optim_G, optim_D, state, controller, z_dim, epochs, log_every, save_every_steps, best_metric)\u001b[39m\n\u001b[32m     89\u001b[39m loss_D = (d_fake - d_real) + state.lambda_gp * gp\n\u001b[32m     91\u001b[39m optim_D.zero_grad(set_to_none=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m \u001b[43mloss_D\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[38;5;66;03m# Call controller BEFORE optimizer step (gradients still exist)\u001b[39;00m\n\u001b[32m     95\u001b[39m metrics_D = {\n\u001b[32m     96\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33md_real\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(d_real.item()),\n\u001b[32m     97\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33md_fake\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(d_fake.item()),\n\u001b[32m   (...)\u001b[39m\u001b[32m    101\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mloss_D\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(loss_D.item()),\n\u001b[32m    102\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/3rd_research/lib/python3.11/site-packages/torch/_tensor.py:630\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    620\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    621\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    622\u001b[39m         Tensor.backward,\n\u001b[32m    623\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    628\u001b[39m         inputs=inputs,\n\u001b[32m    629\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/3rd_research/lib/python3.11/site-packages/torch/autograd/__init__.py:364\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    359\u001b[39m     retain_graph = create_graph\n\u001b[32m    361\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    362\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    363\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/3rd_research/lib/python3.11/site-packages/torch/autograd/graph.py:865\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    863\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    864\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m865\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    866\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    869\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Cell 7 â€” Training Loop (WGAN-GP + Gradient Direction Consistency Adaptive Control)\n",
    "# Logs:\n",
    "#   - wasserstein gap: E[D(real)] - E[D(fake)]\n",
    "#   - gp: gradient penalty term\n",
    "#   - d_grad_cos_sim: discriminator gradient direction consistency\n",
    "#   - g_grad_cos_sim: generator gradient direction consistency\n",
    "#   - losses for D and G\n",
    "\n",
    "RUN_TAG = \"grad_cos_sim_2\"\n",
    "TAG_DIR = MODEL_DIR / RUN_TAG\n",
    "CHECKPOINT_DIR = TAG_DIR / \"checkpoints\"\n",
    "\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TAG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Run directory: {TAG_DIR}\")\n",
    "\n",
    "def save_checkpoint(path, G, D, optim_G, optim_D, state, history):\n",
    "    \"\"\"Save full training checkpoint.\"\"\"\n",
    "    payload = {\n",
    "        \"G\": G.state_dict(),\n",
    "        \"D\": D.state_dict(),\n",
    "        \"optim_G\": optim_G.state_dict(),\n",
    "        \"optim_D\": optim_D.state_dict(),\n",
    "        \"state\": state.__dict__,\n",
    "        \"history\": dict(history),\n",
    "    }\n",
    "    torch.save(payload, path)\n",
    "\n",
    "def load_checkpoint(path, G, D, optim_G=None, optim_D=None):\n",
    "    \"\"\"Load checkpoint and return state + history.\"\"\"\n",
    "    ckpt = torch.load(path, map_location=DEVICE)\n",
    "    G.load_state_dict(ckpt[\"G\"])\n",
    "    D.load_state_dict(ckpt[\"D\"])\n",
    "    if optim_G is not None and \"optim_G\" in ckpt:\n",
    "        optim_G.load_state_dict(ckpt[\"optim_G\"])\n",
    "    if optim_D is not None and \"optim_D\" in ckpt:\n",
    "        optim_D.load_state_dict(ckpt[\"optim_D\"])\n",
    "    state = TrainState(**ckpt[\"state\"])\n",
    "    hist = ckpt.get(\"history\", {})\n",
    "    return state, hist\n",
    "\n",
    "def train_wgan_gp(\n",
    "    loader: DataLoader,\n",
    "    G: nn.Module,\n",
    "    D: nn.Module,\n",
    "    optim_G: torch.optim.Optimizer,\n",
    "    optim_D: torch.optim.Optimizer,\n",
    "    state: TrainState,\n",
    "    controller: AdaptiveDiscriminatorController,\n",
    "    z_dim: int,\n",
    "    epochs: int = 10,\n",
    "    log_every: int = 50,\n",
    "    save_every_steps: int = 500,\n",
    "    best_metric: str = \"gap_ema\",\n",
    "):\n",
    "    G.train(); D.train()\n",
    "    \n",
    "    # History for plotting\n",
    "    history = {\n",
    "        \"step\": [], \"loss_D\": [], \"loss_G\": [], \"gap\": [], \"gap_ema\": [],\n",
    "        \"gp\": [], \"n_critic\": [], \"d_grad_cos_sim\": [], \"g_grad_cos_sim\": [],\n",
    "        \"n_critic\": [], \"n_gen\": [],\n",
    "    }\n",
    "    \n",
    "    best_score = getattr(state, 'best_score', -1e18)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        state.epoch = epoch\n",
    "        for batch_idx, real in enumerate(loader):\n",
    "            state.step += 1\n",
    "            controller.on_batch_start(state)\n",
    "\n",
    "            real = real.to(DEVICE)\n",
    "            bsz = real.size(0)\n",
    "\n",
    "            # -------------------------\n",
    "            # Critic updates (n_critic)\n",
    "            # -------------------------\n",
    "            metrics_D = {}\n",
    "            for _ in range(state.n_critic):\n",
    "                z = torch.randn(bsz, z_dim, device=DEVICE)\n",
    "                fake = G(z).detach()\n",
    "\n",
    "                d_real = D(real).mean()\n",
    "                d_fake = D(fake).mean()\n",
    "                gap = (d_real - d_fake).item()\n",
    "\n",
    "                gp = gradient_penalty(D, real, fake)\n",
    "                loss_D = (d_fake - d_real) + state.lambda_gp * gp\n",
    "\n",
    "                optim_D.zero_grad(set_to_none=True)\n",
    "                loss_D.backward()\n",
    "                \n",
    "                # Call controller BEFORE optimizer step (gradients still exist)\n",
    "                metrics_D = {\n",
    "                    \"d_real\": float(d_real.item()),\n",
    "                    \"d_fake\": float(d_fake.item()),\n",
    "                    \"gap\": float(gap),\n",
    "                    \"gap_ema\": float(state.wasserstein_gap_ema),\n",
    "                    \"gp\": float(gp.item()),\n",
    "                    \"loss_D\": float(loss_D.item()),\n",
    "                }\n",
    "                controller.on_after_critic_update(state, metrics_D, optim_D, D=D)\n",
    "                \n",
    "                optim_D.step()\n",
    "\n",
    "                # Update EMA of the gap\n",
    "                state.wasserstein_gap_ema = state.ema_beta * state.wasserstein_gap_ema + (1 - state.ema_beta) * gap\n",
    "                metrics_D[\"gap_ema\"] = float(state.wasserstein_gap_ema)\n",
    "\n",
    "            # -------------------------\n",
    "            # Generator update\n",
    "            # -------------------------\n",
    "            z = torch.randn(bsz, z_dim, device=DEVICE)\n",
    "            fake = G(z)\n",
    "            loss_G = -D(fake).mean()\n",
    "\n",
    "            optim_G.zero_grad(set_to_none=True)\n",
    "            loss_G.backward()\n",
    "            \n",
    "            metrics_G = {\"loss_G\": float(loss_G.item())}\n",
    "            # Call controller BEFORE optimizer step (gradients still exist)\n",
    "            controller.on_after_generator_update(state, metrics_G, optim_G, G=G)\n",
    "            \n",
    "            optim_G.step()\n",
    "\n",
    "            # -------------------------\n",
    "            # Log history\n",
    "            # -------------------------\n",
    "            history[\"step\"].append(state.step)\n",
    "            history[\"loss_D\"].append(metrics_D.get(\"loss_D\", 0))\n",
    "            history[\"loss_G\"].append(metrics_G.get(\"loss_G\", 0))\n",
    "            history[\"gap\"].append(metrics_D.get(\"gap\", 0))\n",
    "            \n",
    "            history[\"gap_ema\"].append(metrics_D.get(\"gap_ema\", 0))\n",
    "            history[\"gp\"].append(metrics_D.get(\"gp\", 0))\n",
    "            history[\"n_critic\"].append(state.n_critic)\n",
    "            history[\"n_gen\"].append(state.n_gen)\n",
    "            history[\"d_grad_cos_sim\"].append(metrics_D.get(\"d_grad_cos_sim_ema\", 0))\n",
    "            history[\"g_grad_cos_sim\"].append(metrics_G.get(\"g_grad_cos_sim_ema\", 0))\n",
    "            \n",
    "\n",
    "            if (batch_idx % log_every) == 0:\n",
    "                d_cos = metrics_D.get(\"d_grad_cos_sim_ema\", 0)\n",
    "                g_cos = metrics_G.get(\"g_grad_cos_sim_ema\", 0)\n",
    "                print(\n",
    "                    f\"[Epoch {epoch:03d}/{epochs:03d}] [Batch {batch_idx:04d}/{len(loader):04d}] \"\n",
    "                    f\"[n_critic: {state.n_critic}] [n_gen: {state.n_gen}] \"\n",
    "                    f\"[gap: {metrics_D['gap']:+.3f} | ema: {metrics_D['gap_ema']:+.3f}] \"\n",
    "                    f\"[GP: {metrics_D['gp']:.3f}] \"\n",
    "                    f\"[D_cos: {d_cos:+.3f}] [G_cos: {g_cos:+.3f}] \"\n",
    "                    f\"[D: {metrics_D['loss_D']:+.3f}] [G: {metrics_G['loss_G']:+.3f}]\"\n",
    "                )\n",
    "\n",
    "            # -------------------------\n",
    "            # Save periodic checkpoint\n",
    "            # -------------------------\n",
    "            if save_every_steps > 0 and (state.step % save_every_steps) == 0:\n",
    "                ckpt_path = CHECKPOINT_DIR / f\"ckpt_{RUN_TAG}_step_{state.step}.pt\"\n",
    "                save_checkpoint(str(ckpt_path), G, D, optim_G, optim_D, state, history)\n",
    "                print(f\"ðŸ’¾ Saved checkpoint: {ckpt_path}\")\n",
    "\n",
    "            # -------------------------\n",
    "            # Save best checkpoint\n",
    "            # -------------------------\n",
    "            if metrics_D:\n",
    "                score = float(metrics_D.get(best_metric, -1e18))\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_path = CHECKPOINT_DIR / f\"best_{RUN_TAG}.pt\"\n",
    "                    save_checkpoint(str(best_path), G, D, optim_G, optim_D, state, history)\n",
    "                    print(f\"ðŸ† New BEST ({best_metric}={score:.4f}) -> {best_path}\")\n",
    "    \n",
    "    # Save final checkpoint\n",
    "    final_path = TAG_DIR / f\"final_{RUN_TAG}.pt\"\n",
    "    save_checkpoint(str(final_path), G, D, optim_G, optim_D, state, history)\n",
    "    print(f\"âœ… Saved final checkpoint: {final_path}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "history = train_wgan_gp(\n",
    "    loader, G, D, optim_G, optim_D, state, controller, \n",
    "    z_dim=Z_DIM, epochs=1000, log_every=50, save_every_steps=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd61a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8 â€” Quick Sanity Sample (Optional)\n",
    "# This just samples a few generated sequences so you can inspect shapes.\n",
    "with torch.no_grad():\n",
    "    fake = sample_generator(G, n=4, z_dim=Z_DIM)\n",
    "print(\"Generated batch shape:\", tuple(fake.shape))  # (N, C, L)\n",
    "print(\"Example stats:\", fake.mean().item(), fake.std().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70aec8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9 â€” Save Models (Inference-only weights)\n",
    "# Save just the model weights for inference (smaller files, no optimizer state)\n",
    "\n",
    "# Generator only (for generating new samples)\n",
    "generator_path = TAG_DIR / f\"generator_{RUN_TAG}.pt\"\n",
    "torch.save(G.state_dict(), generator_path)\n",
    "print(f\"âœ… Saved Generator: {generator_path}\")\n",
    "\n",
    "# Critic only (for evaluation if needed)\n",
    "critic_path = TAG_DIR / f\"critic_{RUN_TAG}.pt\"\n",
    "torch.save(D.state_dict(), critic_path)\n",
    "print(f\"âœ… Saved Critic: {critic_path}\")\n",
    "\n",
    "# Save training history as well\n",
    "import json\n",
    "history_path = TAG_DIR / f\"history_{RUN_TAG}.json\"\n",
    "with open(history_path, \"w\") as f:\n",
    "    json.dump(history, f)\n",
    "print(f\"âœ… Saved history: {history_path}\")\n",
    "\n",
    "print(f\"\\nðŸ“ All models saved to: {TAG_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e958d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8 â€” Training Curve Visualization (with Gradient Consistency)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_history(history: dict):\n",
    "    if len(history.get(\"step\", [])) == 0:\n",
    "        print(\"History is empty. Train first.\")\n",
    "        return\n",
    "\n",
    "    step = history[\"step\"]\n",
    "    fig, axes = plt.subplots(4, 2, figsize=(16, 12))\n",
    "\n",
    "    # Losses\n",
    "    axes[0, 0].plot(step, history[\"loss_G\"], label=\"loss_G\", alpha=0.8)\n",
    "    axes[0, 0].plot(step, history[\"loss_D\"], label=\"loss_D\", alpha=0.8)\n",
    "    axes[0, 0].set_title(\"Generator / Critic Loss\")\n",
    "    axes[0, 0].set_xlabel(\"step\")\n",
    "    axes[0, 0].set_ylabel(\"loss\")\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True)\n",
    "\n",
    "    # Wasserstein gap\n",
    "    axes[0, 1].plot(step, history[\"gap\"], label=\"gap\", alpha=0.5)\n",
    "    axes[0, 1].plot(step, history[\"gap_ema\"], label=\"gap_ema\", linewidth=2)\n",
    "    axes[0, 1].set_title(\"Wasserstein Gap\")\n",
    "    axes[0, 1].set_xlabel(\"step\")\n",
    "    axes[0, 1].set_ylabel(\"E[D(real)] - E[D(fake)]\")\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True)\n",
    "\n",
    "    # Gradient Penalty\n",
    "    axes[1, 0].plot(step, history[\"gp\"], label=\"GP\", color=\"orange\")\n",
    "    axes[1, 0].set_title(\"Gradient Penalty\")\n",
    "    axes[1, 0].set_xlabel(\"step\")\n",
    "    axes[1, 0].set_ylabel(\"gp\")\n",
    "    axes[1, 0].grid(True)\n",
    "\n",
    "    # Gradient Direction Consistency (Cosine Similarity)\n",
    "    axes[1, 1].plot(step, history[\"d_grad_cos_sim\"], label=\"D grad cos_sim\", alpha=0.8)\n",
    "    axes[1, 1].plot(step, history[\"g_grad_cos_sim\"], label=\"G grad cos_sim\", alpha=0.8)\n",
    "    axes[1, 1].axhline(y=0.9, color='r', linestyle='--', alpha=0.5, label='high threshold')\n",
    "    axes[1, 1].axhline(y=0.3, color='b', linestyle='--', alpha=0.5, label='low threshold')\n",
    "    axes[1, 1].set_title(\"Gradient Direction Consistency (Cosine Similarity)\")\n",
    "    axes[1, 1].set_xlabel(\"step\")\n",
    "    axes[1, 1].set_ylabel(\"cosine similarity\")\n",
    "    axes[1, 1].set_ylim(-1.1, 1.1)\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True)\n",
    "\n",
    "    # n_critic over time\n",
    "    axes[2, 0].plot(step, history[\"n_critic\"], label=\"n_critic\", color=\"green\", drawstyle='steps-post')\n",
    "    axes[2, 0].set_title(\"Adaptive n_critic\")\n",
    "    axes[2, 0].set_xlabel(\"step\")\n",
    "    axes[2, 0].set_ylabel(\"n_critic\")\n",
    "    axes[2, 0].set_ylim(0, 8)\n",
    "    axes[2, 0].grid(True)\n",
    "    \n",
    "\n",
    "    # Combined: D cosine sim vs n_critic\n",
    "    ax2 = axes[2, 1]\n",
    "    ax2.plot(step, history[\"d_grad_cos_sim\"], label=\"D grad cos_sim\", color=\"blue\", alpha=0.7)\n",
    "    ax2.set_xlabel(\"step\")\n",
    "    ax2.set_ylabel(\"D grad cosine similarity\", color=\"blue\")\n",
    "    ax2.tick_params(axis='y', labelcolor=\"blue\")\n",
    "    ax2.set_ylim(-1.1, 1.1)\n",
    "    \n",
    "    ax2_twin = ax2.twinx()\n",
    "    ax2_twin.plot(step, history[\"n_critic\"], label=\"n_critic\", color=\"green\", alpha=0.7, drawstyle='steps-post')\n",
    "    ax2_twin.set_ylabel(\"n_critic\", color=\"green\")\n",
    "    ax2_twin.tick_params(axis='y', labelcolor=\"green\")\n",
    "    ax2_twin.set_ylim(0, 8)\n",
    "    \n",
    "    axes[2, 1].set_title(\"D Gradient Consistency vs n_critic\")\n",
    "    axes[2, 1].grid(True)\n",
    "    \n",
    "    # n_gen over time\n",
    "    axes[3, 0].plot(step, history[\"n_gen\"], label=\"n_gen\", color=\"purple\", drawstyle='steps-post')\n",
    "    axes[3, 0].set_title(\"Adaptive n_gen\")\n",
    "    axes[3, 0].set_xlabel(\"step\")\n",
    "    axes[3, 0].set_ylabel(\"n_gen\")\n",
    "    axes[3, 0].set_ylim(0, 8)\n",
    "    axes[3, 0].grid(True)\n",
    "    \n",
    "    # Combined: G cosine sim vs n_gen\n",
    "    ax4 = axes[3, 1]\n",
    "    ax4.plot(step, history[\"g_grad_cos_sim\"], label=\"G grad cos_sim\", color=\"blue\", alpha=0.7)\n",
    "    ax4.set_xlabel(\"step\")\n",
    "    ax4.set_ylabel(\"G grad cosine similarity\", color=\"blue\")\n",
    "    ax4.tick_params(axis='y', labelcolor=\"blue\")\n",
    "    ax4.set_ylim(-1.1, 1.1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(TAG_DIR / f\"training_history_{RUN_TAG}.png\")\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3rd_research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
