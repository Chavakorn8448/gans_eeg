{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd20fb5c",
   "metadata": {},
   "source": [
    "# WGAN-GP (Standard) + Adaptive-Discriminator Hooks (Template)\n",
    "\n",
    "This notebook provides:\n",
    "- A **standard WGAN-GP** training loop (critic + gradient penalty).\n",
    "- A **pluggable \"Adaptive Discriminator\" controller** with clearly marked hooks so you can experiment with different discriminator/critic adjustment strategies (e.g., dynamic `n_critic`, LR, GP weight, architecture toggles, etc.).\n",
    "\n",
    "> Notes: WGAN-GP uses a gradient penalty to enforce the 1-Lipschitz constraint instead of weight clipping (see WGAN-GP objective in the referenced survey paper). ÓàÄfileciteÓàÇturn0file0ÓàÅ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45a012ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 ‚Äî Imports & Reproducibility\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Dict, Any, Tuple\n",
    "from pathlib import Path\n",
    "import mne\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(42)\n",
    "print(\"Device:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b49240c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models(tag_dir: Path, run_tag: str, G: nn.Module, D: nn.Module, history: Dict[str, list]):\n",
    "    # Generator only (for generating new samples)\n",
    "    generator_path = tag_dir / f\"generator_{run_tag}.pt\"\n",
    "    torch.save(G.state_dict(), generator_path)\n",
    "    print(f\"‚úÖ Saved Generator: {generator_path}\")\n",
    "\n",
    "    # Critic only (for evaluation if needed)\n",
    "    critic_path = tag_dir / f\"critic_{run_tag}.pt\"\n",
    "    torch.save(D.state_dict(), critic_path)\n",
    "    print(f\"‚úÖ Saved Critic: {critic_path}\")\n",
    "\n",
    "    # Save training history as well\n",
    "    import json\n",
    "    history_path = tag_dir / f\"history_{run_tag}.json\"\n",
    "    with open(history_path, \"w\") as f:\n",
    "        json.dump(history, f)\n",
    "    print(f\"‚úÖ Saved history: {history_path}\")\n",
    "\n",
    "    print(f\"\\nüìÅ All models saved to: {tag_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "677f5aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history: dict, TAG_DIR: Path , RUN_TAG: str = \"default\"):\n",
    "    if len(history.get(\"step\", [])) == 0:\n",
    "        print(\"History is empty. Train first.\")\n",
    "        return\n",
    "\n",
    "    step = history[\"step\"]\n",
    "    fig, axes = plt.subplots(4, 2, figsize=(16, 12))\n",
    "\n",
    "    # Losses\n",
    "    axes[0, 0].plot(step, history[\"loss_G\"], label=\"loss_G\", alpha=0.8)\n",
    "    axes[0, 0].plot(step, history[\"loss_D\"], label=\"loss_D\", alpha=0.8)\n",
    "    axes[0, 0].set_title(\"Generator / Critic Loss\")\n",
    "    axes[0, 0].set_xlabel(\"step\")\n",
    "    axes[0, 0].set_ylabel(\"loss\")\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True)\n",
    "\n",
    "    # Wasserstein gap\n",
    "    axes[0, 1].plot(step, history[\"gap\"], label=\"gap\", alpha=0.5)\n",
    "    axes[0, 1].plot(step, history[\"gap_ema\"], label=\"gap_ema\", linewidth=2)\n",
    "    axes[0, 1].set_title(\"Wasserstein Gap\")\n",
    "    axes[0, 1].set_xlabel(\"step\")\n",
    "    axes[0, 1].set_ylabel(\"E[D(real)] - E[D(fake)]\")\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True)\n",
    "\n",
    "    # Gradient Penalty\n",
    "    axes[1, 0].plot(step, history[\"gp\"], label=\"GP\", color=\"orange\")\n",
    "    axes[1, 0].set_title(\"Gradient Penalty\")\n",
    "    axes[1, 0].set_xlabel(\"step\")\n",
    "    axes[1, 0].set_ylabel(\"gp\")\n",
    "    axes[1, 0].grid(True)\n",
    "\n",
    "    # Gradient Direction Consistency (Cosine Similarity)\n",
    "    axes[1, 1].plot(step, history[\"d_grad_cos_sim\"], label=\"D grad cos_sim\", alpha=0.8)\n",
    "    axes[1, 1].plot(step, history[\"g_grad_cos_sim\"], label=\"G grad cos_sim\", alpha=0.8)\n",
    "    axes[1, 1].axhline(y=0.9, color='r', linestyle='--', alpha=0.5, label='high threshold')\n",
    "    axes[1, 1].axhline(y=0.3, color='b', linestyle='--', alpha=0.5, label='low threshold')\n",
    "    axes[1, 1].set_title(\"Gradient Direction Consistency (Cosine Similarity)\")\n",
    "    axes[1, 1].set_xlabel(\"step\")\n",
    "    axes[1, 1].set_ylabel(\"cosine similarity\")\n",
    "    axes[1, 1].set_ylim(-1.1, 1.1)\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True)\n",
    "\n",
    "    # n_critic over time - auto-scale with padding\n",
    "    n_critic_data = history[\"n_critic\"]\n",
    "    axes[2, 0].plot(step, n_critic_data, label=\"n_critic\", color=\"green\", drawstyle='steps-post', linewidth=2)\n",
    "    axes[2, 0].set_title(\"Adaptive n_critic\")\n",
    "    axes[2, 0].set_xlabel(\"step\")\n",
    "    axes[2, 0].set_ylabel(\"n_critic\")\n",
    "    # Auto-scale with padding to make constant lines visible\n",
    "    n_min, n_max = min(n_critic_data), max(n_critic_data)\n",
    "    if n_min == n_max:  # Constant line case\n",
    "        axes[2, 0].set_ylim(n_min - 1, n_max + 1)\n",
    "        axes[2, 0].axhline(y=n_min, color='green', linestyle='-', alpha=0.3, linewidth=10)  # Highlight flat line\n",
    "    else:\n",
    "        axes[2, 0].set_ylim(max(0, n_min - 1), n_max + 1)\n",
    "    axes[2, 0].grid(True)\n",
    "    axes[2, 0].legend()\n",
    "\n",
    "    # Combined: D cosine sim vs n_critic\n",
    "    ax2 = axes[2, 1]\n",
    "    ax2.plot(step, history[\"d_grad_cos_sim\"], label=\"D grad cos_sim\", color=\"blue\", alpha=0.7)\n",
    "    ax2.set_xlabel(\"step\")\n",
    "    ax2.set_ylabel(\"D grad cosine similarity\", color=\"blue\")\n",
    "    ax2.tick_params(axis='y', labelcolor=\"blue\")\n",
    "    ax2.set_ylim(-1.1, 1.1)\n",
    "    \n",
    "    ax2_twin = ax2.twinx()\n",
    "    ax2_twin.plot(step, n_critic_data, label=\"n_critic\", color=\"green\", alpha=0.7, drawstyle='steps-post', linewidth=2)\n",
    "    ax2_twin.set_ylabel(\"n_critic\", color=\"green\")\n",
    "    ax2_twin.tick_params(axis='y', labelcolor=\"green\")\n",
    "    if n_min == n_max:\n",
    "        ax2_twin.set_ylim(n_min - 1, n_max + 1)\n",
    "    else:\n",
    "        ax2_twin.set_ylim(max(0, n_min - 1), n_max + 1)\n",
    "    \n",
    "    axes[2, 1].set_title(\"D Gradient Consistency vs n_critic\")\n",
    "    axes[2, 1].grid(True)\n",
    "    \n",
    "    # n_gen over time - auto-scale with padding\n",
    "    n_gen_data = history[\"n_gen\"]\n",
    "    axes[3, 0].plot(step, n_gen_data, label=\"n_gen\", color=\"purple\", drawstyle='steps-post', linewidth=2)\n",
    "    axes[3, 0].set_title(\"Adaptive n_gen\")\n",
    "    axes[3, 0].set_xlabel(\"step\")\n",
    "    axes[3, 0].set_ylabel(\"n_gen\")\n",
    "    g_min, g_max = min(n_gen_data), max(n_gen_data)\n",
    "    if g_min == g_max:  # Constant line case\n",
    "        axes[3, 0].set_ylim(g_min - 1, g_max + 1)\n",
    "        axes[3, 0].axhline(y=g_min, color='purple', linestyle='-', alpha=0.3, linewidth=10)\n",
    "    else:\n",
    "        axes[3, 0].set_ylim(max(0, g_min - 1), g_max + 1)\n",
    "    axes[3, 0].grid(True)\n",
    "    axes[3, 0].legend()\n",
    "    \n",
    "    # Combined: G cosine sim vs n_gen\n",
    "    ax4 = axes[3, 1]\n",
    "    ax4.plot(step, history[\"g_grad_cos_sim\"], label=\"G grad cos_sim\", color=\"blue\", alpha=0.7)\n",
    "    ax4.set_xlabel(\"step\")\n",
    "    ax4.set_ylabel(\"G grad cosine similarity\", color=\"blue\")\n",
    "    ax4.tick_params(axis='y', labelcolor=\"blue\")\n",
    "    ax4.set_ylim(-1.1, 1.1)\n",
    "    ax4.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(TAG_DIR / f\"training_history_{RUN_TAG}.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1eac159",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparison(history_adaptive: dict, history_normal: dict, TAG_DIR: Path, RUN_TAG: str = \"comparison\"):\n",
    "    \"\"\"\n",
    "    Overlay two training histories to compare adaptive vs normal WGAN-GP.\n",
    "    \n",
    "    Args:\n",
    "        history_adaptive: History dict from adaptive (cosine similarity) training\n",
    "        history_normal: History dict from normal training\n",
    "        TAG_DIR: Directory to save the plot\n",
    "        RUN_TAG: Tag for the saved file name\n",
    "    \"\"\"\n",
    "    if len(history_adaptive.get(\"step\", [])) == 0 or len(history_normal.get(\"step\", [])) == 0:\n",
    "        print(\"One or both histories are empty. Train first.\")\n",
    "        return\n",
    "\n",
    "    fig, axes = plt.subplots(4, 2, figsize=(16, 14))\n",
    "    \n",
    "    step_a = history_adaptive[\"step\"]\n",
    "    step_n = history_normal[\"step\"]\n",
    "\n",
    "    # ---- Row 0: Losses ----\n",
    "    # Generator Loss\n",
    "    axes[0, 0].plot(step_a, history_adaptive[\"loss_G\"], label=\"Adaptive G\", alpha=0.8, color=\"blue\")\n",
    "    axes[0, 0].plot(step_n, history_normal[\"loss_G\"], label=\"Normal G\", alpha=0.8, color=\"red\", linestyle=\"--\")\n",
    "    axes[0, 0].set_title(\"Generator Loss Comparison\")\n",
    "    axes[0, 0].set_xlabel(\"step\")\n",
    "    axes[0, 0].set_ylabel(\"loss_G\")\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True)\n",
    "\n",
    "    # Critic Loss\n",
    "    axes[0, 1].plot(step_a, history_adaptive[\"loss_D\"], label=\"Adaptive D\", alpha=0.8, color=\"blue\")\n",
    "    axes[0, 1].plot(step_n, history_normal[\"loss_D\"], label=\"Normal D\", alpha=0.8, color=\"red\", linestyle=\"--\")\n",
    "    axes[0, 1].set_title(\"Critic Loss Comparison\")\n",
    "    axes[0, 1].set_xlabel(\"step\")\n",
    "    axes[0, 1].set_ylabel(\"loss_D\")\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True)\n",
    "\n",
    "    # ---- Row 1: Wasserstein Gap ----\n",
    "    # Raw gap\n",
    "    axes[1, 0].plot(step_a, history_adaptive[\"gap\"], label=\"Adaptive gap\", alpha=0.5, color=\"blue\")\n",
    "    axes[1, 0].plot(step_n, history_normal[\"gap\"], label=\"Normal gap\", alpha=0.5, color=\"red\")\n",
    "    axes[1, 0].plot(step_a, history_adaptive[\"gap_ema\"], label=\"Adaptive EMA\", linewidth=2, color=\"darkblue\")\n",
    "    axes[1, 0].plot(step_n, history_normal[\"gap_ema\"], label=\"Normal EMA\", linewidth=2, color=\"darkred\", linestyle=\"--\")\n",
    "    axes[1, 0].set_title(\"Wasserstein Gap Comparison\")\n",
    "    axes[1, 0].set_xlabel(\"step\")\n",
    "    axes[1, 0].set_ylabel(\"E[D(real)] - E[D(fake)]\")\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True)\n",
    "\n",
    "    # Gradient Penalty\n",
    "    axes[1, 1].plot(step_a, history_adaptive[\"gp\"], label=\"Adaptive GP\", alpha=0.7, color=\"blue\")\n",
    "    axes[1, 1].plot(step_n, history_normal[\"gp\"], label=\"Normal GP\", alpha=0.7, color=\"red\", linestyle=\"--\")\n",
    "    axes[1, 1].set_title(\"Gradient Penalty Comparison\")\n",
    "    axes[1, 1].set_xlabel(\"step\")\n",
    "    axes[1, 1].set_ylabel(\"gp\")\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True)\n",
    "\n",
    "    # ---- Row 2: Gradient Cosine Similarity ----\n",
    "    # D grad cos_sim\n",
    "    axes[2, 0].plot(step_a, history_adaptive[\"d_grad_cos_sim\"], label=\"Adaptive D cos_sim\", alpha=0.8, color=\"blue\")\n",
    "    axes[2, 0].plot(step_n, history_normal[\"d_grad_cos_sim\"], label=\"Normal D cos_sim\", alpha=0.8, color=\"red\", linestyle=\"--\")\n",
    "    axes[2, 0].axhline(y=0.9, color='green', linestyle=':', alpha=0.5, label='high threshold')\n",
    "    axes[2, 0].axhline(y=0.3, color='orange', linestyle=':', alpha=0.5, label='low threshold')\n",
    "    axes[2, 0].set_title(\"Discriminator Gradient Consistency\")\n",
    "    axes[2, 0].set_xlabel(\"step\")\n",
    "    axes[2, 0].set_ylabel(\"cosine similarity\")\n",
    "    axes[2, 0].set_ylim(-1.1, 1.1)\n",
    "    axes[2, 0].legend()\n",
    "    axes[2, 0].grid(True)\n",
    "\n",
    "    # G grad cos_sim\n",
    "    axes[2, 1].plot(step_a, history_adaptive[\"g_grad_cos_sim\"], label=\"Adaptive G cos_sim\", alpha=0.8, color=\"blue\")\n",
    "    axes[2, 1].plot(step_n, history_normal[\"g_grad_cos_sim\"], label=\"Normal G cos_sim\", alpha=0.8, color=\"red\", linestyle=\"--\")\n",
    "    axes[2, 1].set_title(\"Generator Gradient Consistency\")\n",
    "    axes[2, 1].set_xlabel(\"step\")\n",
    "    axes[2, 1].set_ylabel(\"cosine similarity\")\n",
    "    axes[2, 1].set_ylim(-1.1, 1.1)\n",
    "    axes[2, 1].legend()\n",
    "    axes[2, 1].grid(True)\n",
    "\n",
    "    # ---- Row 3: n_critic and n_gen ----\n",
    "    # n_critic\n",
    "    axes[3, 0].plot(step_a, history_adaptive[\"n_critic\"], label=\"Adaptive n_critic\", color=\"blue\", drawstyle='steps-post', linewidth=2)\n",
    "    axes[3, 0].plot(step_n, history_normal[\"n_critic\"], label=\"Normal n_critic\", color=\"red\", drawstyle='steps-post', linewidth=2, linestyle=\"--\")\n",
    "    axes[3, 0].set_title(\"n_critic Over Time\")\n",
    "    axes[3, 0].set_xlabel(\"step\")\n",
    "    axes[3, 0].set_ylabel(\"n_critic\")\n",
    "    all_n_critic = history_adaptive[\"n_critic\"] + history_normal[\"n_critic\"]\n",
    "    axes[3, 0].set_ylim(max(0, min(all_n_critic) - 1), max(all_n_critic) + 1)\n",
    "    axes[3, 0].legend()\n",
    "    axes[3, 0].grid(True)\n",
    "\n",
    "    # n_gen\n",
    "    axes[3, 1].plot(step_a, history_adaptive[\"n_gen\"], label=\"Adaptive n_gen\", color=\"blue\", drawstyle='steps-post', linewidth=2)\n",
    "    axes[3, 1].plot(step_n, history_normal[\"n_gen\"], label=\"Normal n_gen\", color=\"red\", drawstyle='steps-post', linewidth=2, linestyle=\"--\")\n",
    "    axes[3, 1].set_title(\"n_gen Over Time\")\n",
    "    axes[3, 1].set_xlabel(\"step\")\n",
    "    axes[3, 1].set_ylabel(\"n_gen\")\n",
    "    all_n_gen = history_adaptive[\"n_gen\"] + history_normal[\"n_gen\"]\n",
    "    axes[3, 1].set_ylim(max(0, min(all_n_gen) - 1), max(all_n_gen) + 1)\n",
    "    axes[3, 1].legend()\n",
    "    axes[3, 1].grid(True)\n",
    "\n",
    "    plt.suptitle(\"Adaptive (Cosine Similarity) vs Normal WGAN-GP\", fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(TAG_DIR / f\"comparison_{RUN_TAG}.png\", dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SUMMARY STATISTICS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"{'Metric':<25} {'Adaptive':>15} {'Normal':>15}\")\n",
    "    print(\"-\"*60)\n",
    "    print(f\"{'Final loss_G':<25} {history_adaptive['loss_G'][-1]:>15.4f} {history_normal['loss_G'][-1]:>15.4f}\")\n",
    "    print(f\"{'Final loss_D':<25} {history_adaptive['loss_D'][-1]:>15.4f} {history_normal['loss_D'][-1]:>15.4f}\")\n",
    "    print(f\"{'Final gap_ema':<25} {history_adaptive['gap_ema'][-1]:>15.4f} {history_normal['gap_ema'][-1]:>15.4f}\")\n",
    "    print(f\"{'Mean GP':<25} {np.mean(history_adaptive['gp']):>15.4f} {np.mean(history_normal['gp']):>15.4f}\")\n",
    "    print(f\"{'Mean D cos_sim':<25} {np.mean(history_adaptive['d_grad_cos_sim']):>15.4f} {np.mean(history_normal['d_grad_cos_sim']):>15.4f}\")\n",
    "    print(f\"{'Mean G cos_sim':<25} {np.mean(history_adaptive['g_grad_cos_sim']):>15.4f} {np.mean(history_normal['g_grad_cos_sim']):>15.4f}\")\n",
    "    print(f\"{'Final n_critic':<25} {history_adaptive['n_critic'][-1]:>15} {history_normal['n_critic'][-1]:>15}\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6611d860",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_gradient_vector(parameters) -> torch.Tensor:\n",
    "    \"\"\"Flatten all gradients into a single vector.\"\"\"\n",
    "    grads = []\n",
    "    for p in parameters:\n",
    "        if p.grad is not None:\n",
    "            grads.append(p.grad.detach().view(-1))\n",
    "    if len(grads) == 0:\n",
    "        return None\n",
    "    return torch.cat(grads)\n",
    "\n",
    "def cosine_similarity_gradients(grad_vec1: torch.Tensor, grad_vec2: torch.Tensor) -> float:\n",
    "    \"\"\"Compute cosine similarity between two gradient vectors.\"\"\"\n",
    "    if grad_vec1 is None or grad_vec2 is None:\n",
    "        return 0.0\n",
    "    cos_sim = F.cosine_similarity(\n",
    "        grad_vec1.unsqueeze(0), \n",
    "        grad_vec2.unsqueeze(0)\n",
    "    ).item()\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9386bbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_to_tanh(x: np.ndarray, clip: float = 3.0, eps: float = 1e-6):\n",
    "    \"\"\"\n",
    "    x: (C, T) numpy\n",
    "    per-channel standardize, clip, then map to ~[-1, 1]\n",
    "    \"\"\"\n",
    "    mean = x.mean(axis=-1, keepdims=True)\n",
    "    std  = x.std(axis=-1, keepdims=True)\n",
    "    std = np.maximum(std, eps)\n",
    "    x = (x - mean) / std\n",
    "    x = np.clip(x, -clip, clip) / clip\n",
    "    return x\n",
    "\n",
    "def descale_from_tanh(x: np.ndarray, original_mean: np.ndarray, original_std: np.ndarray, clip: float = 3.0):\n",
    "    \"\"\"\n",
    "    x: (C, T) numpy in ~[-1, 1]\n",
    "    reverse of scale_to_tanh\n",
    "    \"\"\"\n",
    "    x = x * clip\n",
    "    x = x * original_std + original_mean\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77d463de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 ‚Äî Generator & Critic (1D Conv, Standard WGAN-GP Style)\n",
    "# This is a *standard* WGAN-GP setup: the \"discriminator\" is a *critic* with a linear output (no sigmoid).\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, (nn.Conv1d, nn.ConvTranspose1d, nn.Linear)):\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "class Generator1D(nn.Module):\n",
    "    def __init__(self, z_dim: int, out_channels: int, seq_len: int, base: int = 64):\n",
    "        super().__init__()\n",
    "        assert seq_len % 16 == 0, \"For this template, seq_len should be divisible by 16.\"\n",
    "        self.z_dim = z_dim\n",
    "        self.out_channels = out_channels\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        # Project noise to a small temporal resolution then upsample x16 via ConvTranspose1d\n",
    "        self.init_len = seq_len // 16\n",
    "        self.fc = nn.Linear(z_dim, base * 8 * self.init_len)\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.ConvTranspose1d(base * 8, base * 4, kernel_size=4, stride=2, padding=1),  # x2\n",
    "            nn.BatchNorm1d(base * 4),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose1d(base * 4, base * 2, kernel_size=4, stride=2, padding=1),  # x4\n",
    "            nn.BatchNorm1d(base * 2),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose1d(base * 2, base, kernel_size=4, stride=2, padding=1),      # x8\n",
    "            nn.BatchNorm1d(base),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose1d(base, out_channels, kernel_size=4, stride=2, padding=1),  # x16\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.fc(z)\n",
    "        x = x.view(z.size(0), -1, self.init_len)\n",
    "        x = self.net(x)\n",
    "        return x\n",
    "\n",
    "class Critic1D(nn.Module):\n",
    "    def __init__(self, in_channels: int, seq_len: int, base: int = 64):\n",
    "        super().__init__()\n",
    "        assert seq_len % 16 == 0, \"For this template, seq_len should be divisible by 16.\"\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, base, kernel_size=4, stride=2, padding=1),   # /2\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv1d(base, base * 2, kernel_size=4, stride=2, padding=1),      # /4\n",
    "            nn.InstanceNorm1d(base * 2, affine=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv1d(base * 2, base * 4, kernel_size=4, stride=2, padding=1),  # /8\n",
    "            nn.InstanceNorm1d(base * 4, affine=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv1d(base * 4, base * 8, kernel_size=4, stride=2, padding=1),  # /16\n",
    "            nn.InstanceNorm1d(base * 8, affine=True),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "        self.out = nn.Linear(base * 8 * (seq_len // 16), 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.net(x)\n",
    "        h = h.view(x.size(0), -1)\n",
    "        return self.out(h).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a09633e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 ‚Äî WGAN-GP Utilities (Gradient Penalty, Losses)\n",
    "# WGAN-GP objective uses:\n",
    "#   loss_D = E[D(fake)] - E[D(real)] + lambda_gp * (||‚àá_x_hat D(x_hat)||_2 - 1)^2\n",
    "#   loss_G = -E[D(fake)]\n",
    "# This matches the standard WGAN-GP formulation referenced in the survey paper. ÓàÄfileciteÓàÇturn0file0ÓàÅ\n",
    "\n",
    "def gradient_penalty(critic: nn.Module, real: torch.Tensor, fake: torch.Tensor) -> torch.Tensor:\n",
    "    bsz = real.size(0)\n",
    "    eps = torch.rand(bsz, 1, 1, device=real.device)\n",
    "    x_hat = eps * real + (1 - eps) * fake\n",
    "    x_hat.requires_grad_(True)\n",
    "\n",
    "    d_hat = critic(x_hat)\n",
    "    grads = torch.autograd.grad(\n",
    "        outputs=d_hat,\n",
    "        inputs=x_hat,\n",
    "        grad_outputs=torch.ones_like(d_hat),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )[0]\n",
    "    grads = grads.view(bsz, -1)\n",
    "    gp = ((grads.norm(2, dim=1) - 1.0) ** 2).mean()\n",
    "    return gp\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_generator(generator: nn.Module, n: int, z_dim: int) -> torch.Tensor:\n",
    "    z = torch.randn(n, z_dim, device=DEVICE)\n",
    "    return generator(z).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33604732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 ‚Äî Adaptive Discriminator Controller (Gradient Direction Consistency)\n",
    "# Uses cosine similarity between consecutive gradient vectors to measure training stability.\n",
    "# High cosine similarity (‚âà1) = consistent gradient direction (stable training)\n",
    "# Low/negative cosine similarity = oscillating gradients (potentially unstable)\n",
    "#\n",
    "# NOTE: Cosine similarity is now computed in update_critic/update_generator.\n",
    "# This controller only adjusts n_critic based on the EMA values in state.\n",
    "\n",
    "@dataclass\n",
    "class TrainState:\n",
    "    step: int = 0\n",
    "    epoch: int = 0\n",
    "    n_critic: int = 5\n",
    "    n_gen: int = 1\n",
    "    lambda_gp: float = 10.0\n",
    "\n",
    "    # Common diagnostics\n",
    "    wasserstein_gap_ema: float = 0.0\n",
    "    ema_beta: float = 0.99\n",
    "    \n",
    "    # Gradient consistency tracking\n",
    "    d_grad_cos_sim: float = 0.0  # Discriminator gradient consistency\n",
    "    g_grad_cos_sim: float = 0.0  # Generator gradient consistency\n",
    "    \n",
    "    # EMA of cosine similarities\n",
    "    d_cos_sim_ema: float | None = None\n",
    "    g_cos_sim_ema: float | None = None\n",
    "    \n",
    "    # Store previous gradient vectors\n",
    "    prev_d_grad: torch.Tensor | None = None\n",
    "    prev_g_grad: torch.Tensor | None = None\n",
    "\n",
    "class AdaptiveDiscriminatorController:\n",
    "    \"\"\"\n",
    "    Adjusts n_critic based on gradient cosine similarity EMA stored in state.\n",
    "    Cosine similarity is computed in update_critic/update_generator functions.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "        cos_sim_threshold_high: float = 0.9,  # Too consistent ‚Üí reduce n_critic\n",
    "        cos_sim_threshold_low: float = 0.3,   # Too inconsistent ‚Üí increase n_critic\n",
    "        k_min: int = 2,\n",
    "        k_max: int = 10,\n",
    "        state: TrainState = None,\n",
    "    ):\n",
    "        self.cos_sim_threshold_high = cos_sim_threshold_high\n",
    "        self.cos_sim_threshold_low = cos_sim_threshold_low\n",
    "        self.k_min = k_min\n",
    "        self.k_max = k_max\n",
    "        self.state = state if state is not None else TrainState()\n",
    "\n",
    "    def on_batch_start(self, state: TrainState) -> None:\n",
    "        pass\n",
    "\n",
    "    def on_after_critic_update(self, optim_D: torch.optim.Optimizer, D: nn.Module = None) -> None:\n",
    "        \"\"\"Adjust n_critic based on D gradient cosine similarity EMA.\"\"\"\n",
    "        if self.state.d_cos_sim_ema is None:\n",
    "            return\n",
    "            \n",
    "        # Adaptive control based on gradient consistency\n",
    "        if self.state.d_cos_sim_ema > self.cos_sim_threshold_high:\n",
    "            # Gradients too consistent ‚Üí D might be too strong, reduce training\n",
    "            self.state.n_critic = max(self.k_min, self.state.n_critic - 1)\n",
    "        elif self.state.d_cos_sim_ema < self.cos_sim_threshold_low:\n",
    "            # Gradients too inconsistent ‚Üí D needs more updates\n",
    "            self.state.n_critic = min(self.k_max, self.state.n_critic + 1)\n",
    "\n",
    "    def on_after_generator_update(self, optim_G: torch.optim.Optimizer, G: nn.Module = None) -> None:\n",
    "        \"\"\"Optional: Could adjust n_gen based on G gradient consistency.\"\"\"\n",
    "        pass  # Currently no adjustment for generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f300da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_critic(z, real, G, D, optim_D, state, controller: Optional[AdaptiveDiscriminatorController] = None) -> Dict[str, float]:\n",
    "    fake = G(z).detach() # generate fake samples without gradients\n",
    "    \n",
    "    d_real = D(real).mean()\n",
    "    d_fake = D(fake).mean()\n",
    "    gap = (d_real - d_fake).item()\n",
    "    \n",
    "    gp = gradient_penalty(D, real, fake)\n",
    "    loss_D = (d_fake - d_real) + state.lambda_gp * gp\n",
    "    \n",
    "    optim_D.zero_grad(set_to_none=True)\n",
    "    loss_D.backward()\n",
    "    \n",
    "    # Always compute cosine similarity (for both adaptive and normal training)\n",
    "    curr_d_grad = compute_gradient_vector(D.parameters())\n",
    "    if curr_d_grad is not None and state.prev_d_grad is not None:\n",
    "        cos_sim = cosine_similarity_gradients(curr_d_grad, state.prev_d_grad)\n",
    "        # Update EMA\n",
    "        if state.d_cos_sim_ema is None:\n",
    "            state.d_cos_sim_ema = cos_sim\n",
    "        else:\n",
    "            state.d_cos_sim_ema = state.ema_beta * state.d_cos_sim_ema + (1 - state.ema_beta) * cos_sim\n",
    "        state.d_grad_cos_sim = state.d_cos_sim_ema\n",
    "    state.prev_d_grad = curr_d_grad.clone() if curr_d_grad is not None else None\n",
    "    \n",
    "    # Let controller do adaptive adjustments (if present)\n",
    "    if controller is not None:\n",
    "        controller.on_after_critic_update(optim_D, D=D)\n",
    "    \n",
    "    optim_D.step()\n",
    "    \n",
    "    return {\n",
    "        \"d_real\": float(d_real.item()),\n",
    "        \"d_fake\": float(d_fake.item()),\n",
    "        \"gap\": float(gap),\n",
    "        \"gp\": float(gp.item()),\n",
    "        \"loss_D\": float(loss_D.item()),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "913401fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_generator(z, optim_G, G, D, state, controller: Optional[AdaptiveDiscriminatorController] = None) -> float:\n",
    "    \n",
    "    fake = G(z)\n",
    "    loss_G = -D(fake).mean()\n",
    "    \n",
    "    optim_G.zero_grad(set_to_none=True)\n",
    "    loss_G.backward()\n",
    "    \n",
    "    # Always compute cosine similarity (for both adaptive and normal training)\n",
    "    curr_g_grad = compute_gradient_vector(G.parameters())\n",
    "    if curr_g_grad is not None and state.prev_g_grad is not None:\n",
    "        cos_sim = cosine_similarity_gradients(curr_g_grad, state.prev_g_grad)\n",
    "        # Update EMA\n",
    "        if state.g_cos_sim_ema is None:\n",
    "            state.g_cos_sim_ema = cos_sim\n",
    "        else:\n",
    "            state.g_cos_sim_ema = state.ema_beta * state.g_cos_sim_ema + (1 - state.ema_beta) * cos_sim\n",
    "        state.g_grad_cos_sim = state.g_cos_sim_ema\n",
    "    state.prev_g_grad = curr_g_grad.clone() if curr_g_grad is not None else None\n",
    "    \n",
    "    # Let controller do adaptive adjustments (if present)\n",
    "    if controller is not None:\n",
    "        controller.on_after_generator_update(optim_G, G=G)\n",
    "        \n",
    "    optim_G.step()\n",
    "    \n",
    "    return {\"loss_G\": float(loss_G.item())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecdeb20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(path, G, D, optim_G, optim_D, state, history):\n",
    "    \"\"\"Save full training checkpoint.\"\"\"\n",
    "    payload = {\n",
    "        \"G\": G.state_dict(),\n",
    "        \"D\": D.state_dict(),\n",
    "        \"optim_G\": optim_G.state_dict(),\n",
    "        \"optim_D\": optim_D.state_dict(),\n",
    "        \"state\": state.__dict__,\n",
    "        \"history\": dict(history),\n",
    "    }\n",
    "    torch.save(payload, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ee997e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(path, G, D, optim_G=None, optim_D=None):\n",
    "    \"\"\"Load checkpoint and return state + history.\"\"\"\n",
    "    ckpt = torch.load(path, map_location=DEVICE)\n",
    "    G.load_state_dict(ckpt[\"G\"])\n",
    "    D.load_state_dict(ckpt[\"D\"])\n",
    "    if optim_G is not None and \"optim_G\" in ckpt:\n",
    "        optim_G.load_state_dict(ckpt[\"optim_G\"])\n",
    "    if optim_D is not None and \"optim_D\" in ckpt:\n",
    "        optim_D.load_state_dict(ckpt[\"optim_D\"])\n",
    "    state = TrainState(**ckpt[\"state\"])\n",
    "    hist = ckpt.get(\"history\", {})\n",
    "    return state, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "994ebc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(\n",
    "    Z_DIM: int, \n",
    "    CHANNELS: int, \n",
    "    SEQ_LEN: int, \n",
    "    LR_G: float, \n",
    "    LR_D: float,\n",
    "    BETAS: Tuple[float, float], \n",
    "    DEVICE: str,\n",
    "    kmax: int = 10,\n",
    "    kmin: int = 2,\n",
    "    t_high: float = 0.9,\n",
    "    t_low: float = 0.3,\n",
    "):\n",
    "    G = Generator1D(z_dim=Z_DIM, out_channels=CHANNELS, seq_len=SEQ_LEN).to(DEVICE)\n",
    "    D = Critic1D(in_channels=CHANNELS, seq_len=SEQ_LEN).to(DEVICE)\n",
    "    \n",
    "    G.apply(weights_init) # need checked\n",
    "    D.apply(weights_init) # need checked\n",
    "    \n",
    "    optim_G = torch.optim.Adam(G.parameters(), lr=LR_G, betas=BETAS)\n",
    "    optim_D = torch.optim.Adam(D.parameters(), lr=LR_D, betas=BETAS)\n",
    "    \n",
    "    state = TrainState(step=0, epoch=0, n_gen=1, n_critic=5, lambda_gp=10.0)\n",
    "\n",
    "    controller = AdaptiveDiscriminatorController(k_max=kmax, k_min=kmin, state=state, cos_sim_threshold_high=t_high, cos_sim_threshold_low=t_low)\n",
    "    \n",
    "    print(f\"LR_G: {LR_G}, LR_D: {LR_D}, BETAS: {BETAS}\")\n",
    "    print(f\"n_critic: {state.n_critic}, n_gen: {state.n_gen}, lambda_gp: {state.lambda_gp}\")\n",
    "    \n",
    "    return G, D, optim_G, optim_D, state, controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706940d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MODEL_DIR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain_wgan_gp\u001b[39m(\n\u001b[32m      2\u001b[39m     loader: DataLoader,\n\u001b[32m      3\u001b[39m     G: nn.Module,\n\u001b[32m      4\u001b[39m     D: nn.Module,\n\u001b[32m      5\u001b[39m     optim_G: torch.optim.Optimizer,\n\u001b[32m      6\u001b[39m     optim_D: torch.optim.Optimizer,\n\u001b[32m      7\u001b[39m     state: TrainState,\n\u001b[32m      8\u001b[39m     z_dim: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m      9\u001b[39m     controller: Optional[AdaptiveDiscriminatorController] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     10\u001b[39m     epochs: \u001b[38;5;28mint\u001b[39m = \u001b[32m10\u001b[39m,\n\u001b[32m     11\u001b[39m     log_every: \u001b[38;5;28mint\u001b[39m = \u001b[32m50\u001b[39m,\n\u001b[32m     12\u001b[39m     save_every_steps: \u001b[38;5;28mint\u001b[39m = \u001b[32m500\u001b[39m,\n\u001b[32m     13\u001b[39m     best_metric: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mgap_ema\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     model_dir: \u001b[38;5;28mstr\u001b[39m = \u001b[43mMODEL_DIR\u001b[49m,\n\u001b[32m     15\u001b[39m     run_tag: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mwgan_gp_adaptiveD\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     16\u001b[39m ):\n\u001b[32m     17\u001b[39m     CHECKPOINT_DIR = Path(model_dir) / \u001b[33m\"\u001b[39m\u001b[33mcheckpoints\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     18\u001b[39m     CHECKPOINT_DIR.mkdir(parents=\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'MODEL_DIR' is not defined"
     ]
    }
   ],
   "source": [
    "def train_wgan_gp(\n",
    "    loader: DataLoader,\n",
    "    G: nn.Module,\n",
    "    D: nn.Module,\n",
    "    optim_G: torch.optim.Optimizer,\n",
    "    optim_D: torch.optim.Optimizer,\n",
    "    state: TrainState,\n",
    "    z_dim: int,\n",
    "    controller: Optional[AdaptiveDiscriminatorController] = None,\n",
    "    epochs: int = 10,\n",
    "    log_every: int = 50,\n",
    "    save_every_steps: int = 500,\n",
    "    best_metric: str = \"gap_ema\",\n",
    "    model_dir: str = MODEL_DIR,\n",
    "    run_tag: str = \"wgan_gp_adaptiveD\",\n",
    "):\n",
    "    CHECKPOINT_DIR = Path(model_dir) / \"checkpoints\"\n",
    "    CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    TAG_DIR = Path(model_dir) / run_tag\n",
    "    TAG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    RUN_TAG = run_tag\n",
    "    \n",
    "    G.train(); D.train()\n",
    "    \n",
    "    # History for plotting\n",
    "    history = {\n",
    "        \"step\": [], \"loss_D\": [], \"loss_G\": [], \"gap\": [], \"gap_ema\": [],\n",
    "        \"gp\": [], \"n_critic\": [], \"d_grad_cos_sim\": [], \"g_grad_cos_sim\": [],\n",
    "        \"n_critic\": [], \"n_gen\": [],\n",
    "    }\n",
    "    \n",
    "    best_score = getattr(state, 'best_score', -1e18)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        state.epoch = epoch\n",
    "        for batch_idx, real in enumerate(loader):\n",
    "            state.step += 1\n",
    "            if controller is not None:\n",
    "                controller.on_batch_start(state)\n",
    "                \n",
    "            real = real.to(DEVICE)\n",
    "            bsz = real.size(0)\n",
    "\n",
    "            # -------------------------\n",
    "            # Critic updates (n_critic)\n",
    "            # -------------------------\n",
    "            metrics_D = {}\n",
    "            for _ in range(state.n_critic):\n",
    "                z = torch.randn(bsz, z_dim, device=DEVICE)\n",
    "                metrics_D = update_critic(z, real, G, D, optim_D, state, controller)\n",
    "\n",
    "                # Update EMA of the gap\n",
    "                state.wasserstein_gap_ema = state.ema_beta * state.wasserstein_gap_ema + (1 - state.ema_beta) * metrics_D[\"gap\"]\n",
    "                metrics_D[\"gap_ema\"] = float(state.wasserstein_gap_ema) \n",
    "\n",
    "            # -------------------------\n",
    "            # Generator update\n",
    "            # -------------------------\n",
    "            z = torch.randn(bsz, z_dim, device=DEVICE)\n",
    "            metrics_G = update_generator(z, optim_G, G=G, D=D, state=state, controller=controller)\n",
    "\n",
    "            # -------------------------\n",
    "            # Log history (read cosine similarity from state, not metrics)\n",
    "            # -------------------------\n",
    "            history[\"step\"].append(state.step)\n",
    "            history[\"loss_D\"].append(metrics_D.get(\"loss_D\", 0))\n",
    "            history[\"loss_G\"].append(metrics_G.get(\"loss_G\", 0))\n",
    "            history[\"gap\"].append(metrics_D.get(\"gap\", 0))\n",
    "            \n",
    "            history[\"gap_ema\"].append(metrics_D.get(\"gap_ema\", 0))\n",
    "            history[\"gp\"].append(metrics_D.get(\"gp\", 0))\n",
    "            history[\"n_critic\"].append(state.n_critic)\n",
    "            history[\"n_gen\"].append(state.n_gen)\n",
    "            # Read from state where the controller stores the values\n",
    "            history[\"d_grad_cos_sim\"].append(state.d_grad_cos_sim)\n",
    "            history[\"g_grad_cos_sim\"].append(state.g_grad_cos_sim)\n",
    "\n",
    "            if (batch_idx % log_every) == 0:\n",
    "                d_cos = state.d_grad_cos_sim\n",
    "                g_cos = state.g_grad_cos_sim\n",
    "                print(\n",
    "                    f\"[Epoch {epoch:03d}/{epochs:03d}] [Batch {batch_idx:04d}/{len(loader):04d}] \"\n",
    "                    f\"[n_critic: {state.n_critic}] [n_gen: {state.n_gen}] \"\n",
    "                    f\"[gap: {metrics_D['gap']:+.3f} | ema: {metrics_D['gap_ema']:+.3f}] \"\n",
    "                    f\"[GP: {metrics_D['gp']:.3f}] \"\n",
    "                    f\"[D_cos: {d_cos:+.3f}] [G_cos: {g_cos:+.3f}] \"\n",
    "                    f\"[D: {metrics_D['loss_D']:+.3f}] [G: {metrics_G['loss_G']:+.3f}]\"\n",
    "                )\n",
    "\n",
    "            # -------------------------\n",
    "            # Save periodic checkpoint\n",
    "            # -------------------------\n",
    "            if save_every_steps > 0 and (state.step % save_every_steps) == 0:\n",
    "                ckpt_path = CHECKPOINT_DIR / f\"ckpt_{RUN_TAG}_step_{state.step}.pt\"\n",
    "                save_checkpoint(str(ckpt_path), G, D, optim_G, optim_D, state, history)\n",
    "                print(f\"üíæ Saved checkpoint: {ckpt_path}\")\n",
    "\n",
    "            # -------------------------\n",
    "            # Save best checkpoint\n",
    "            # -------------------------\n",
    "            if metrics_D:\n",
    "                score = float(metrics_D.get(best_metric, -1e18))\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_path = CHECKPOINT_DIR / f\"best_{RUN_TAG}.pt\"\n",
    "                    save_checkpoint(str(best_path), G, D, optim_G, optim_D, state, history)\n",
    "                    print(f\"üèÜ New BEST ({best_metric}={score:.4f}) -> {best_path}\")\n",
    "    \n",
    "    # Save final checkpoint\n",
    "    final_path = TAG_DIR / f\"final_{RUN_TAG}.pt\"\n",
    "    save_checkpoint(str(final_path), G, D, optim_G, optim_D, state, history)\n",
    "    print(f\"‚úÖ Saved final checkpoint: {final_path}\")\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b916a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_both(init_cos, init_og, tag, tag_dir, epochs):\n",
    "    cos_tag = f\"cos:{tag}\"\n",
    "    cos_dir = tag_dir / cos_tag\n",
    "    cos_checkpoints = cos_dir / \"checkpoints\"\n",
    "\n",
    "    cos_checkpoints.mkdir(parents=True, exist_ok=True)\n",
    "    cos_dir.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Run directory: {cos_tag}\")\n",
    "    \n",
    "    G_cos, D_cos, optim_G_cos, optim_D_cos, state_cos, controller = initialize(\n",
    "        Z_DIM=init_cos[\"z_dim\"], CHANNELS=init_cos[\"channels\"], SEQ_LEN=init_cos[\"seq_len\"], LR_D=init_cos[\"lr_D\"], LR_G=init_cos[\"lr_G\"], BETAS=init_cos[\"betas\"], DEVICE=init_cos[\"device\"],\n",
    "        kmax=init_cos[\"kmax\"], kmin=init_cos[\"kmin\"], t_high=init_cos[\"t_high\"], t_low=init_cos[\"t_low\"]\n",
    "    )\n",
    "\n",
    "    history_cos = train_wgan_gp(\n",
    "        loader, G_cos , D_cos, optim_G_cos, optim_D_cos, state_cos,\n",
    "        z_dim=init_cos[\"z_dim\"], controller=controller,\n",
    "        epochs=epochs, log_every=50, save_every_steps=500\n",
    "    )\n",
    "    \n",
    "    save_models(cos_dir, cos_tag, G_cos, D_cos, history_cos)\n",
    "    \n",
    "    og_tag = f\"og:{tag}\"\n",
    "    og_dir = tag_dir / og_tag\n",
    "    og_checkpoints = og_dir / \"checkpoints\"\n",
    "\n",
    "    og_checkpoints.mkdir(parents=True, exist_ok=True)\n",
    "    og_dir.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Run directory: {og_tag}\")\n",
    "    \n",
    "    G_og, D_og, optim_G_og, optim_D_og, state_og, _ = initialize(\n",
    "        Z_DIM=init_og[\"z_dim\"], CHANNELS=init_og[\"channels\"], SEQ_LEN=init_og[\"seq_len\"], LR_D=init_og[\"lr_D\"], LR_G=init_og[\"lr_G\"], BETAS=init_og[\"betas\"], DEVICE=init_og[\"device\"]\n",
    "    )\n",
    "\n",
    "    history_og = train_wgan_gp(\n",
    "        loader, G_og , D_og, optim_G_og, optim_D_og, state_og,\n",
    "        z_dim=init_og[\"z_dim\"], controller=None,  # No controller for normal training\n",
    "        epochs=epochs, log_every=50, save_every_steps=500\n",
    "    )\n",
    "    \n",
    "    save_models(og_dir, og_tag, G_og, D_og, history_og)\n",
    "    \n",
    "    # Return histories, directories, and tags\n",
    "    return {\n",
    "        \"cos\": {\"history\": history_cos, \"dir\": cos_dir, \"tag\": cos_tag},\n",
    "        \"og\": {\"history\": history_og, \"dir\": og_dir, \"tag\": og_tag},\n",
    "        \"tag_dir\": tag_dir,  # Parent directory for comparison plots\n",
    "        \"run_tag\": tag,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7288693",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9847bfed",
   "metadata": {},
   "source": [
    "# Dataset Fuc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc848a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gdf_files(data_dir: str, resample_hz: int, mode: str, verbose: bool = False)  -> list[str]:\n",
    "    if mode == \"train\":\n",
    "        pattern = \"*T.gdf\"  # Training files\n",
    "    elif mode == \"eval\":\n",
    "        pattern = \"*E.gdf\"  # Evaluation files\n",
    "    else:\n",
    "        raise ValueError(\"mode should be 'train' or 'eval'\")\n",
    "    \n",
    "    all_files = sorted([file for file in data_dir.glob(pattern)])\n",
    "    \n",
    "    if len(all_files) == 0:\n",
    "        raise ValueError(f\"No .gdf files found in {data_dir} with pattern {pattern}\")\n",
    "    \n",
    "    raws = []   \n",
    "    for file in all_files:\n",
    "        raw = mne.io.read_raw_gdf(file, preload=True, verbose=\"error\")\n",
    "        raw.pick(\"eeg\")  \n",
    "        raw.resample(resample_hz)\n",
    "        raws.append(raw)\n",
    "    \n",
    "    return raws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30ff8b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_epoch(raw: mne.io.Raw, event_id: Dict[str, int], tmin: float, tmax: float) -> np.ndarray:\n",
    "    events, event_dict = mne.events_from_annotations(raw, verbose=\"error\")\n",
    "    lh = str(event_id.get(\"LH\"))\n",
    "    rh = str(event_id.get(\"RH\"))\n",
    "    event_id = {\"LH\": event_dict.get(lh), \"RH\": event_dict.get(rh)}\n",
    "    \n",
    "    if event_id[\"LH\"] is None or event_id[\"RH\"] is None:\n",
    "        print(f\"At {raw.filenames[0].name} Event ID(lh({type(lh)}):{lh}) or RH({type(rh)}):{rh} not found in annotations: {event_dict}\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"From:{event_dict} to {event_id}\")\n",
    "    epochs = mne.Epochs(raw, events, event_id=event_id, tmin=tmin, tmax=tmax, baseline=None, preload=True, verbose=\"error\")\n",
    "    return epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6460fa51",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mne' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmake_dataset\u001b[39m(raws: \u001b[38;5;28mlist\u001b[39m[\u001b[43mmne\u001b[49m.io.Raw], tmin: \u001b[38;5;28mfloat\u001b[39m, tmax: \u001b[38;5;28mfloat\u001b[39m, event_id: Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]) -> np.ndarray:\n\u001b[32m      2\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m raws:\n\u001b[32m      3\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mThe list of raws is empty.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'mne' is not defined"
     ]
    }
   ],
   "source": [
    "def make_dataset(raws: list[mne.io.Raw], tmin: float, tmax: float, event_id: Dict[str, int]) -> np.ndarray:\n",
    "    if not raws:\n",
    "        raise ValueError(\"The list of raws is empty.\")\n",
    "    \n",
    "    epochs_list = []\n",
    "    for i, raw in enumerate(raws):\n",
    "        epochs = create_epoch(raw, event_id=event_id, tmin=tmin, tmax=tmax)\n",
    "        \n",
    "        if len(epochs) == 0:\n",
    "            continue\n",
    "        \n",
    "        epochs_list.append(epochs.get_data())  # shape: (n_epochs, n_channels, n_times)\n",
    "    \n",
    "    if not epochs_list:\n",
    "        raise ValueError(\"No epochs were created from the provided raw data.\")\n",
    "    \n",
    "    dataset = np.concatenate(epochs_list, axis=0)  # shape: (total_epochs, n_channels, n_times)\n",
    "    print(f\"Dataset shape: {dataset.shape}\")\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17a304e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From first\n",
    "class EEGTensorDataset(Dataset):\n",
    "    def __init__(self, X: np.ndarray, y: Optional[np.ndarray] = None, zscore_per_channel: bool = True):\n",
    "        \"\"\"\n",
    "        X: (N, C, L)\n",
    "        y: optional (N,)\n",
    "        zscore_per_channel:\n",
    "          - global per-channel mean/std computed across (N,L) for each channel\n",
    "        \"\"\"\n",
    "        assert X.ndim == 3\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = None if y is None else y.astype(np.int64)\n",
    "\n",
    "        if zscore_per_channel:\n",
    "            mean = self.X.mean(axis=(0, 2), keepdims=True)\n",
    "            std  = self.X.std(axis=(0, 2), keepdims=True) + 1e-6\n",
    "            self.X = (self.X - mean) / std\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.from_numpy(self.X[idx])  # (C, L)\n",
    "        if self.y is None:\n",
    "            return x\n",
    "        return x, int(self.y[idx])\n",
    "\n",
    "# Unconditional GAN: we only use x; y is available if you want conditional later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df75a091",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5400693",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74f6020f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_DIR exists: True\n",
      "MODEL_DIR exists: True\n",
      "Found 45 .gdf files.\n",
      "First 10 files: ['B0101T.gdf', 'B0102T.gdf', 'B0103T.gdf', 'B0104E.gdf', 'B0105E.gdf', 'B0201T.gdf', 'B0202T.gdf', 'B0203T.gdf', 'B0204E.gdf', 'B0205E.gdf']\n"
     ]
    }
   ],
   "source": [
    "PLUEM_DIR = Path.cwd()\n",
    "DATA_DIR = PLUEM_DIR.parent / \"BCICIV_2b_gdf\"\n",
    "MODEL_DIR = PLUEM_DIR / \"models\"\n",
    "\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"DATA_DIR exists: {DATA_DIR.exists()}\")\n",
    "print(f\"MODEL_DIR exists: {MODEL_DIR.exists()}\")\n",
    "\n",
    "files = list(DATA_DIR.glob(\"*.gdf\"))\n",
    "files.sort()\n",
    "print(f\"Found {len(files)} .gdf files.\")\n",
    "print(\"First 10 files:\", [f.name for f in files[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f08db4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Info | 8 non-empty values\n",
      " bads: []\n",
      " ch_names: EEG:C3, EEG:Cz, EEG:C4, EOG:ch01, EOG:ch02, EOG:ch03\n",
      " chs: 6 EEG\n",
      " custom_ref_applied: False\n",
      " highpass: 0.0 Hz\n",
      " lowpass: 50.0 Hz\n",
      " meas_date: 2005-10-25 09:35:11 UTC\n",
      " nchan: 6\n",
      " projs: []\n",
      " sfreq: 100.0 Hz\n",
      " subject_info: <subject_info | his_id: B01, sex: 0, last_name: X, birthday: 1984-12-01>\n",
      ">\n",
      "Using qt as 2D backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mne_qt_browser._pg_figure.MNEQtBrowser(0x30f9ae2d0) at 0x31d002b00>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels marked as bad:\n",
      "[np.str_('EEG:Cz')]\n",
      "Attempting to create new mne-python configuration file:\n",
      "/Users/ratchanonkhongsawi/.mne/mne-python.json\n",
      "Could not read the /Users/ratchanonkhongsawi/.mne/mne-python.json json file during the writing. Assuming it is empty. Got: Expecting value: line 1 column 1 (char 0)\n"
     ]
    }
   ],
   "source": [
    "one_file = files[0]\n",
    "raw = mne.io.read_raw_gdf(one_file, preload=True, verbose=\"error\")\n",
    "raw.pick(\"eeg\")  # Keep only EEG channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90bdd23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Reading: B0101T.gdf ===\n",
      "\n",
      "=== Reading: B0102T.gdf ===\n",
      "\n",
      "=== Reading: B0103T.gdf ===\n",
      "\n",
      "=== Reading: B0201T.gdf ===\n",
      "\n",
      "=== Reading: B0202T.gdf ===\n",
      "\n",
      "=== Reading: B0203T.gdf ===\n",
      "\n",
      "=== Reading: B0301T.gdf ===\n",
      "\n",
      "=== Reading: B0302T.gdf ===\n",
      "\n",
      "=== Reading: B0303T.gdf ===\n",
      "\n",
      "=== Reading: B0401T.gdf ===\n",
      "\n",
      "=== Reading: B0402T.gdf ===\n",
      "\n",
      "=== Reading: B0403T.gdf ===\n",
      "\n",
      "=== Reading: B0501T.gdf ===\n",
      "\n",
      "=== Reading: B0502T.gdf ===\n",
      "\n",
      "=== Reading: B0503T.gdf ===\n",
      "\n",
      "=== Reading: B0601T.gdf ===\n",
      "\n",
      "=== Reading: B0602T.gdf ===\n",
      "\n",
      "=== Reading: B0603T.gdf ===\n",
      "\n",
      "=== Reading: B0701T.gdf ===\n",
      "\n",
      "=== Reading: B0702T.gdf ===\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m TMIN, TMAX = \u001b[32m0.0\u001b[39m, \u001b[32m4.0\u001b[39m\n\u001b[32m      3\u001b[39m EVENT_ID = {\u001b[33m\"\u001b[39m\u001b[33mLH\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m769\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mRH\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m770\u001b[39m}\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m raws = \u001b[43mload_gdf_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mRESAMPLE_HZ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m X = make_dataset(raws, TMIN, TMAX, event_id=EVENT_ID)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDataset shape:\u001b[39m\u001b[33m\"\u001b[39m, X.shape)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mload_gdf_files\u001b[39m\u001b[34m(data_dir, resample_hz, mode, verbose)\u001b[39m\n\u001b[32m     18\u001b[39m     raw = mne.io.read_raw_gdf(file, preload=\u001b[38;5;28;01mTrue\u001b[39;00m, verbose=\u001b[33m\"\u001b[39m\u001b[33merror\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m     raw.pick(\u001b[33m\"\u001b[39m\u001b[33meeg\u001b[39m\u001b[33m\"\u001b[39m)  \n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     \u001b[43mraw\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresample_hz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m     raws.append(raw)\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m raws\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<decorator-gen-195>:12\u001b[39m, in \u001b[36mresample\u001b[39m\u001b[34m(self, sfreq, npad, window, stim_picks, n_jobs, events, pad, method, verbose)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/3rd_research/lib/python3.11/site-packages/mne/io/base.py:1440\u001b[39m, in \u001b[36mBaseRaw.resample\u001b[39m\u001b[34m(self, sfreq, npad, window, stim_picks, n_jobs, events, pad, method, verbose)\u001b[39m\n\u001b[32m   1438\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.preload:\n\u001b[32m   1439\u001b[39m     data_chunk = \u001b[38;5;28mself\u001b[39m._data[:, offsets[ri] : offsets[ri + \u001b[32m1\u001b[39m]]\n\u001b[32m-> \u001b[39m\u001b[32m1440\u001b[39m     new_data[:, this_sl] = \u001b[43mresample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1441\u001b[39m     \u001b[38;5;66;03m# In empirical testing, it was faster to resample all channels\u001b[39;00m\n\u001b[32m   1442\u001b[39m     \u001b[38;5;66;03m# (above) and then replace the stim channels than it was to\u001b[39;00m\n\u001b[32m   1443\u001b[39m     \u001b[38;5;66;03m# only resample the proper subset of channels and then use\u001b[39;00m\n\u001b[32m   1444\u001b[39m     \u001b[38;5;66;03m# np.insert() to restore the stims.\u001b[39;00m\n\u001b[32m   1445\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(stim_picks) > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<decorator-gen-87>:12\u001b[39m, in \u001b[36mresample\u001b[39m\u001b[34m(x, up, down, axis, window, n_jobs, pad, npad, method, verbose)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/3rd_research/lib/python3.11/site-packages/mne/filter.py:1876\u001b[39m, in \u001b[36mresample\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   1874\u001b[39m kwargs = \u001b[38;5;28mdict\u001b[39m(pad=pad, window=window, n_jobs=n_jobs)\n\u001b[32m   1875\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m method == \u001b[33m\"\u001b[39m\u001b[33mfft\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1876\u001b[39m     y = \u001b[43m_resample_fft\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnpad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnpad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mratio\u001b[49m\u001b[43m=\u001b[49m\u001b[43mratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_len\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfinal_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1877\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1878\u001b[39m     up, down, kwargs[\u001b[33m\"\u001b[39m\u001b[33mwindow\u001b[39m\u001b[33m\"\u001b[39m] = _prep_polyphase(\n\u001b[32m   1879\u001b[39m         ratio, x.shape[-\u001b[32m1\u001b[39m], final_len, window\n\u001b[32m   1880\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/3rd_research/lib/python3.11/site-packages/mne/filter.py:1972\u001b[39m, in \u001b[36m_resample_fft\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   1970\u001b[39m     y = np.zeros((\u001b[38;5;28mlen\u001b[39m(x_flat), new_len - to_removes.sum()), dtype=x_flat.dtype)\n\u001b[32m   1971\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m xi, x_ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(x_flat):\n\u001b[32m-> \u001b[39m\u001b[32m1972\u001b[39m         y[xi] = \u001b[43m_fft_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnpads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_removes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcuda_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1973\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1974\u001b[39m     y = parallel(\n\u001b[32m   1975\u001b[39m         p_fun(x_, new_len, npads, to_removes, cuda_dict, pad) \u001b[38;5;28;01mfor\u001b[39;00m x_ \u001b[38;5;129;01min\u001b[39;00m x_flat\n\u001b[32m   1976\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/3rd_research/lib/python3.11/site-packages/mne/cuda.py:354\u001b[39m, in \u001b[36m_fft_resample\u001b[39m\u001b[34m(x, new_len, npads, to_removes, cuda_dict, pad)\u001b[39m\n\u001b[32m    352\u001b[39m     x_fft[nyq : nyq + \u001b[32m1\u001b[39m] *= \u001b[32m2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m shorter \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0.5\u001b[39m\n\u001b[32m    353\u001b[39m x_fft *= cuda_dict[\u001b[33m\"\u001b[39m\u001b[33mW\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m y = \u001b[43mcuda_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mirfft\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_fft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[38;5;66;03m# now let's trim it back to the correct size (if there was padding)\u001b[39;00m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (to_removes > \u001b[32m0\u001b[39m).any():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/3rd_research/lib/python3.11/site-packages/scipy/fft/_backend.py:29\u001b[39m, in \u001b[36m_ScipyBackend.__ua_function__\u001b[39m\u001b[34m(method, args, kwargs)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     28\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/3rd_research/lib/python3.11/site-packages/scipy/fft/_basic_backend.py:97\u001b[39m, in \u001b[36mirfft\u001b[39m\u001b[34m(x, n, axis, norm, overwrite_x, workers, plan)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mirfft\u001b[39m(x, n=\u001b[38;5;28;01mNone\u001b[39;00m, axis=-\u001b[32m1\u001b[39m, norm=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     96\u001b[39m           overwrite_x=\u001b[38;5;28;01mFalse\u001b[39;00m, workers=\u001b[38;5;28;01mNone\u001b[39;00m, *, plan=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_execute_1D\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mirfft\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pocketfft\u001b[49m\u001b[43m.\u001b[49m\u001b[43mirfft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m                       \u001b[49m\u001b[43moverwrite_x\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverwrite_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mplan\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/3rd_research/lib/python3.11/site-packages/scipy/fft/_basic_backend.py:32\u001b[39m, in \u001b[36m_execute_1D\u001b[39m\u001b[34m(func_str, pocketfft_func, x, n, axis, norm, overwrite_x, workers, plan)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_numpy(xp):\n\u001b[32m     31\u001b[39m     x = np.asarray(x)\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpocketfft_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m                          \u001b[49m\u001b[43moverwrite_x\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverwrite_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mplan\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m norm = _validate_fft_args(workers, plan, norm)\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(xp, \u001b[33m'\u001b[39m\u001b[33mfft\u001b[39m\u001b[33m'\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/3rd_research/lib/python3.11/site-packages/scipy/fft/_pocketfft/basic.py:95\u001b[39m, in \u001b[36mc2r\u001b[39m\u001b[34m(forward, x, n, axis, norm, overwrite_x, workers, plan)\u001b[39m\n\u001b[32m     92\u001b[39m     tmp, _ = _fix_shape_1d(tmp, (n//\u001b[32m2\u001b[39m) + \u001b[32m1\u001b[39m, axis)\n\u001b[32m     94\u001b[39m \u001b[38;5;66;03m# Note: overwrite_x is not utilized\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpfft\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc2r\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtmp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "RESAMPLE_HZ = 256\n",
    "TMIN, TMAX = 0.0, 4.0\n",
    "EVENT_ID = {\"LH\": \"769\", \"RH\": \"770\"}\n",
    "\n",
    "raws = load_gdf_files(DATA_DIR, RESAMPLE_HZ, verbose=True, mode=\"train\")\n",
    "X = make_dataset(raws, TMIN, TMAX, event_id=EVENT_ID)\n",
    "print(\"Dataset shape:\", X.shape)\n",
    "\n",
    "SEQ_LEN = X.shape[2]\n",
    "if SEQ_LEN % 16 != 0:\n",
    "    new_len = (SEQ_LEN // 16) * 16  # floor to nearest multiple of 16\n",
    "    print(f\"Cropping SEQ_LEN from {SEQ_LEN} -> {new_len} to satisfy architecture constraint.\")\n",
    "    X = X[:, :, :new_len]\n",
    "    \n",
    "CHANNELS = X.shape[1]\n",
    "SEQ_LEN  = X.shape[2]\n",
    "print(\"CHANNELS:\", CHANNELS, \"SEQ_LEN:\", SEQ_LEN)\n",
    "assert SEQ_LEN % 16 == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec847ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches: 57\n"
     ]
    }
   ],
   "source": [
    "dataset = EEGTensorDataset(X, y=None, zscore_per_channel=True)\n",
    "loader = DataLoader(dataset, batch_size=64, shuffle=True, drop_last=True, num_workers=0, pin_memory=(DEVICE==\"cuda\"))\n",
    "print(\"Batches:\", len(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cb6f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model configs ---\n",
    "\n",
    "\n",
    "# G = Generator1D(z_dim=Z_DIM, out_channels=CHANNELS, seq_len=SEQ_LEN).to(DEVICE)\n",
    "# D = Critic1D(in_channels=CHANNELS, seq_len=SEQ_LEN).to(DEVICE)\n",
    "\n",
    "# G.apply(weights_init) # need checked\n",
    "# D.apply(weights_init) # need checked\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5529422",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_DIM = 128\n",
    "\n",
    "LR_G = 1e-4  # Generator learning rate\n",
    "LR_D = [5e-5, 2e-5]  # Discriminator/Critic learning rate\n",
    "BETAS = (0.2, 0.9)   # standard WGAN-GP choice\n",
    "\n",
    "KMAX = 10\n",
    "KMIN = 2\n",
    "T_HIGH = 0.9\n",
    "T_LOW = 0.0\n",
    "\n",
    "EPOCH = 1\n",
    "TAG = \"test\"\n",
    "\n",
    "tag_dir = MODEL_DIR / TAG\n",
    "tag_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# print(\"G params:\", sum(p.numel() for p in G.parameters())/1e6, \"M\")\n",
    "# print(\"D params:\", sum(p.numel() for p in D.parameters())/1e6, \"M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c649ebbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run directory: cos:test\n",
      "LR_G: 0.0001, LR_D: 5e-05, BETAS: (0.0, 0.9)\n",
      "n_critic: 5, n_gen: 1, lambda_gp: 10.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[79]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m histories = [] \u001b[38;5;66;03m# to store histories for different LR_D [5e-5, 2e-5]\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m lr_D \u001b[38;5;129;01min\u001b[39;00m LR_D:\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     history = \u001b[43mtrain_both\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr_D\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr_D\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr_G\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLR_G\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtag\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTAG\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtag_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtag_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEPOCH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43mz_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mZ_DIM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchannels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCHANNELS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSEQ_LEN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbetas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBETAS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDEVICE\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m     histories.append(history)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[76]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mtrain_both\u001b[39m\u001b[34m(lr_D, lr_G, tag, tag_dir, epochs, z_dim, channels, seq_len, betas, device)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRun directory: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcos_tag\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m G_cos, D_cos, optim_G_cos, optim_D_cos, state_cos, controller = initialize(\n\u001b[32m     11\u001b[39m     Z_DIM=z_dim, CHANNELS=channels, SEQ_LEN=seq_len, LR_D=lr_D, LR_G=lr_G, BETAS=betas, DEVICE=device\n\u001b[32m     12\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m history_cos = \u001b[43mtrain_wgan_gp\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mG_cos\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD_cos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim_G_cos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim_D_cos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_cos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mz_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mz_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontroller\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontroller\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_every\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_every_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m500\u001b[39;49m\n\u001b[32m     18\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m save_models(cos_dir, cos_tag, G_cos, D_cos, history_cos)\n\u001b[32m     22\u001b[39m og_tag = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mog:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtag\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[73]\u001b[39m\u001b[32m, line 50\u001b[39m, in \u001b[36mtrain_wgan_gp\u001b[39m\u001b[34m(loader, G, D, optim_G, optim_D, state, z_dim, controller, epochs, log_every, save_every_steps, best_metric, model_dir, run_tag)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(state.n_critic):\n\u001b[32m     49\u001b[39m     z = torch.randn(bsz, z_dim, device=DEVICE)\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     metrics_D = \u001b[43mupdate_critic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim_D\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontroller\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m     \u001b[38;5;66;03m# Update EMA of the gap\u001b[39;00m\n\u001b[32m     53\u001b[39m     state.wasserstein_gap_ema = state.ema_beta * state.wasserstein_gap_ema + (\u001b[32m1\u001b[39m - state.ema_beta) * metrics_D[\u001b[33m\"\u001b[39m\u001b[33mgap\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[67]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mupdate_critic\u001b[39m\u001b[34m(z, real, G, D, optim_D, state, controller)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mupdate_critic\u001b[39m(z, real, G, D, optim_D, state, controller: Optional[AdaptiveDiscriminatorController] = \u001b[38;5;28;01mNone\u001b[39;00m) -> Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]:\n\u001b[32m      2\u001b[39m     fake = G(z).detach() \u001b[38;5;66;03m# generate fake samples without gradients\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     d_real = \u001b[43mD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal\u001b[49m\u001b[43m)\u001b[49m.mean()\n\u001b[32m      5\u001b[39m     d_fake = D(fake).mean()\n\u001b[32m      6\u001b[39m     gap = (d_real - d_fake).item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/3rd_research/lib/python3.11/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/3rd_research/lib/python3.11/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 70\u001b[39m, in \u001b[36mCritic1D.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     h = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m     h = h.view(x.size(\u001b[32m0\u001b[39m), -\u001b[32m1\u001b[39m)\n\u001b[32m     72\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.out(h).view(-\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/3rd_research/lib/python3.11/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/3rd_research/lib/python3.11/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/3rd_research/lib/python3.11/site-packages/torch/nn/modules/container.py:253\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    250\u001b[39m \u001b[33;03mRuns the forward pass.\u001b[39;00m\n\u001b[32m    251\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/3rd_research/lib/python3.11/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/3rd_research/lib/python3.11/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/3rd_research/lib/python3.11/site-packages/torch/nn/modules/conv.py:375\u001b[39m, in \u001b[36mConv1d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    374\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m375\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/3rd_research/lib/python3.11/site-packages/torch/nn/modules/conv.py:370\u001b[39m, in \u001b[36mConv1d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv1d(\n\u001b[32m    359\u001b[39m         F.pad(\n\u001b[32m    360\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    367\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    368\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m370\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "init_cos = {\n",
    "    \"z_dim\": Z_DIM,\n",
    "    \"channels\": CHANNELS,\n",
    "    \"seq_len\": SEQ_LEN,\n",
    "    \"lr_D\": None,  # to be set in loop\n",
    "    \"lr_G\": LR_G,\n",
    "    \"betas\": BETAS,\n",
    "    \"device\": DEVICE,\n",
    "    \"kmax\": KMAX,\n",
    "    \"kmin\": KMIN,\n",
    "    \"t_high\": T_HIGH,\n",
    "    \"t_low\": T_LOW,\n",
    "}\n",
    "\n",
    "init_og = {\n",
    "    \"z_dim\": Z_DIM,\n",
    "    \"channels\": CHANNELS,\n",
    "    \"seq_len\": SEQ_LEN,\n",
    "    \"lr_D\": None,  # to be set in loop\n",
    "    \"lr_G\": LR_G,\n",
    "    \"betas\": BETAS,\n",
    "    \"device\": DEVICE,\n",
    "}\n",
    "\n",
    "results = []  # to store results for different LR_D [5e-5, 2e-5]\n",
    "for lr_D in LR_D:\n",
    "    run_tag = f\"{TAG}_lr_D_{lr_D}\"\n",
    "    init_cos[\"lr_D\"] = lr_D\n",
    "    init_og[\"lr_D\"] = lr_D\n",
    "    result = train_both(\n",
    "        init_cos=init_cos,\n",
    "        init_og=init_og,\n",
    "        tag=run_tag,\n",
    "        tag_dir=tag_dir,\n",
    "        epochs=EPOCH,\n",
    "    )\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e958d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8 ‚Äî Training Curve Visualization (with Gradient Consistency)\n",
    "# LR_D = 5e-5\n",
    "r0 = results[0]\n",
    "\n",
    "# Plot adaptive (cosine) - saves to cos_dir\n",
    "plot_training_history(r0[\"cos\"][\"history\"], r0[\"cos\"][\"dir\"], r0[\"cos\"][\"tag\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f91802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot normal (og) - saves to og_dir\n",
    "plot_training_history(r0[\"og\"][\"history\"], r0[\"og\"][\"dir\"], r0[\"og\"][\"tag\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6581fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison plot - saves to parent tag_dir\n",
    "plot_comparison(\n",
    "    r0[\"cos\"][\"history\"],\n",
    "    r0[\"og\"][\"history\"],\n",
    "    r0[\"tag_dir\"],\n",
    "    f\"comparison_{r0['run_tag']}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c1ddac",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef41d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LR_D = 2e-5\n",
    "r1 = results[1]\n",
    "\n",
    "# Plot adaptive (cosine) - saves to cos_dir\n",
    "plot_training_history(r1[\"cos\"][\"history\"], r1[\"cos\"][\"dir\"], r1[\"cos\"][\"tag\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626f3cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot normal (og) - saves to og_dir\n",
    "plot_training_history(r1[\"og\"][\"history\"], r1[\"og\"][\"dir\"], r1[\"og\"][\"tag\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1931f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison plot - saves to parent tag_dir\n",
    "plot_comparison(\n",
    "    r1[\"cos\"][\"history\"],\n",
    "    r1[\"og\"][\"history\"],\n",
    "    r1[\"tag_dir\"],\n",
    "    f\"comparison_{r1['run_tag']}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777f8c90",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45e9533",
   "metadata": {},
   "source": [
    "metric: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105b1841",
   "metadata": {},
   "source": [
    "### Normal"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3rd_research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
